
@inproceedings{bellard_qemu_2005,
	address = {USA},
	series = {{ATEC} '05},
	title = {{QEMU}, a fast and portable dynamic translator},
	abstract = {We present the internals of QEMU, a fast machine emulator using an original portable dynamic translator. It emulates several CPUs (x86, PowerPC, ARM and Sparc) on several hosts (x86, PowerPC, ARM, Sparc, Alpha and MIPS). QEMU supports full system emulation in which a complete and unmodified operating system is run in a virtual machine and Linux user mode emulation where a Linux process compiled for one target CPU can be run on another CPU.},
	booktitle = {Proceedings of the {Annual} {Conference} on {USENIX} {Annual} {Technical} {Conference}},
	publisher = {USENIX Association},
	author = {Bellard, Fabrice},
	year = {2005},
	note = {event-place: Anaheim, CA},
	pages = {41},
}

@inproceedings{schaefer_complexity_1978,
	title = {The {Complexity} of {Satisfiability} {Problems}},
	url = {https://doi.org/10.1145/800133.804350},
	doi = {10.1145/800133.804350},
	booktitle = {Proceedings of the 10th {Annual} {ACM} {Symposium} on {Theory} of {Computing}, {May} 1-3, 1978, {San} {Diego}, {California}, {USA}},
	author = {Schaefer, Thomas J.},
	year = {1978},
	pages = {216--226},
}

@inproceedings{sun_efficient_2018,
	title = {Efficient dynamic analysis for {Node}.js},
	url = {https://doi.org/10.1145/3178372.3179527},
	doi = {10.1145/3178372.3179527},
	booktitle = {Proceedings of the 27th {International} {Conference} on {Compiler} {Construction}, {CC} 2018, {February} 24-25, 2018, {Vienna}, {Austria}},
	author = {Sun, Haiyang and Bonetta, Daniele and Humer, Christian and Binder, Walter},
	year = {2018},
	pages = {196--206},
}

@phdthesis{loring_practical_2021,
	type = {{PhD} {Thesis}},
	title = {Practical {Dynamic} {Symbolic} {Execution} for {JavaScript}},
	language = {en},
	school = {Royal Holloway, University of London},
	author = {Loring, Blake William},
	year = {2021},
}

@techreport{herb_krasner_cost_2022,
	title = {The {Cost} of {Poor} {Software} {Qualitiy} in the {US}: {A} 2022 {Report}},
	url = {https://www.it-cisq.org/wp-content/uploads/sites/6/2022/11/CPSQ-Report-Nov-22-2.pdf},
	urldate = {2025-04-08},
	institution = {Consortium for Information \& Software Quality},
	author = {{Herb Krasner}},
	month = dec,
	year = {2022},
}

@misc{joel_montvelliksy_state_2024,
	title = {State of {Testing} {Report}},
	url = {https://www.practitest.com/assets/pdf/stot-2024.pdf},
	urldate = {2025-04-08},
	publisher = {PractiTest},
	author = {{Joel Montvelliksy} and {Lalitkumar Bhamare}},
	year = {2024},
}

@article{boehm_verifying_1984,
	title = {Verifying and {Validating} {Software} {Requirements} and {Design} {Specifications}},
	volume = {1},
	url = {https://doi.org/10.1109/MS.1984.233702},
	doi = {10.1109/MS.1984.233702},
	number = {1},
	journal = {IEEE Softw.},
	author = {Boehm, Barry W.},
	year = {1984},
	pages = {75--88},
}

@inproceedings{van_sprundel_fuzzing_2005,
	address = {Berlin, Germany},
	title = {Fuzzing: {Breaking} software in an automated fashion},
	url = {https://events.ccc.de/congress/2005/fahrplan/attachments/582-paper_fuzzing.pdf},
	booktitle = {Chaos {Communication} {Congress}},
	publisher = {Chaos Computer Club},
	author = {van Sprundel, Ilja},
	month = aug,
	year = {2005},
}

@inproceedings{silva_grasp_1996,
	title = {{GRASP} - a new search algorithm for satisfiability},
	url = {https://doi.org/10.1109/ICCAD.1996.569607},
	doi = {10.1109/ICCAD.1996.569607},
	booktitle = {Proceedings of the 1996 {IEEE}/{ACM} {International} {Conference} on {Computer}-{Aided} {Design}, {ICCAD} 1996, {San} {Jose}, {CA}, {USA}, {November} 10-14, 1996},
	author = {Silva, João P. Marques and Sakallah, Karem A.},
	year = {1996},
	pages = {220--227},
}

@inproceedings{ryan_sylvia_2023,
	title = {Sylvia: {Countering} the {Path} {Explosion} {Problem} in the {Symbolic} {Execution} of {Hardware} {Designs}},
	url = {https://doi.org/10.34727/2023/isbn.978-3-85448-060-0\_19},
	doi = {10.34727/2023/ISBN.978-3-85448-060-0_19},
	booktitle = {Formal {Methods} in {Computer}-{Aided} {Design}, {FMCAD} 2023, {Ames}, {IA}, {USA}, {October} 24-27, 2023},
	author = {Ryan, Kaki and Sturton, Cynthia},
	year = {2023},
	pages = {110--121},
}

@inproceedings{paul_end--end_2001,
	title = {End-to-{End} {Integration} {Testing}},
	url = {https://doi.org/10.1109/APAQS.2001.990022},
	doi = {10.1109/APAQS.2001.990022},
	booktitle = {2nd {Asia}-{Pacific} {Conference} on {Quality} {Software} ({APAQS} 2001), 10-11 {December} 2001, {Hong} {Kong}, {China}, {Proceedings}},
	author = {Paul, Raymond A.},
	year = {2001},
	pages = {211--222},
}

@techreport{barrett_smt-lib_2025,
	title = {The {SMT}-{LIB} {Standard}: {Version} 2.7},
	institution = {Department of Computer Science, The University of Iowa},
	author = {Barrett, Clark and Fontaine, Pascal and Tinelli, Cesare},
	year = {2025},
}

@article{loring_systematic_2021,
	title = {Systematic {Generation} of {Conformance} {Tests} for {JavaScript}},
	volume = {abs/2108.07075},
	url = {https://arxiv.org/abs/2108.07075},
	journal = {CoRR},
	author = {Loring, Blake and Kinder, Johannes},
	year = {2021},
	note = {arXiv: 2108.07075},
}

@inproceedings{kuznetsov_efficient_2012,
	title = {Efficient state merging in symbolic execution},
	url = {https://doi.org/10.1145/2254064.2254088},
	doi = {10.1145/2254064.2254088},
	booktitle = {{ACM} {SIGPLAN} {Conference} on {Programming} {Language} {Design} and {Implementation}, {PLDI} '12, {Beijing}, {China} - {June} 11 - 16, 2012},
	author = {Kuznetsov, Volodymyr and Kinder, Johannes and Bucur, Stefan and Candea, George},
	year = {2012},
	pages = {193--204},
}

@article{nieuwenhuis_solving_2006,
	title = {Solving {SAT} and {SAT} {Modulo} {Theories}: {From} an abstract {Davis}–{Putnam}–{Logemann}–{Loveland} procedure to {DPLL}({T})},
	volume = {53},
	issn = {0004-5411},
	url = {https://doi.org/10.1145/1217856.1217859},
	doi = {10.1145/1217856.1217859},
	abstract = {We first introduce Abstract DPLL, a rule-based formulation of the Davis–Putnam–Logemann–Loveland (DPLL) procedure for propositional satisfiability. This abstract framework allows one to cleanly express practical DPLL algorithms and to formally reason about them in a simple way. Its properties, such as soundness, completeness or termination, immediately carry over to the modern DPLL implementations with features such as backjumping or clause learning.We then extend the framework to Satisfiability Modulo background Theories (SMT) and use it to model several variants of the so-called lazy approach for SMT. In particular, we use it to introduce a few variants of a new, efficient and modular approach for SMT based on a general DPLL(X) engine, whose parameter X can be instantiated with a specialized solver SolverT for a given theory T, thus producing a DPLL(T) system. We describe the high-level design of DPLL(X) and its cooperation with SolverT, discuss the role of theory propagation, and describe different DPLL(T) strategies for some theories arising in industrial applications.Our extensive experimental evidence, summarized in this article, shows that DPLL(T) systems can significantly outperform the other state-of-the-art tools, frequently even in orders of magnitude, and have better scaling properties.},
	number = {6},
	journal = {J. ACM},
	author = {Nieuwenhuis, Robert and Oliveras, Albert and Tinelli, Cesare},
	month = nov,
	year = {2006},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {SAT solvers, Satisfiability Modulo Theories},
	pages = {937--977},
}

@inproceedings{moskewicz_chaff_2001,
	title = {Chaff: {Engineering} an {Efficient} {SAT} {Solver}},
	url = {https://doi.org/10.1145/378239.379017},
	doi = {10.1145/378239.379017},
	booktitle = {Proceedings of the 38th {Design} {Automation} {Conference}, {DAC} 2001, {Las} {Vegas}, {NV}, {USA}, {June} 18-22, 2001},
	author = {Moskewicz, Matthew W. and Madigan, Conor F. and Zhao, Ying and Zhang, Lintao and Malik, Sharad},
	year = {2001},
	pages = {530--535},
}

@incollection{boyer_integrating_1988,
	address = {USA},
	title = {Integrating decision procedures into heuristic theorem provers: a case study of linear arithmetic},
	isbn = {0-19-853718-2},
	booktitle = {Machine {Intelligence} 11},
	publisher = {Oxford University Press, Inc.},
	author = {Boyer, R. S. and Moore, J. S.},
	year = {1988},
	pages = {83--124},
}

@article{cadar_exe_2008,
	title = {{EXE}: {Automatically} {Generating} {Inputs} of {Death}},
	volume = {12},
	url = {https://doi.org/10.1145/1455518.1455522},
	doi = {10.1145/1455518.1455522},
	number = {2},
	journal = {ACM Trans. Inf. Syst. Secur.},
	author = {Cadar, Cristian and Ganesh, Vijay and Pawlowski, Peter M. and Dill, David L. and Engler, Dawson R.},
	year = {2008},
	pages = {10:1--10:38},
}

@inproceedings{godefroid_dart_2005,
	title = {{DART}: directed automated random testing},
	url = {https://doi.org/10.1145/1065010.1065036},
	doi = {10.1145/1065010.1065036},
	booktitle = {Proceedings of the {ACM} {SIGPLAN} 2005 {Conference} on {Programming} {Language} {Design} and {Implementation}, {Chicago}, {IL}, {USA}, {June} 12-15, 2005},
	author = {Godefroid, Patrice and Klarlund, Nils and Sen, Koushik},
	year = {2005},
	pages = {213--223},
}

@inproceedings{godefroid_automated_2008,
	title = {Automated {Whitebox} {Fuzz} {Testing}},
	url = {https://www.ndss-symposium.org/ndss2008/automated-whitebox-fuzz-testing/},
	booktitle = {Proceedings of the {Network} and {Distributed} {System} {Security} {Symposium}, {NDSS} 2008, {San} {Diego}, {California}, {USA}, 10th {February} - 13th {February} 2008},
	author = {Godefroid, Patrice and Levin, Michael Y. and Molnar, David A.},
	year = {2008},
}

@article{godefroid_automated_nodate,
	title = {Automated {Whitebox} {Fuzz} {Testing}},
	abstract = {Fuzz testing is an effective technique for ﬁnding security vulnerabilities in software. Traditionally, fuzz testing tools apply random mutations to well-formed inputs of a program and test the resulting values. We present an alternative whitebox fuzz testing approach inspired by recent advances in symbolic execution and dynamic test generation. Our approach records an actual run of the program under test on a well-formed input, symbolically evaluates the recorded trace, and gathers constraints on inputs capturing how the program uses these. The collected constraints are then negated one by one and solved with a constraint solver, producing new inputs that exercise different control paths in the program. This process is repeated with the help of a code-coverage maximizing heuristic designed to ﬁnd defects as fast as possible. We have implemented this algorithm in SAGE (Scalable, Automated, Guided Execution), a new tool employing x86 instruction-level tracing and emulation for whitebox fuzzing of arbitrary ﬁle-reading Windows applications. We describe key optimizations needed to make dynamic test generation scale to large input ﬁles and long execution traces with hundreds of millions of instructions. We then present detailed experiments with several Windows applications. Notably, without any format-speciﬁc knowledge, SAGE detects the MS07-017 ANI vulnerability, which was missed by extensive blackbox fuzzing and static analysis tools. Furthermore, while still in an early stage of development, SAGE has already discovered 30+ new bugs in large shipped Windows applications including image processors, media players, and ﬁle decoders. Several of these bugs are potentially exploitable memory access violations.},
	language = {en},
	author = {Godefroid, Patrice and Levin, Michael Y and Molnar, David},
}

@article{umar_comparative_2021,
	title = {A {Comparative} {Study} {Of} {Dynamic} {Software} {Testing} {Techniques}},
	volume = {12},
	doi = {10.35444/IJANA.2020.12301},
	author = {Umar, Mubarak Albarka and Chen, Zhanfang},
	month = jan,
	year = {2021},
	pages = {4575--4584},
}

@article{artho_combined_2005,
	title = {Combined {Static} and {Dynamic} {Analysis}},
	volume = {131},
	issn = {1571-0661},
	url = {https://www.sciencedirect.com/science/article/pii/S1571066105002537},
	doi = {https://doi.org/10.1016/j.entcs.2005.01.018},
	abstract = {Static analysis is usually faster than dynamic analysis but less precise. Therefore it is often desirable to retain information from static analysis for run-time verification, or to compare the results of both techniques. However, this requires writing two programs, which may not act identically under the same conditions. It would be desirable to share the same generic algorithm by static and dynamic analysis. In JNuke, a framework for static and dynamic analysis of Java programs, this has been achieved. By keeping the architecture of static analysis similar to a virtual machine, the only key difference between abstract interpretation and execution remains the nature of program states. In dynamic analysis, concrete states are available, while in static analysis, sets of (abstract) states are considered. Our new analysis is generic because it can re-use the same algorithm in static analysis and dynamic analysis. This paper describes the architecture of such a generic analysis. To our knowledge, JNuke is the first tool that has achieved this integration, which enables static and dynamic analysis to interact in novel ways.},
	journal = {Electronic Notes in Theoretical Computer Science},
	author = {Artho, Cyrille and Biere, Armin},
	year = {2005},
	keywords = {Java, Static analysis, dynamic analysis},
	pages = {3--14},
}

@article{umar_comparative_2020,
	title = {A {Comparative} {Study} {Of} {Dynamic} {Software} {Testing} {Techniques}},
	volume = {12},
	issn = {09750290, 09750282},
	url = {https://www.ijana.in/download12-3-1.php?file=V12I3-1.pdf},
	doi = {10.35444/IJANA.2020.12301},
	language = {en},
	number = {03},
	urldate = {2025-04-05},
	journal = {International Journal of Advanced Networking and Applications},
	author = {Umar, Mubarak Albarka and Zhanfang, Chen},
	year = {2020},
	pages = {4575--4584},
}

@article{hennell_comparison_1990,
	title = {A {Comparison} of {Static} and {Dynamic} {Conformance} {Analyses}},
	volume = {23},
	issn = {1474-6670},
	url = {https://www.sciencedirect.com/science/article/pii/S1474667017521886},
	doi = {https://doi.org/10.1016/S1474-6670(17)52188-6},
	abstract = {The paper examines the strengths and weaknesses of Static and Dynamic Analysis when used to show conformance with a set of assertions.},
	number = {6},
	journal = {IFAC Proceedings Volumes},
	author = {Hennell, M. A. and Fergus, E.},
	year = {1990},
	keywords = {Dynamic Analysis \& Conformance, Static Analysis},
	pages = {119--124},
}

@misc{openai_chatgpt_2025,
	title = {{ChatGPT}},
	url = {https://chat.openai.com/chat.},
	author = {{OpenAI}},
	month = mar,
	year = {2025},
}

@inproceedings{xu_symbolic_2024,
	title = {Symbolic {Execution} with {Test} {Cases} {Generated} by {Large} {Language} {Models}},
	doi = {10.1109/QRS62785.2024.00031},
	booktitle = {2024 {IEEE} 24th {International} {Conference} on {Software} {Quality}, {Reliability} and {Security} ({QRS})},
	author = {Xu, Jiahe and Xu, Jingwei and Chen, Taolue and Ma, Xiaoxing},
	year = {2024},
	keywords = {Codes, Large language models, Manuals, Pipelines, Reliability engineering, Software quality, Software reliability, large language model, software testing, symbolic execution},
	pages = {228--237},
}

@inproceedings{trabish_chopped_2018,
	address = {Gothenburg Sweden},
	title = {Chopped symbolic execution},
	isbn = {978-1-4503-5638-1},
	url = {https://dl.acm.org/doi/10.1145/3180155.3180251},
	doi = {10.1145/3180155.3180251},
	abstract = {Symbolic execution is a powerful program analysis technique that systematically explores multiple program paths. However, despite important technical advances, symbolic execution often struggles to reach deep parts of the code due to the well-known path explosion problem and constraint solving limitations.},
	language = {en},
	urldate = {2025-03-31},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Trabish, David and Mattavelli, Andrea and Rinetzky, Noam and Cadar, Cristian},
	month = may,
	year = {2018},
	pages = {350--360},
}

@article{cadar_exe_nodate,
	title = {{EXE}: {Automatically} {Generating} {Inputs} of {Death}},
	abstract = {This paper presents EXE, an eﬀective bug-ﬁnding tool that automatically generates inputs that crash real code. Instead of running code on manually or randomly constructed input, EXE runs it on symbolic input initially allowed to be “anything.” As checked code runs, EXE tracks the constraints on each symbolic (i.e., input-derived) memory location. If a statement uses a symbolic value, EXE does not run it, but instead adds it as an input-constraint; all other statements run as usual. If code conditionally checks a symbolic expression, EXE forks execution, constraining the expression to be true on the true branch and false on the other. Because EXE reasons about all possible values on a path, it has much more power than a traditional runtime tool: (1) it can force execution down any feasible program path and (2) at dangerous operations (e.g., a pointer dereference), it detects if the current path constraints allow any value that causes a bug. When a path terminates or hits a bug, EXE automatically generates a test case by solving the current path constraints to ﬁnd concrete values using its own codesigned constraint solver, STP. Because EXE’s constraints have no approximations, feeding this concrete input to an uninstrumented version of the checked code will cause it to follow the same path and hit the same bug (assuming deterministic code).},
	language = {en},
	author = {Cadar, Cristian and Ganesh, Vijay and Pawlowski, Peter M and Dill, David L and Engler, Dawson R},
}

@article{chipounov_selective_nodate,
	title = {Selective {Symbolic} {Execution}},
	abstract = {Symbolic execution is a powerful technique for analyzing program behavior, ﬁnding bugs, and generating tests, but suffers from severely limited scalability: the largest programs that can be symbolically executed today are on the order of thousands of lines of code. To ensure feasibility of symbolic execution, even small programs must curtail their interactions with libraries, the operating system, and hardware devices. This paper introduces selective symbolic execution, a technique for creating the illusion of fullsystem symbolic execution, while symbolically running only the code that is of interest to the developer. We describe a prototype that can symbolically execute arbitrary portions of a full system, including applications, libraries, operating system, and device drivers. It seamlessly transitions back and forth between symbolic and concrete execution, while transparently converting system state from symbolic to concrete and back. Our technique makes symbolic execution practical for large software that runs in real environments, without requiring explicit modeling of these environments.},
	language = {en},
	author = {Chipounov, Vitaly and Georgescu, Vlad and Zamﬁr, Cristian and Candea, George},
}

@article{barrett_smt-lib_nodate,
	title = {The {SMT}-{LIB} {Standard}},
	language = {en},
	author = {Barrett, Clark and Stump, Aaron},
}

@incollection{hutchison_smtinterpol_2012,
	address = {Berlin, Heidelberg},
	title = {{SMTInterpol}: {An} {Interpolating} {SMT} {Solver}},
	volume = {7385},
	isbn = {978-3-642-31758-3 978-3-642-31759-0},
	shorttitle = {{SMTInterpol}},
	url = {http://link.springer.com/10.1007/978-3-642-31759-0_19},
	abstract = {Craig interpolation is an active research topic and has become a powerful technique in veriﬁcation. We present SMTInterpol, an interpolating SMT solver for the quantiﬁer-free fragment of the combination of the theory of uninterpreted functions and the theory of linear arithmetic over integers and reals. SMTInterpol is SMTLIB 2 compliant and available under an open source software license (LGPL v3).},
	language = {en},
	urldate = {2025-03-30},
	booktitle = {Model {Checking} {Software}},
	publisher = {Springer Berlin Heidelberg},
	author = {Christ, Jürgen and Hoenicke, Jochen and Nutz, Alexander},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Donaldson, Alastair and Parker, David},
	year = {2012},
	doi = {10.1007/978-3-642-31759-0_19},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {248--254},
}

@inproceedings{barbosa_cvc5_2022,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {cvc5: {A} {Versatile} and {Industrial}-{Strength} {SMT} {Solver}},
	volume = {13243},
	url = {https://doi.org/10.1007/978-3-030-99524-9\_24},
	doi = {10.1007/978-3-030-99524-9_24},
	booktitle = {Tools and {Algorithms} for the {Construction} and {Analysis} of {Systems} - 28th {International} {Conference}, {TACAS} 2022, {Held} as {Part} of the {European} {Joint} {Conferences} on {Theory} and {Practice} of {Software}, {ETAPS} 2022, {Munich}, {Germany}, {April} 2-7, 2022, {Proceedings}, {Part} {I}},
	publisher = {Springer},
	author = {Barbosa, Haniel and Barrett, Clark W. and Brain, Martin and Kremer, Gereon and Lachnitt, Hanna and Mann, Makai and Mohamed, Abdalrhman and Mohamed, Mudathir and Niemetz, Aina and Nötzli, Andres and Ozdemir, Alex and Preiner, Mathias and Reynolds, Andrew and Sheng, Ying and Tinelli, Cesare and Zohar, Yoni},
	editor = {Fisman, Dana and Rosu, Grigore},
	year = {2022},
	pages = {415--442},
}

@incollection{biere_chapter_2021,
	title = {Chapter 33. {Satisfiability} {Modulo} {Theories}},
	isbn = {978-1-64368-160-3 978-1-64368-161-0},
	url = {http://ebooks.iospress.nl/doi/10.3233/FAIA201017},
	abstract = {Applications in artificial intelligence, formal verification, and other areas have greatly benefited from the recent advances in SAT. It is often the case, however, that applications in these fields require determining the satisfiability of formulas in more expressive logics such as first-order logic. Also, these applications typically require not general first-order satisfiability, but rather satisfiability with respect to some background theory, which fixes the interpretations of certain predicate and function symbols. For many background theories, specialized methods yield decision procedures for the satisfiability of quantifier-free formulas or some subclass thereof. Specialized decision procedures have been discovered for a long and still growing list of theories with practical applications. These include the theory of equality, various theories of arithmetic, and certain theories of arrays, as well as theories of lists, tuples, records, and bit-vectors of a fixed or arbitrary finite size. The research field concerned with determining the satisfiability of formulas with respect to some background theory is called Satisfiability Modulo Theories (SMT). This chapter provides a brief overview of SMT together with references to the relevant literature for a deeper study. It begins with an overview of techniques for solving SMT problems by encodings to SAT. The rest of the chapter is mostly concerned with an alternative approach in which a SAT solver is integrated with a separate decision procedure (called a theory solver) for conjunctions of literals in the background theory. After presenting this approach as a whole, the chapter provides more details on how to construct and combine theory solvers, and discusses several extensions and enhancements.},
	language = {en},
	urldate = {2025-03-30},
	booktitle = {Frontiers in {Artificial} {Intelligence} and {Applications}},
	publisher = {IOS Press},
	author = {Barrett, Clark and Sebastiani, Roberto and Seshia, Sanjit A. and Tinelli, Cesare},
	editor = {Biere, Armin and Heule, Marijn and Van Maaren, Hans and Walsh, Toby},
	month = feb,
	year = {2021},
	doi = {10.3233/FAIA201017},
}

@article{nieuwenhuis_solving_nodate,
	title = {Solving {SAT} and {SAT} {Modulo} {Theories}: from an {Abstract} {Davis}-{Putnam}-{Logemann}-{Loveland} {Procedure} to {DPLL}({T})},
	abstract = {DPLL formalism, we discuss in a clean and uniform way properties such as soundness, completeness, and termination. These properties immediately carry over to modern DPLL implementations with features such as backjumping and learning.},
	language = {en},
	journal = {Journal of the ACM},
	author = {Nieuwenhuis, Robert and Oliveras, Albert},
}

@incollection{bonacina_strategy_2013,
	address = {Berlin, Heidelberg},
	title = {The {Strategy} {Challenge} in {SMT} {Solving}},
	volume = {7788},
	isbn = {978-3-642-36674-1 978-3-642-36675-8},
	url = {http://link.springer.com/10.1007/978-3-642-36675-8_2},
	abstract = {High-performance SMT solvers contain many tightly integrated, hand-crafted heuristic combinations of algorithmic proof methods. While these heuristic combinations tend to be highly tuned for known classes of problems, they may easily perform badly on classes of problems not anticipated by solver developers. This issue is becoming increasingly pressing as SMT solvers begin to gain the attention of practitioners in diverse areas of science and engineering. We present a challenge to the SMT community: to develop methods through which users can exert strategic control over core heuristic aspects of SMT solvers. We present evidence that the adaptation of ideas of strategy prevalent both within the Argonne and LCF theorem proving paradigms can go a long way towards realizing this goal.},
	language = {en},
	urldate = {2025-03-30},
	booktitle = {Automated {Reasoning} and {Mathematics}},
	publisher = {Springer Berlin Heidelberg},
	author = {De Moura, Leonardo and Passmore, Grant Olney},
	editor = {Bonacina, Maria Paola and Stickel, Mark E.},
	year = {2013},
	doi = {10.1007/978-3-642-36675-8_2},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {15--44},
}

@inproceedings{cadar_klee_2008,
	title = {{KLEE}: {Unassisted} and {Automatic} {Generation} of {High}-{Coverage} {Tests} for {Complex} {Systems} {Programs}},
	url = {http://www.usenix.org/events/osdi08/tech/full\_papers/cadar/cadar.pdf},
	booktitle = {8th {USENIX} {Symposium} on {Operating} {Systems} {Design} and {Implementation}, {OSDI} 2008, {December} 8-10, 2008, {San} {Diego}, {California}, {USA}, {Proceedings}},
	author = {Cadar, Cristian and Dunbar, Daniel and Engler, Dawson R.},
	year = {2008},
	pages = {209--224},
}

@article{chen_ostrich_nodate,
	title = {The {OSTRICH} {String} {Solver}},
	abstract = {This paper gives a high-level overview of the string solver OSTRICH version 1.2, a solver entering SMTCOMP 2022. For more details and theoretical results we refer to the full version of the paper [4] and to the website https://github.com/uuverifiers/ostrich.},
	language = {en},
	author = {Chen, Taolue and Masellis, Riccardo De and Flores-Lamas, Alejandro and Hague, Matthew and Han, Zhilei and Hu, Denghang and Kan, Shuanglong and Lin, Anthony W and Markgraf, Oliver and Rümmer, Philipp and Stjerna, Amanda and Wu, Zhilin},
}

@inproceedings{wassermann_static_2008,
	address = {Leipzig, Germany},
	title = {Static detection of cross-site scripting vulnerabilities},
	isbn = {978-1-60558-079-1},
	url = {http://portal.acm.org/citation.cfm?doid=1368088.1368112},
	doi = {10.1145/1368088.1368112},
	abstract = {Web applications function in many of our daily activities, but they often have security problems, and their accessibility makes them easy to exploit. In cross-site scripting (XSS), an attacker exploits the trust a web client (browser) has for a trusted server and executes injected script on the browser with the server’s privileges. In 2006, XSS constituted the largest class of newly reported vulnerabilities making it the most prevalent class of attacks today. Web applications have XSS vulnerabilities because the validation they perform on untrusted input does not suﬃce to prevent that input from invoking a browser’s JavaScript (or other script) interpreter, and this validation is particularly diﬃcult to get right if it must admit some HTML mark-up. Most existing approaches to ﬁnding XSS vulnerabilities are taint-based, and taint-based approaches assume input validation functions to be adequate, so they either miss real vulnerabilities or report many false positives.},
	language = {en},
	urldate = {2025-03-25},
	booktitle = {Proceedings of the 13th international conference on {Software} engineering  - {ICSE} '08},
	publisher = {ACM Press},
	author = {Wassermann, Gary and Su, Zhendong},
	year = {2008},
	pages = {171},
}

@article{patil_cross_2011,
	title = {Cross {Site} {Scripting}: {An} {Overview}},
	abstract = {This paper describes the security attacks and specially focuses on Cross Site Scripting attacks. It further also discusses types and several counter measures. The major problem faced by the web application is the parameter manipulation, through which the attackers are aiming to access the database. Generally web applications maintain same structure and value. In that, required information is being accessed by the identical variables and keywords through web parameters. Parameter manipulation is the major issue in the web application used by the attacker to manipulate the parameter being sent by the browser and executed by the server.},
	language = {en},
	journal = {Intelligent Systems},
	author = {Patil, Vishwajit S and Bamnote, Dr G R and Nair, Sanil S},
	year = {2011},
}

@misc{noauthor_owasp_2025,
	title = {{OWASP} {Top} {Ten}},
	shorttitle = {{OWASP} {Top} {Ten} {\textbar} {OWASP} {Foundation} https},
	url = {https://owasp.org/www-project-top-ten/},
	abstract = {The OWASP Top 10 is the reference standard for the most critical web application security risks. Adopting the OWASP Top 10 is perhaps the most effective first step towards changing your software development culture focused on producing secure code.},
	language = {en},
	urldate = {2025-03-25},
	journal = {owasp},
	month = mar,
	year = {2025},
}

@article{tang_identifying_2012,
	title = {Identifying {Cross}-{Site} {Scripting} {Attacks} {Based} on {URL} {Analysis}},
	volume = {2},
	doi = {10.5815/ijem.2012.05.08},
	journal = {International Journal of Engineering and Manufacturing},
	author = {Tang, Zhihua and Zheng, Ning and Xu, Ming},
	month = oct,
	year = {2012},
	pages = {52--61},
}

@misc{noauthor_cross_nodate,
	title = {Cross {Site} {Scripting} {Prevention} - {OWASP} {Cheat} {Sheet} {Series} https://cheatsheetseries.owasp.org/cheatsheets/{Cross}\_Site\_Scripting\_Prevention\_Cheat\_Sheet.html},
	url = {https://cheatsheetseries.owasp.org/cheatsheets/Cross_Site_Scripting_Prevention_Cheat_Sheet.html},
	urldate = {2025-03-25},
}

@inproceedings{kiezun_automatic_2009,
	title = {Automatic creation of {SQL} {Injection} and cross-site scripting attacks},
	url = {https://doi.org/10.1109/ICSE.2009.5070521},
	doi = {10.1109/ICSE.2009.5070521},
	booktitle = {31st {International} {Conference} on {Software} {Engineering}, {ICSE} 2009, {May} 16-24, 2009, {Vancouver}, {Canada}, {Proceedings}},
	author = {Kiezun, Adam and Guo, Philip J. and Jayaraman, Karthick and Ernst, Michael D.},
	year = {2009},
	pages = {199--209},
}

@inproceedings{kieyzun_automatic_2009,
	address = {Vancouver, BC, Canada},
	title = {Automatic creation of {SQL} {Injection} and cross-site scripting attacks},
	isbn = {978-1-4244-3453-4},
	url = {http://ieeexplore.ieee.org/document/5070521/},
	doi = {10.1109/ICSE.2009.5070521},
	abstract = {We present a technique for ﬁnding security vulnerabilities in Web applications. SQL Injection (SQLI) and crosssite scripting (XSS) attacks are widespread forms of attack in which the attacker crafts the input to the application to access or modify user data and execute malicious code. In the most serious attacks (called second-order, or persistent, XSS), an attacker can corrupt a database so as to cause subsequent users to execute malicious code.},
	language = {en},
	urldate = {2025-03-25},
	booktitle = {2009 {IEEE} 31st {International} {Conference} on {Software} {Engineering}},
	publisher = {IEEE},
	author = {Kieyzun, Adam and Guo, Philip J. and Jayaraman, Karthick and Ernst, Michael D.},
	year = {2009},
	pages = {199--209},
}

@article{moskewicz_chaff_nodate,
	title = {Chaff: {Engineering} an {Efficient} {SAT} {Solver}},
	abstract = {Boolean Satisfiability is probably the most studied of combinatorial optimization/search problems. Significant effort has been devoted to trying to provide practical solutions to this problem for problem instances encountered in a range of applications in Electronic Design Automation (EDA), as well as in Artificial Intelligence (AI). This study has culminated in the development of several SAT packages, both proprietary and in the public domain (e.g. GRASP, SATO) which find significant use in both research and industry. Most existing complete solvers are variants of the Davis-Putnam (DP) search algorithm. In this paper we describe the development of a new complete solver, Chaff, which achieves significant performance gains through careful engineering of all aspects of the search – especially a particularly efficient implementation of Boolean constraint propagation (BCP) and a novel low overhead decision strategy. Chaff has been able to obtain one to two orders of magnitude performance improvement on difficult SAT benchmarks in comparison with other solvers (DP or otherwise), including GRASP and SATO.},
	language = {en},
	author = {Moskewicz, Matthew W and Madigan, Conor F and Zhao, Ying and Zhang, Lintao and Malik, Sharad},
}

@article{biere_symbolic_nodate,
	title = {Symbolic {Model} {Checking} without {BDDs}?},
	abstract = {Symbolic Model Checking [3, 14] has proven to be a powerful technique for the veriﬁcation of reactive systems. BDDs [2] have traditionally been used as a symbolic representation of the system. In this paper we show how boolean decision procedures, like St˚almarck’s Method [16] or the Davis \& Putnam Procedure [7], can replace BDDs. This new technique avoids the space blow up of BDDs, generates counterexamples much faster, and sometimes speeds up the veriﬁcation. In addition, it produces counterexamples of minimal length. We introduce a bounded model checking procedure for LTL which reduces model checking to propositional satisﬁability. We show that bounded LTL model checking can be done without a tableau construction. We have implemented a model checker BMC, based on bounded model checking, and preliminary results are presented.},
	language = {en},
	author = {Biere, Armin and Cimatti, Alessandro and Clarke, Edmund and Zhu, Yunshan},
}

@article{silva_graspnew_nodate,
	title = {{GRASP}—{A} {New} {Search} {Algorithm} for {Satisﬁability}},
	language = {en},
	author = {Silva, João P Marques and Sakallah, Karem A},
}

@article{boyer_integrating_1988-1,
	title = {Integrating decision procedures into heuristic theorem provers: a case study of linear arithmetic},
	journal = {Machine intelligence},
	author = {Boyer, Robert S. and Moore, J. Strother},
	year = {1988},
	pages = {83--124},
}

@article{shostak_algorithm_1978,
	title = {An algorithm for reasoning about equality},
	volume = {21},
	issn = {0001-0782},
	url = {https://doi.org/10.1145/359545.359570},
	doi = {10.1145/359545.359570},
	abstract = {A simple technique for reasoning about equalities that is fast and complete for ground formulas with function symbols and equality is presented. A proof of correctness is given as well.},
	number = {7},
	journal = {Commun. ACM},
	author = {Shostak, Robert E.},
	month = jul,
	year = {1978},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {deduction, equality, program verification, theorem proving},
	pages = {583--585},
}

@article{nelson_fast_1980,
	title = {Fast {Decision} {Procedures} {Based} on {Congruence} {Closure}},
	volume = {27},
	issn = {0004-5411},
	url = {https://doi.org/10.1145/322186.322198},
	doi = {10.1145/322186.322198},
	abstract = {The notion of the congruence closure of a relation on a graph is defined and several algorithms for computing it are surveyed. A simple proof is given that the congruence closure algorithm provides a decision procedure for the quantifier-free theory of equality. A decision procedure is then given for the quantifier-free theory of LISP list structure based on the congruence closure algorithm. Both decision procedures determine the satisfiability of a conjunction of literals of length n in average time O(n log n) using the fastest known congruence closure algorithm. It is also shown that if the axiomatization of the theory of list structure is changed slightly, the problem of determining the satisfiability of a conjunction of literals becomes NP-complete. The decision procedures have been implemented in the authors' simplifier for the Stanford Pascal Verifier.},
	number = {2},
	journal = {J. ACM},
	author = {Nelson, Greg and Oppen, Derek C.},
	month = apr,
	year = {1980},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	pages = {356--364},
}

@inproceedings{paul_end--end_2001-1,
	title = {End-to-end integration testing},
	doi = {10.1109/APAQS.2001.990022},
	booktitle = {Proceedings {Second} {Asia}-{Pacific} {Conference} on {Quality} {Software}},
	author = {Paul, R.},
	year = {2001},
	keywords = {Collaboration, Investments, Java, Object oriented modeling, Project management, Risk analysis, Software quality, Software testing, Statistical analysis, System testing},
	pages = {211--220},
}

@book{beck_test-driven_2003,
	series = {The {Addison}-{Wesley} signature series},
	title = {Test-driven {Development} - by example},
	isbn = {978-0-321-14653-3},
	publisher = {Addison-Wesley},
	author = {Beck, Kent L.},
	year = {2003},
}

@incollection{hutchison_cute_2006,
	address = {Berlin, Heidelberg},
	title = {{CUTE} and {jCUTE}: {Concolic} {Unit} {Testing} and {Explicit} {Path} {Model}-{Checking} {Tools}},
	volume = {4144},
	isbn = {978-3-540-37406-0 978-3-540-37411-4},
	shorttitle = {{CUTE} and {jCUTE}},
	url = {http://link.springer.com/10.1007/11817963_38},
	abstract = {CUTE, a Concolic Unit Testing Engine for C and Java, is a tool to systematically and automatically test sequential C programs (including pointers) and concurrent Java programs. CUTE combines concrete and symbolic execution in a way that avoids redundant test cases as well as false warnings. The tool also introduces a race-ﬂipping technique to eﬃciently test and model check concurrent programs with data inputs.},
	language = {en},
	urldate = {2025-03-22},
	booktitle = {Computer {Aided} {Verification}},
	publisher = {Springer Berlin Heidelberg},
	author = {Sen, Koushik and Agha, Gul},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Ball, Thomas and Jones, Robert B.},
	year = {2006},
	doi = {10.1007/11817963_38},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {419--423},
}

@inproceedings{balasubramanian_dynamic_2019,
	address = {Limassol Cyprus},
	title = {Dynamic symbolic execution for the analysis of web server applications in {Java}},
	isbn = {978-1-4503-5933-7},
	url = {https://dl.acm.org/doi/10.1145/3297280.3297494},
	doi = {10.1145/3297280.3297494},
	abstract = {Symbolic execution is a well-known program analysis technique that explores multiple program paths simultaneously. Among other things, it is used to uncover subtle bugs and corner cases in programs, as well as to produce high-coverage test suites. Even though symbolic execution has seen successful use in practice, there remain challenges in applying it to programs like web servers that use features such as multithreading and callbacks. This paper describes our dynamic symbolic execution framework for Java that was designed with these types of features in mind. Our framework uses bytecode instrumentation combined with a run-time agent to perform the symbolic execution. We give a detailed description of the challenges we faced along with our design choices. We also present benchmark results on various examples including programs that use web server frameworks.},
	language = {en},
	urldate = {2025-03-22},
	booktitle = {Proceedings of the 34th {ACM}/{SIGAPP} {Symposium} on {Applied} {Computing}},
	publisher = {ACM},
	author = {Balasubramanian, Daniel and Zhang, Zhenkai and McDermet, Dan and Karsai, Gabor},
	month = apr,
	year = {2019},
	pages = {2178--2185},
}

@inproceedings{saxena_symbolic_2010,
	address = {USA},
	series = {{SP} '10},
	title = {A {Symbolic} {Execution} {Framework} for {JavaScript}},
	isbn = {978-0-7695-4035-1},
	url = {https://doi.org/10.1109/SP.2010.38},
	doi = {10.1109/SP.2010.38},
	abstract = {As AJAX applications gain popularity, client-side JavaScript code is becoming increasingly complex. However, few automated vulnerability analysis tools for JavaScript exist. In this paper, we describe the first system for exploring the execution space of JavaScript code using symbolic execution. To handle JavaScript code’s complex use of string operations, we design a new language of string constraints and implement a solver for it. We build an automatic end-to-end tool, Kudzu, and apply it to the problem of finding client-side code injection vulnerabilities. In experiments on 18 live web applications, Kudzu automatically discovers 2 previously unknown vulnerabilities and 9 more that were previously found only with a manually-constructed test suite.},
	booktitle = {Proceedings of the 2010 {IEEE} {Symposium} on {Security} and {Privacy}},
	publisher = {IEEE Computer Society},
	author = {Saxena, Prateek and Akhawe, Devdatta and Hanna, Steve and Mao, Feng and McCamant, Stephen and Song, Dawn},
	year = {2010},
	pages = {513--528},
}

@inproceedings{chipounov_s2e_2011,
	address = {New York, NY, USA},
	series = {{ASPLOS} {XVI}},
	title = {{S2E}: a platform for in-vivo multi-path analysis of software systems},
	isbn = {978-1-4503-0266-1},
	url = {https://doi.org/10.1145/1950365.1950396},
	doi = {10.1145/1950365.1950396},
	abstract = {This paper presents S2E, a platform for analyzing the properties and behavior of software systems. We demonstrate S2E's use in developing practical tools for comprehensive performance profiling, reverse engineering of proprietary software, and bug finding for both kernel-mode and user-mode binaries. Building these tools on top of S2E took less than 770 LOC and 40 person-hours each.S2E's novelty consists of its ability to scale to large real systems, such as a full Windows stack. S2E is based on two new ideas: selective symbolic execution, a way to automatically minimize the amount of code that has to be executed symbolically given a target analysis, and relaxed execution consistency models, a way to make principled performance/accuracy trade-offs in complex analyses. These techniques give S2E three key abilities: to simultaneously analyze entire families of execution paths, instead of just one execution at a time; to perform the analyses in-vivo within a real software stack–user programs, libraries, kernel, drivers, etc.–instead of using abstract models of these layers; and to operate directly on binaries, thus being able to analyze even proprietary software.Conceptually, S2E is an automated path explorer with modular path analyzers: the explorer drives the target system down all execution paths of interest, while analyzers check properties of each such path (e.g., to look for bugs) or simply collect information (e.g., count page faults). Desired paths can be specified in multiple ways, and S2E users can either combine existing analyzers to build a custom analysis tool, or write new analyzers using the S2E API.},
	booktitle = {Proceedings of the {Sixteenth} {International} {Conference} on {Architectural} {Support} for {Programming} {Languages} and {Operating} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Chipounov, Vitaly and Kuznetsov, Volodymyr and Candea, George},
	year = {2011},
	note = {event-place: Newport Beach, California, USA},
	keywords = {analysis, binary, consistency models, dbt, framework, in-vivo, performance, symbolic execution, testing, virtualization},
	pages = {265--278},
}

@article{chipounov_s2e_2011-1,
	title = {{S2E}: a platform for in-vivo multi-path analysis of software systems},
	volume = {39},
	issn = {0163-5964},
	url = {https://doi.org/10.1145/1961295.1950396},
	doi = {10.1145/1961295.1950396},
	abstract = {This paper presents S2E, a platform for analyzing the properties and behavior of software systems. We demonstrate S2E's use in developing practical tools for comprehensive performance profiling, reverse engineering of proprietary software, and bug finding for both kernel-mode and user-mode binaries. Building these tools on top of S2E took less than 770 LOC and 40 person-hours each.S2E's novelty consists of its ability to scale to large real systems, such as a full Windows stack. S2E is based on two new ideas: selective symbolic execution, a way to automatically minimize the amount of code that has to be executed symbolically given a target analysis, and relaxed execution consistency models, a way to make principled performance/accuracy trade-offs in complex analyses. These techniques give S2E three key abilities: to simultaneously analyze entire families of execution paths, instead of just one execution at a time; to perform the analyses in-vivo within a real software stack–user programs, libraries, kernel, drivers, etc.–instead of using abstract models of these layers; and to operate directly on binaries, thus being able to analyze even proprietary software.Conceptually, S2E is an automated path explorer with modular path analyzers: the explorer drives the target system down all execution paths of interest, while analyzers check properties of each such path (e.g., to look for bugs) or simply collect information (e.g., count page faults). Desired paths can be specified in multiple ways, and S2E users can either combine existing analyzers to build a custom analysis tool, or write new analyzers using the S2E API.},
	number = {1},
	journal = {SIGARCH Comput. Archit. News},
	author = {Chipounov, Vitaly and Kuznetsov, Volodymyr and Candea, George},
	month = mar,
	year = {2011},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {analysis, binary, consistency models, dbt, framework, in-vivo, performance, symbolic execution, testing, virtualization},
	pages = {265--278},
}

@inproceedings{bucur_prototyping_2014,
	address = {New York, NY, USA},
	series = {{ASPLOS} '14},
	title = {Prototyping symbolic execution engines for interpreted languages},
	isbn = {978-1-4503-2305-5},
	url = {https://doi.org/10.1145/2541940.2541977},
	doi = {10.1145/2541940.2541977},
	abstract = {Symbolic execution is being successfully used to automatically test statically compiled code. However, increasingly more systems and applications are written in dynamic interpreted languages like Python. Building a new symbolic execution engine is a monumental effort, and so is keeping it up-to-date as the target language evolves. Furthermore, ambiguous language specifications lead to their implementation in a symbolic execution engine potentially differing from the production interpreter in subtle ways.We address these challenges by flipping the problem and using the interpreter itself as a specification of the language semantics. We present a recipe and tool (called Chef) for turning a vanilla interpreter into a sound and complete symbolic execution engine. Chef symbolically executes the target program by symbolically executing the interpreter's binary while exploiting inferred knowledge about the program's high-level structure.Using Chef, we developed a symbolic execution engine for Python in 5 person-days and one for Lua in 3 person-days. They offer complete and faithful coverage of language features in a way that keeps up with future language versions at near-zero cost. Chef-produced engines are up to 1000 times more performant than if directly executing the interpreter symbolically without Chef.},
	booktitle = {Proceedings of the 19th {International} {Conference} on {Architectural} {Support} for {Programming} {Languages} and {Operating} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Bucur, Stefan and Kinder, Johannes and Candea, George},
	year = {2014},
	note = {event-place: Salt Lake City, Utah, USA},
	keywords = {interpreter instrumentation, software analysis optimizations, state selection strategies},
	pages = {239--254},
}

@article{bucur_prototyping_2014-1,
	title = {Prototyping symbolic execution engines for interpreted languages},
	volume = {49},
	issn = {0362-1340},
	url = {https://doi.org/10.1145/2644865.2541977},
	doi = {10.1145/2644865.2541977},
	abstract = {Symbolic execution is being successfully used to automatically test statically compiled code. However, increasingly more systems and applications are written in dynamic interpreted languages like Python. Building a new symbolic execution engine is a monumental effort, and so is keeping it up-to-date as the target language evolves. Furthermore, ambiguous language specifications lead to their implementation in a symbolic execution engine potentially differing from the production interpreter in subtle ways.We address these challenges by flipping the problem and using the interpreter itself as a specification of the language semantics. We present a recipe and tool (called Chef) for turning a vanilla interpreter into a sound and complete symbolic execution engine. Chef symbolically executes the target program by symbolically executing the interpreter's binary while exploiting inferred knowledge about the program's high-level structure.Using Chef, we developed a symbolic execution engine for Python in 5 person-days and one for Lua in 3 person-days. They offer complete and faithful coverage of language features in a way that keeps up with future language versions at near-zero cost. Chef-produced engines are up to 1000 times more performant than if directly executing the interpreter symbolically without Chef.},
	number = {4},
	journal = {SIGPLAN Not.},
	author = {Bucur, Stefan and Kinder, Johannes and Candea, George},
	month = feb,
	year = {2014},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {interpreter instrumentation, software analysis optimizations, state selection strategies},
	pages = {239--254},
}

@article{godefroid_fuzzing_2020,
	title = {Fuzzing: hack, art, and science},
	volume = {63},
	issn = {0001-0782, 1557-7317},
	shorttitle = {Fuzzing},
	url = {https://dl.acm.org/doi/10.1145/3363824},
	doi = {10.1145/3363824},
	abstract = {Reviewing software testing techniques for finding security vulnerabilities.},
	language = {en},
	number = {2},
	urldate = {2025-03-20},
	journal = {Communications of the ACM},
	author = {Godefroid, Patrice},
	month = jan,
	year = {2020},
	pages = {70--76},
}

@misc{oxford_dictionary_definition_nodate,
	title = {Definition {Reliable}},
	url = {https://www.oxfordlearnersdictionaries.com/definition/american_english/reliable},
	urldate = {2025-03-19},
	journal = {https://www.oxfordlearnersdictionaries.com/definition},
	author = {{Oxford Dictionary}},
}

@article{davis_computing_1960,
	title = {A {Computing} {Procedure} for {Quantification} {Theory}},
	volume = {7},
	url = {https://doi.org/10.1145/321033.321034},
	doi = {10.1145/321033.321034},
	number = {3},
	journal = {J. ACM},
	author = {Davis, Martin and Putnam, Hilary},
	year = {1960},
	pages = {201--215},
}

@article{davis_machine_1962,
	title = {A machine program for theorem-proving},
	volume = {5},
	issn = {0001-0782},
	url = {https://doi.org/10.1145/368273.368557},
	doi = {10.1145/368273.368557},
	abstract = {The programming of a proof procedure is discussed in connection with trial runs and possible improvements.},
	number = {7},
	journal = {Commun. ACM},
	author = {Davis, Martin and Logemann, George and Loveland, Donald},
	month = jul,
	year = {1962},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	pages = {394--397},
}

@article{davis_machine_nodate,
	title = {A machine program for theorem-proving},
	language = {en},
	author = {Davis, Martin and Logemann, George and Loveland, Donald},
}

@incollection{hutchison_computing_2014,
	address = {Cham},
	title = {Computing with an {SMT} {Solver}},
	volume = {8570},
	isbn = {978-3-319-09098-6 978-3-319-09099-3},
	url = {http://link.springer.com/10.1007/978-3-319-09099-3_2},
	abstract = {Satisﬁability modulo theories (SMT) solvers that support quantiﬁer instantiations via matching triggers can be programmed to give practical support for user-deﬁned theories. Care must be taken to avoid so-called matching loops, which may prevent termination of the solver. By design, such avoidance limits the extent to which the SMT solver is able to apply the deﬁnitions of user-deﬁned functions. For some inputs to these functions, however, it is instead desireable to allow unadulterated use of the functions; in particular, if it is known that evaluation will terminate.},
	language = {en},
	urldate = {2025-03-19},
	booktitle = {Tests and {Proofs}},
	publisher = {Springer International Publishing},
	author = {Amin, Nada and Leino, K. Rustan M. and Rompf, Tiark},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Kobsa, Alfred and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Terzopoulos, Demetri and Tygar, Doug and Weikum, Gerhard and Seidl, Martina and Tillmann, Nikolai},
	year = {2014},
	doi = {10.1007/978-3-319-09099-3_2},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {20--35},
}

@inproceedings{abraham_smt_2017,
	address = {Timisoara},
	title = {{SMT} {Solving} for {Arithmetic} {Theories}: {Theory} and {Tool} {Support}},
	isbn = {978-1-5386-2626-9},
	shorttitle = {{SMT} {Solving} for {Arithmetic} {Theories}},
	url = {https://ieeexplore.ieee.org/document/8531255/},
	doi = {10.1109/SYNASC.2017.00009},
	abstract = {Satisﬁability checking aims to develop algorithms and tools for checking the satisﬁability of logical formulas. Driven by the impressive success of SAT solvers for propositional logic, Satisﬁability-Modulo-Theories (SMT) solvers were developed to extend the scope also to different theories. Today, SMT solving is widely used in many applications, for example veriﬁcation, synthesis, planning or product design automation. In this tutorial paper we give a short introduction to the foundations of SMT solving, describe some popular SMT solvers and illustrate their usage. We also present our own solver SMT-RAT, which was developed to support the strategic combination of different decision procedures, putting a focus on arithmetic theories.},
	language = {en},
	urldate = {2025-03-19},
	booktitle = {2017 19th {International} {Symposium} on {Symbolic} and {Numeric} {Algorithms} for {Scientific} {Computing} ({SYNASC})},
	publisher = {IEEE},
	author = {Abraham, Erika and Kremer, Gereon},
	month = sep,
	year = {2017},
	pages = {1--8},
}

@misc{loring_systematic_2021-1,
	title = {Systematic {Generation} of {Conformance} {Tests} for {JavaScript}},
	url = {http://arxiv.org/abs/2108.07075},
	doi = {10.48550/arXiv.2108.07075},
	abstract = {JavaScript implementations are tested for conformance to the ECMAScript standard using a large hand-written test suite. Not only in this a tedious approach, it also relies solely on the natural language specification for differentiating behaviors, while hidden implementation details can also affect behavior and introduce divergences. We propose to generate conformance tests through dynamic symbolic execution of polyfills, drop-in replacements for newer JavaScript language features that are not yet widely supported. We then run these generated tests against multiple implementations of JavaScript, using a majority vote to identify the correct behavior. To facilitate test generation for polyfill code, we introduce a model for structured symbolic inputs that is suited to the dynamic nature of JavaScript. In our evaluation, we found 17 divergences in the widely used core-js polyfill and were able to increase branch coverage in interpreter code by up to 15\%. Because polyfills are typically written even before standardization, our approach will allow to maintain and extend standardization test suites with reduced effort.},
	language = {en},
	urldate = {2025-03-18},
	publisher = {arXiv},
	author = {Loring, Blake and Kinder, Johannes},
	month = aug,
	year = {2021},
	note = {arXiv:2108.07075 [cs]},
	keywords = {Computer Science - Programming Languages, Computer Science - Software Engineering},
}

@inproceedings{he_learning_2021,
	address = {New York, NY, USA},
	series = {{CCS} '21},
	title = {Learning to {Explore} {Paths} for {Symbolic} {Execution}},
	isbn = {978-1-4503-8454-4},
	url = {https://doi.org/10.1145/3460120.3484813},
	doi = {10.1145/3460120.3484813},
	abstract = {Symbolic execution is a powerful technique that can generate tests steering program execution into desired paths. However, the scalability of symbolic execution is often limited by path explosion, i.e., the number of symbolic states representing the paths under exploration quickly explodes as execution goes on. Therefore, the effectiveness of symbolic execution engines hinges on the ability to select and explore the right symbolic states.In this work, we propose a novel learning-based strategy, called Learch, able to effectively select promising states for symbolic execution to tackle the path explosion problem. Learch directly estimates the contribution of each state towards the goal of maximizing coverage within a time budget, as opposed to relying on manually crafted heuristics based on simple statistics as a crude proxy for the objective. Moreover, Learch leverages existing heuristics in training data generation and feature extraction, and can thus benefit from any new expert-designed heuristics. We instantiated Learch in KLEE, a widely adopted symbolic execution engine. We evaluated Learch on a diverse set of programs, showing that Learch is practically effective: it covers more code and detects more security violations than existing manual heuristics, as well as combinations of those heuristics. We also show that using tests generated by Learch as initial fuzzing seeds enables the popular fuzzer AFL to find more paths and security violations.},
	urldate = {2025-03-18},
	booktitle = {Proceedings of the 2021 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {Association for Computing Machinery},
	author = {He, Jingxuan and Sivanrupan, Gishor and Tsankov, Petar and Vechev, Martin},
	month = nov,
	year = {2021},
	pages = {2526--2540},
}

@article{miao_extensive_nodate,
	title = {An {Extensive} {Empirical} {Study} of {Nondeterministic} {Behavior} in {Static} {Analysis} {Tools}},
	abstract = {Recent research has studied the importance and identified causes of nondeterminism in software. Static analysis tools exhibit many risk factors for nondeterministic behavior, but no work has analyzed the occurrence of such behavior in these tools. To bridge this gap, we perform an extensive empirical study aiming to understand past and ongoing nondeterminism in 12 popular, open-source static analysis tools that target 5 types of projects. We first conduct a qualitative study to understand the extent to which nondeterministic behavior has been found and addressed within the tools under study, and find results in 7 tool repositories. After classifying the issues and commits by root cause, we find that the majority of nondeterminisms are caused by concurrency issues, incorrect analysis logic, or assumed orderings of unordered data structures, which have shared patterns. We also perform a quantitative analysis, where we use two strategies and diverse input programs and configurations to detect yet-unknown nondeterministic behaviors. We discover such behavior in 8 out of the 12 tools, including 3 which had no results from the qualitative analysis. We find that nondeterminism often appears in multiple configurations on a variety of input programs. We communicated all identified nondeterminism to the developers, and received confirmation of five tools. Finally, we detail a case study of fixing FlowDroid’s nondeterministic behavior.},
	language = {en},
	author = {Miao, Miao and Mordahl, Austin},
}

@article{ryan_countering_nodate,
	title = {Countering the {Path} {Explosion} {Problem} in the {Symbolic} {Execution} of {Hardware} {Designs}},
	abstract = {Symbolic execution is a powerful veriﬁcation tool for hardware designs, but suffers from the path explosion problem. We introduce a new approach, piecewise composition, which leverages the modular structure of hardware to transfer the work of path exploration to SMT solvers. We present a symbolic execution engine implementing the technique. The engine operates directly over register transfer level (RTL) Verilog designs without requiring translation to a netlist or software simulation. In our evaluation, piecewise composition reduces the number of paths explored by an order of magnitude and reduces the runtime by 97\%. Using 84 properties from the literature we ﬁnd assertion violations in 5 open-source designs including an SoC and CPU.},
	language = {en},
	author = {Ryan, Kaki and Sturton, Cynthia},
}

@article{kuznetsov_efficient_nodate,
	title = {Efficient {State} {Merging} in {Symbolic} {Execution}},
	abstract = {Symbolic execution has proven to be a practical technique for building automated test case generation and bug ﬁnding tools. Nevertheless, due to state explosion, these tools still struggle to achieve scalability. Given a program, one way to reduce the number of states that the tools need to explore is to merge states obtained on different paths. Alas, doing so increases the size of symbolic path conditions (thereby stressing the underlying constraint solver) and interferes with optimizations of the exploration process (also referred to as search strategies). The net effect is that state merging may actually lower performance rather than increase it.},
	language = {en},
	author = {Kuznetsov, Volodymyr and Kinder, Johannes and Bucur, Stefan and Candea, George},
}

@book{martin_managing_1983,
	address = {USA},
	edition = {1st},
	title = {Managing the {Data} {Base} {Environment}},
	isbn = {0-13-550582-8},
	publisher = {Prentice Hall PTR},
	author = {Martin, James},
	year = {1983},
}

@inproceedings{bisht_xss-guard_2008,
	address = {Berlin, Heidelberg},
	series = {{DIMVA} '08},
	title = {{XSS}-{GUARD}: {Precise} {Dynamic} {Prevention} of {Cross}-{Site} {Scripting} {Attacks}},
	isbn = {978-3-540-70541-3},
	url = {https://doi.org/10.1007/978-3-540-70542-0_2},
	doi = {10.1007/978-3-540-70542-0_2},
	abstract = {This paper focuses on defense mechanisms for cross-site scripting attacks, the top threat on web applications today. It is believed that input validation (or filtering) can effectively prevent XSS attacks on the server side. In this paper, we discuss several recent real-world XSS attacks and analyze the reasons for the failure of filtering mechanisms in defending these attacks. We conclude that while filtering is useful as a first level of defense against XSS attacks, it is ineffective in preventing several instances of attack, especially when user input includes content-rich HTML. We then propose XSS-Guard , a new framework that is designed to be a prevention mechanism against XSS attacks on the server side. XSS-Guard works by dynamically learning the set of scripts that a web application intends to create for any HTML request. Our approach also includes a robust mechanism for identifying scripts at the server side and removes any script in the output that is not intended by the web application. We discuss extensive experimental results that demonstrate the resilience of XSS-Guard in preventing a number of real-world XSS exploits.},
	booktitle = {Proceedings of the 5th {International} {Conference} on {Detection} of {Intrusions} and {Malware}, and {Vulnerability} {Assessment}},
	publisher = {Springer-Verlag},
	author = {Bisht, Prithvi and Venkatakrishnan, V. N.},
	year = {2008},
	note = {event-place: Paris, France},
	keywords = {Attack Prevention, Cross-site scripting (XSS), Filtering, Security},
	pages = {23--43},
}

@book{hutchison_detection_2008,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Detection of {Intrusions} and {Malware}, and {Vulnerability} {Assessment}: 5th {International} {Conference}, {DIMVA} 2008, {Paris}, {France}, {July} 10-11, 2008. {Proceedings}},
	isbn = {978-3-540-70541-3 978-3-540-70542-0},
	shorttitle = {Detection of {Intrusions} and {Malware}, and {Vulnerability} {Assessment}},
	language = {en},
	number = {5137},
	publisher = {Springer Berlin Heidelberg},
	editor = {Hutchison, David and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Zamboni, Diego and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C.},
	year = {2008},
	doi = {10.1007/978-3-540-70542-0},
}

@article{miller_empirical_1990,
	title = {An empirical study of the reliability of {UNIX} utilities},
	volume = {33},
	issn = {0001-0782},
	url = {https://doi.org/10.1145/96267.96279},
	doi = {10.1145/96267.96279},
	abstract = {The following section describes the tools we built to test the utilities. These tools include the fuzz (random character) generator, ptyjig (to test interactive utilities), and scripts to automate the testing process. Next, we will describe the tests we performed, giving the types of input we presented to the utilities. Results from the tests will follow along with an analysis of the results, including identification and classification of the program bugs that caused the crashes. The final section presents concluding remarks, including suggestions for avoiding the types of problems detected by our study and some commentary on the bugs we found. We include an Appendix with the user manual pages for fuzz and ptyjig.},
	number = {12},
	journal = {Commun. ACM},
	author = {Miller, Barton P. and Fredriksen, Lars and So, Bryan},
	month = dec,
	year = {1990},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	pages = {32--44},
}

@article{miller_empirical_1990-1,
	title = {An empirical study of the reliability of {UNIX} utilities},
	volume = {33},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/96267.96279},
	doi = {10.1145/96267.96279},
	abstract = {Operating system facilities, such as the kernel and utility programs, are typically assumed to be reliable. In our recent experiments, we have been able to crash 25-33\% of the utility programs on any version of UNIX that was tested. This report describes these tests and an analysis of the program bugs that caused the crashes.},
	language = {en},
	number = {12},
	urldate = {2025-03-12},
	journal = {Communications of the ACM},
	author = {Miller, Barton P. and Fredriksen, Lars and So, Bryan},
	month = dec,
	year = {1990},
	pages = {32--44},
}

@inproceedings{boyer_selectformal_1975,
	address = {New York, NY, USA},
	title = {{SELECT}—a formal system for testing and debugging programs by symbolic execution},
	isbn = {978-1-4503-7385-2},
	url = {https://doi.org/10.1145/800027.808445},
	doi = {10.1145/800027.808445},
	abstract = {SELECT is an experimental system for assisting in the formal systematic debugging of programs. It is intended to be a compromise between an automated program proving system and the current ad hoc debugging practice, and is similar to a system being developed by King et al. of IBM. SELECT systematically handles the paths of programs written in a LISP subset that includes arrays. For each execution path SELECT returns simplified conditions on input variables that cause the path to be executed, and simplified symbolic values for program variables at the path output. For conditions which form a system of linear equalities and inequalities SELECT will return input variable values that can serve as sample test data. The user can insert constraint conditions, at any point in the program including the output, in the form of symbolically executable assertions. These conditions can induce the system to select test data in user-specified regions. SELECT can also determine if the path is correct with respect to an output assertion. We present four examples demonstrating the various modes of system operation and their effectiveness in finding bugs. In some examples, SELECT was successful in automatically finding useful test data. In others, user interaction was required in the form of output assertions. SELECT appears to be a useful tool for rapidly revealing program errors, but for the future there is a need to expand its expressive and deductive power.},
	booktitle = {Proceedings of the {International} {Conference} on {Reliable} {Software}},
	publisher = {Association for Computing Machinery},
	author = {Boyer, Robert S. and Elspas, Bernard and Levitt, Karl N.},
	year = {1975},
	note = {event-place: Los Angeles, California},
	keywords = {Program debugging, Program testing, Program verification, Solution of systems of inequalities, Symbolic execution, Test data generation},
	pages = {234--245},
}

@inproceedings{king_new_1975,
	address = {New York, NY, USA},
	title = {A new approach to program testing},
	isbn = {978-1-4503-7385-2},
	url = {https://doi.org/10.1145/800027.808444},
	doi = {10.1145/800027.808444},
	abstract = {The current approach for testing a program is, in principle, quite primitive. Some small sample of the data that a program is expected to handle is presented to the program. If the program produces correct results for the sample, it is assumed to be correct. Much current work focuses on the question of how to choose this sample. We propose that a program can be more effectively tested by executing it "symbolically." Instead of supplying specific constants as input values to a program being tested, one supplies symbols. The normal computational definitions for the basic operations performed by a program can be expanded to accept symbolic inputs and produce symbolic formulae as output. If the flow of control in the program is completely independent of its input parameters, then all output values can be symbolically computed as formulae over the symbolic inputs and examined for correctness. When the control flow of the program is input dependent, a case analysis can be performed producing output formulae for each class of inputs determined by the control flow dependencies. Using these ideas, we have designed and implemented an interactive debugging/testing system called EFFIGY.},
	booktitle = {Proceedings of the {International} {Conference} on {Reliable} {Software}},
	publisher = {Association for Computing Machinery},
	author = {King, James C.},
	year = {1975},
	note = {event-place: Los Angeles, California},
	keywords = {Program correctness, Program testing, Program verification, Symbolic execution, Symbolic interpretation},
	pages = {228--233},
}

@article{king_symbolic_1976,
	title = {Symbolic execution and program testing},
	volume = {19},
	issn = {0001-0782},
	url = {https://doi.org/10.1145/360248.360252},
	doi = {10.1145/360248.360252},
	abstract = {This paper describes the symbolic execution of programs. Instead of supplying the normal inputs to a program (e.g. numbers) one supplies symbols representing arbitrary values. The execution proceeds as in a normal execution except that values may be symbolic formulas over the input symbols. The difficult, yet interesting issues arise during the symbolic execution of conditional branch type statements. A particular system called EFFIGY which provides symbolic execution for program testing and debugging is also described. It interpretively executes programs written in a simple PL/I style programming language. It includes many standard debugging features, the ability to manage and to prove things about symbolic expressions, a simple program testing manager, and a program verifier. A brief discussion of the relationship between symbolic execution and program proving is also included.},
	number = {7},
	journal = {Commun. ACM},
	author = {King, James C.},
	month = jul,
	year = {1976},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {program debugging, program proving, program testing, program verification, symbolic execution, symbolic interpretation},
	pages = {385--394},
}

@article{king_new_nodate,
	title = {A {NEW} {APPROACH} {TO} {PROGRAM} {TESTING}},
	abstract = {The current approach for testing a program is, in principle, quite primitive. Some small sample of the data that a program is expected to handle is presented to the program. If the program produces correct results for the sample, it is assumed to be correct. Much current work focuses on the question of how to choose this sample. We propose that a program can be more effectively tested by executing it "symbolically." Instead of supplying specific constants as input values to a program being tested, one supplies symbols. The normal computational definitions for the basic operations performed by a program can be expanded to accept symbolic inputs and produce symbolic formulae as output.},
	language = {en},
	author = {King, James C},
}

@misc{baldoni_survey_2018,
	title = {A {Survey} of {Symbolic} {Execution} {Techniques}},
	url = {http://arxiv.org/abs/1610.00502},
	doi = {10.48550/arXiv.1610.00502},
	abstract = {Many security and software testing applications require checking whether certain properties of a program hold for any possible usage scenario. For instance, a tool for identifying software vulnerabilities may need to rule out the existence of any backdoor to bypass a program's authentication. One approach would be to test the program using different, possibly random inputs. As the backdoor may only be hit for very specific program workloads, automated exploration of the space of possible inputs is of the essence. Symbolic execution provides an elegant solution to the problem, by systematically exploring many possible execution paths at the same time without necessarily requiring concrete inputs. Rather than taking on fully specified input values, the technique abstractly represents them as symbols, resorting to constraint solvers to construct actual instances that would cause property violations. Symbolic execution has been incubated in dozens of tools developed over the last four decades, leading to major practical breakthroughs in a number of prominent software reliability applications. The goal of this survey is to provide an overview of the main ideas, challenges, and solutions developed in the area, distilling them for a broad audience. The present survey has been accepted for publication at ACM Computing Surveys. If you are considering citing this survey, we would appreciate if you could use the following BibTeX entry: http://goo.gl/Hf5Fvc},
	language = {en},
	urldate = {2025-03-12},
	publisher = {arXiv},
	author = {Baldoni, Roberto and Coppa, Emilio and D'Elia, Daniele Cono and Demetrescu, Camil and Finocchi, Irene},
	month = may,
	year = {2018},
	note = {arXiv:1610.00502 [cs]},
	keywords = {Computer Science - Programming Languages, Computer Science - Software Engineering},
}

@article{king_symbolic_1976-1,
	title = {Symbolic execution and program testing},
	volume = {19},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/360248.360252},
	doi = {10.1145/360248.360252},
	abstract = {This paper describes the symbolic execution of programs. Instead of supplying the normal inputs to a program (e.g. numbers) one supplies symbols representing arbitrary values. The execution proceeds as in a normal execution except that values may he symbolic formulas over the input symbols. The difficult, yet interesting issues arise during the symbolic execution of conditional branch type statements. A particular system called EFFIGY which provides symbolic execution for program testing and debugging is also described, it interpretively executes programs written in a simple PL/I style programming language. It includes many standard debugging features, the ability to manage and to prove things about symbolic expressions, a simple program testing manager, and a program verifier. A brief discussion of the relationship between symbolic execution and program proving is also included.},
	language = {en},
	number = {7},
	urldate = {2025-03-12},
	journal = {Communications of the ACM},
	author = {King, James C.},
	month = jul,
	year = {1976},
	pages = {385--394},
}

@article{cadar_klee_nodate,
	title = {{KLEE}: {Unassisted} and {Automatic} {Generation} of {High}-{Coverage} {Tests} for {Complex} {Systems} {Programs}},
	abstract = {We present a new symbolic execution tool, KLEE, capable of automatically generating tests that achieve high coverage on a diverse set of complex and environmentally-intensive programs. We applied KLEE to all 90 programs in the GNU COREUTILS utility suite, which form the core user-level environment installed on almost all Unix systems and, as such, represent some of the most heavily used and tested open-source programs in existence. For 84\% of these utilities, KLEE’s automatically generated tests covered 80–100\% of executable statements and, in aggregate, signiﬁcantly beat the coverage of the developers’ own hand-written test suites. KLEE also found nine serious bugs (including three that had been missed for over 15 years!) and produced concrete inputs that triggered the errors when run on the uninstrumented code. When applied to MINIX’s versions of a small selection of the same applications, KLEE achieved similar coverage (along with two bugs). In addition, we also used KLEE to automatically ﬁnd numerous incorrect differences between several MINIX and COREUTILS tools. Finally, we checked the kernel of the HISTAR operating system, generating tests that achieved 76.4\% (without paging enabled) and 67.1\% coverage (with paging) and found one important security bug.},
	language = {en},
	author = {Cadar, Cristian and Dunbar, Daniel and Engler, Dawson},
}

@inproceedings{burnim_heuristics_2008,
	address = {L'Aquila, Italy},
	title = {Heuristics for {Scalable} {Dynamic} {Test} {Generation}},
	isbn = {978-1-4244-2187-9},
	url = {http://ieeexplore.ieee.org/document/4639362/},
	doi = {10.1109/ASE.2008.69},
	abstract = {Recently there has been great success in using symbolic execution to automatically generate test inputs for small software systems. A primary challenge in scaling such approaches to larger programs is the combinatorial explosion of the path space. It is likely that sophisticated strategies for searching this path space are needed to generate inputs that effectively test large programs (by, e.g., achieving signiﬁcant branch coverage). We present several such heuristic search strategies, including a novel strategy guided by the control ﬂow graph of the program under test. We have implemented these strategies in CREST, our open source concolic testing tool for C, and evaluated them on two widely-used software tools, grep 2.2 (15K lines of code) and Vim 5.7 (150K lines). On these benchmarks, the presented heuristics achieve signiﬁcantly greater branch coverage on the same testing budget than concolic testing with a traditional depth-ﬁrst search strategy.},
	language = {en},
	urldate = {2025-03-12},
	booktitle = {2008 23rd {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {IEEE},
	author = {Burnim, Jacob and Sen, Koushik},
	month = sep,
	year = {2008},
	pages = {443--446},
}

@incollection{hutchison_generating_2013,
	address = {Berlin, Heidelberg},
	title = {Generating {Test} {Suites} with {Augmented} {Dynamic} {Symbolic} {Execution}},
	volume = {7942},
	isbn = {978-3-642-38915-3 978-3-642-38916-0},
	url = {http://link.springer.com/10.1007/978-3-642-38916-0_9},
	abstract = {Unit test generation tools typically aim at one of two objectives: to explore the program behavior in order to exercise automated oracles, or to produce a representative test set that can be used to manually add oracles or to use as a regression test set. Dynamic symbolic execution (DSE) can eﬃciently explore all simple paths through a program, exercising automated oracles such as assertions or code contracts. However, its original intention was not to produce representative test sets. Although DSE tools like Pex can retain subsets of the tests seen during the exploration, customer feedback revealed that users expect different values than those produced by Pex, and sometimes also more than one value for a given condition or program path. This threatens the applicability of DSE in a scenario without automated oracles. Indeed, even though all paths might be covered by DSE, the resulting tests are usually not sensitive enough to make a good regression test suite. In this paper, we present augmented dynamic symbolic execution, which aims to produce representative test sets with DSE by augmenting path conditions with additional conditions that enforce target criteria such as boundary or mutation adequacy, or logical coverage criteria. Experiments with our Apex prototype demonstrate that the resulting test cases can detect up to 30\% more seeded defects than those produced with Pex.},
	language = {en},
	urldate = {2025-03-12},
	booktitle = {Tests and {Proofs}},
	publisher = {Springer Berlin Heidelberg},
	author = {Jamrozik, Konrad and Fraser, Gordon and Tillman, Nikolai and De Halleux, Jonathan},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Veanes, Margus and Viganò, Luca},
	year = {2013},
	doi = {10.1007/978-3-642-38916-0_9},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {152--167},
}

@inproceedings{berthier_efficient_2023,
	title = {An {Efficient} {Black}-{Box} {Support} of {Advanced} {Coverage} {Criteria} for {Klee}},
	url = {http://arxiv.org/abs/2211.14592},
	doi = {10.1145/3555776.3577713},
	abstract = {Dynamic symbolic execution (DSE) is a powerful test generation approach based on an exploration of the path space of the program under test. Well-adapted for path coverage, this approach is however less efficient for conditions, decisions, advanced coverage criteria (such as multiple conditions, weak mutations, boundary testing) or user-provided test objectives. While theoretical solutions to adapt DSE to a large set of criteria have been proposed, they have never been integrated into publicly available testing tools. This paper presents a first integration of an optimized test generation strategy for advanced coverage criteria into a popular open-source testing tool based on DSE, namely, Klee. The integration is performed in a fully black-box manner, and can therefore inspire an easy integration into other similar tools. The resulting version of the tool, named Klee4labels, is publicly available. We present the design of the proposed technique and evaluate it on several benchmarks. Our results confirm the benefits of the proposed tool for advanced coverage criteria.},
	language = {en},
	urldate = {2025-03-12},
	booktitle = {Proceedings of the 38th {ACM}/{SIGAPP} {Symposium} on {Applied} {Computing}},
	author = {Berthier, Nicolas and Oliveira, Steven De and Kosmatov, Nikolai and Longuet, Delphine and Soulat, Romain},
	month = mar,
	year = {2023},
	note = {arXiv:2211.14592 [cs]},
	keywords = {Computer Science - Software Engineering},
	pages = {1706--1715},
}

@inproceedings{eriksson_black_2021,
	title = {Black {Widow}: {Blackbox} {Data}-driven {Web} {Scanning}},
	url = {https://doi.org/10.1109/SP40001.2021.00022},
	doi = {10.1109/SP40001.2021.00022},
	booktitle = {42nd {IEEE} {Symposium} on {Security} and {Privacy}, {SP} 2021, {San} {Francisco}, {CA}, {USA}, 24-27 {May} 2021},
	publisher = {IEEE},
	author = {Eriksson, Benjamin and Pellegrino, Giancarlo and Sabelfeld, Andrei},
	year = {2021},
	pages = {1125--1142},
}

@inproceedings{eriksson_black_2023,
	title = {Black {Ostrich}: {Web} {Application} {Scanning} with {String} {Solvers}},
	url = {https://doi.org/10.1145/3576915.3616582},
	doi = {10.1145/3576915.3616582},
	booktitle = {Proceedings of the 2023 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}, {CCS} 2023, {Copenhagen}, {Denmark}, {November} 26-30, 2023},
	publisher = {ACM},
	author = {Eriksson, Benjamin and Stjerna, Amanda and Masellis, Riccardo De and Rümmer, Philipp and Sabelfeld, Andrei},
	editor = {Meng, Weizhi and Jensen, Christian Damsgaard and Cremers, Cas and Kirda, Engin},
	year = {2023},
	pages = {549--563},
}

@inproceedings{cha_unleashing_2012,
	title = {Unleashing {Mayhem} on {Binary} {Code}},
	url = {https://doi.org/10.1109/SP.2012.31},
	doi = {10.1109/SP.2012.31},
	booktitle = {{IEEE} {Symposium} on {Security} and {Privacy}, {SP} 2012, 21-23 {May} 2012, {San} {Francisco}, {California}, {USA}},
	publisher = {IEEE Computer Society},
	author = {Cha, Sang Kil and Avgerinos, Thanassis and Rebert, Alexandre and Brumley, David},
	year = {2012},
	pages = {380--394},
}

@incollection{barrett_satisfiability_2009,
	series = {Frontiers in {Artificial} {Intelligence} and {Applications}},
	title = {Satisfiability {Modulo} {Theories}},
	volume = {185},
	url = {https://doi.org/10.3233/978-1-58603-929-5-825},
	booktitle = {Handbook of {Satisfiability}},
	publisher = {IOS Press},
	author = {Barrett, Clark W. and Sebastiani, Roberto and Seshia, Sanjit A. and Tinelli, Cesare},
	editor = {Biere, Armin and Heule, Marijn and Maaren, Hans van and Walsh, Toby},
	year = {2009},
	doi = {10.3233/978-1-58603-929-5-825},
	pages = {825--885},
}

@article{yurichev_satsmt_nodate,
	title = {{SAT}/{SMT} by {Example}},
	language = {en},
	author = {Yurichev, Dennis},
}

@misc{bill_q_2002,
	title = {Q\&{A}: {Bill} {Gates} {On} {Trustworthy} {Computing} {\textbar} {InformationWeek}},
	shorttitle = {Q\&{A}},
	url = {https://www.informationweek.com/it-leadership/q-a-bill-gates-on-trustworthy-computing},
	abstract = {Bill Gates, Microsoft's chairman and chief software architect, spoke with \&lt;I\&gt;InformationWeek\&lt;/I\&gt; about the companywide effort to improve the quality and security of Microsoft's products.},
	language = {en},
	urldate = {2025-03-06},
	journal = {Information Week},
	author = {Bill, Gates},
	month = may,
	year = {2002},
}

@article{yun_qsym_nodate,
	title = {{QSYM}: {A} {Practical} {Concolic} {Execution} {Engine} {Tailored} for {Hybrid} {Fuzzing}},
	abstract = {Recently, hybrid fuzzing has been proposed to address the limitations of fuzzing and concolic execution by combining both approaches. The hybrid approach has shown its effectiveness in various synthetic benchmarks such as DARPA Cyber Grand Challenge (CGC) binaries, but it still suffers from scaling to find bugs in complex, realworld software. We observed that the performance bottleneck of the existing concolic executor is the main limiting factor for its adoption beyond a small-scale study. To overcome this problem, we design a fast concolic execution engine, called QSYM, to support hybrid fuzzing. The key idea is to tightly integrate the symbolic emulation with the native execution using dynamic binary translation, making it possible to implement more fine-grained, so faster, instruction-level symbolic emulation. Additionally, QSYM loosens the strict soundness requirements of conventional concolic executors for better performance, yet takes advantage of a faster fuzzer for validation, providing unprecedented opportunities for performance optimizations, e.g., optimistically solving constraints and pruning uninteresting basic blocks. Our evaluation shows that QSYM does not just outperform state-of-the-art fuzzers (i.e., found 14× more bugs than VUzzer in the LAVA-M dataset, and outperformed Driller in 104 binaries out of 126), but also found 13 previously unknown security bugs in eight real-world programs like Dropbox Lepton, ffmpeg, and OpenJPEG, which have already been intensively tested by the stateof-the-art fuzzers, AFL and OSS-Fuzz.},
	language = {en},
	author = {Yun, Insu and Lee, Sangho and Xu, Meng and Jang, Yeongjin and Kim, Taesoo},
}

@inproceedings{farina_relational_2019,
	address = {Porto Portugal},
	title = {Relational {Symbolic} {Execution}},
	isbn = {978-1-4503-7249-7},
	url = {https://dl.acm.org/doi/10.1145/3354166.3354175},
	doi = {10.1145/3354166.3354175},
	abstract = {Symbolic execution is a classical program analysis technique used to show that programs satisfy or violate given specifications. In this work we generalize symbolic execution to support program analysis for relational specifications in the form of relational properties - these are properties about two runs of two programs on related inputs, or about two executions of a single program on related inputs. Relational properties are useful to formalize notions in security and privacy, and to reason about program optimizations. We design a relational symbolic execution engine, named RelSym which supports interactive refutation, as well as proving of relational properties for programs written in a language with arrays and for-like loops.},
	language = {en},
	urldate = {2025-03-05},
	booktitle = {Proceedings of the 21st {International} {Symposium} on {Principles} and {Practice} of {Declarative} {Programming}},
	publisher = {ACM},
	author = {Farina, Gian Pietro and Chong, Stephen and Gaboardi, Marco},
	month = oct,
	year = {2019},
	pages = {1--14},
}

@inproceedings{de_moura_z3_2008,
	title = {Z3: an efficient {SMT} solver},
	volume = {4963},
	isbn = {978-3-540-78799-0},
	doi = {10.1007/978-3-540-78800-3_24},
	booktitle = {Tools and {Algorithms} for the {Construction} and {Analysis} of {Systems}},
	author = {de Moura, Leonardo and Bjørner, Nikolaj},
	month = apr,
	year = {2008},
	pages = {337--340},
}

@inproceedings{sen_jalangi_2013,
	address = {Saint Petersburg Russia},
	title = {Jalangi: a selective record-replay and dynamic analysis framework for {JavaScript}},
	isbn = {978-1-4503-2237-9},
	shorttitle = {Jalangi},
	url = {https://dl.acm.org/doi/10.1145/2491411.2491447},
	doi = {10.1145/2491411.2491447},
	abstract = {JavaScript is widely used for writing client-side web applications and is getting increasingly popular for writing mobile applications. However, unlike C, C++, and Java, there are not that many tools available for analysis and testing of JavaScript applications. In this paper, we present a simple yet powerful framework, called Jalangi, for writing heavyweight dynamic analyses. Our framework incorporates two key techniques: 1) selective record-replay, a technique which enables to record and to faithfully replay a user-selected part of the program, and 2) shadow values and shadow execution, which enables easy implementation of heavy-weight dynamic analyses. Our implementation makes no special assumption about JavaScript, which makes it applicable to realworld JavaScript programs running on multiple platforms. We have implemented concolic testing, an analysis to track origins of nulls and undeﬁned, a simple form of taint analysis, an analysis to detect likely type inconsistencies, and an object allocation proﬁler in Jalangi. Our evaluation of Jalangi on the SunSpider benchmark suite and on ﬁve web applications shows that Jalangi has an average slowdown of 26X during recording and 30X slowdown during replay and analysis. The slowdowns are comparable with slowdowns reported for similar tools, such as PIN and Valgrind for x86 binaries. We believe that the techniques proposed in this paper are applicable to other dynamic languages.},
	language = {en},
	urldate = {2025-02-26},
	booktitle = {Proceedings of the 2013 9th {Joint} {Meeting} on {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Sen, Koushik and Kalasapur, Swaroop and Brutch, Tasneem and Gibbs, Simon},
	month = aug,
	year = {2013},
	pages = {488--498},
}

@misc{max_ogden_callback_2019,
	title = {Callback {Hell} http://callbackhell.com/},
	url = {http://callbackhell.com/},
	urldate = {2025-02-26},
	journal = {Callback Hell},
	author = {{Max Ogden}},
	year = {2019},
}

@misc{noauthor_z3proverz3_2025,
	title = {{Z3Prover}/z3},
	url = {https://github.com/Z3Prover/z3},
	abstract = {The Z3 Theorem Prover},
	urldate = {2025-02-26},
	publisher = {Z3 Theorem Prover},
	month = feb,
	year = {2025},
	note = {original-date: 2015-03-26T18:16:07Z},
}

@misc{noauthor_samsungjalangi2_nodate,
	title = {Samsung/jalangi2: {Dynamic} analysis framework for {JavaScript}},
	url = {https://github.com/Samsung/jalangi2/tree/master},
	urldate = {2025-02-25},
}

@article{ball_deconstructing_nodate,
	title = {Deconstructing {Dynamic} {Symbolic} {Execution}},
	abstract = {Dynamic symbolic execution (DSE) is a well-known technique for automatically generating tests to achieve higher levels of coverage in a program. Two keys ideas of DSE are to: (1) seed symbolic execution by executing a program on an initial input; (2) using concrete values from the program execution in place of symbolic expressions whenever symbolic reasoning is hard or not desired. We describe DSE for a simple core language and then present a minimalist implementation of DSE for Python (in Python) that follows this basic recipe. The code is available at https://www.github.com/thomasjball/PyExZ3/ (tagged “v1.0”) and has been designed to make it easy to experiment with and extend.},
	language = {en},
	author = {Ball, Thomas and Daniel, Jakub},
}

@article{gong_dynamic_nodate,
	title = {Dynamic {Analysis} for {JavaScript} {Code}},
	language = {en},
	author = {Gong, Liang},
}

@inproceedings{santos_symbolic_2018,
	address = {Frankfurt am Main Germany},
	title = {Symbolic {Execution} for {JavaScript}},
	isbn = {978-1-4503-6441-6},
	url = {https://dl.acm.org/doi/10.1145/3236950.3236956},
	doi = {10.1145/3236950.3236956},
	abstract = {We present a framework for trustworthy symbolic execution of JavaScripts programs, whose aim is to assist developers in the testing of their code: the developer writes symbolic tests for which the framework provides concrete counter-models. We create the framework following a new, general methodology for designing compositional program analyses for dynamic languages. We prove that the underlying symbolic execution is sound and does not generate false positives. We establish additional trust by using the theory to precisely guide the implementation and by thorough testing. We apply our framework to whole-program symbolic testing of real-world JavaScript libraries and compositional debugging of separation logic specifications of JavaScript programs.},
	language = {en},
	urldate = {2025-02-24},
	booktitle = {Proceedings of the 20th {International} {Symposium} on {Principles} and {Practice} of {Declarative} {Programming}},
	publisher = {ACM},
	author = {Santos, José Fragoso and Maksimović, Petar and Grohens, Théotime and Dolby, Julian and Gardner, Philippa},
	month = sep,
	year = {2018},
	pages = {1--14},
}

@article{godefroid_automated_nodate,
	title = {Automated {Whitebox} {Fuzz} {Testing}},
	abstract = {Fuzz testing is an effective technique for ﬁnding security vulnerabilities in software. Traditionally, fuzz testing tools apply random mutations to well-formed inputs of a program and test the resulting values. We present an alternative whitebox fuzz testing approach inspired by recent advances in symbolic execution and dynamic test generation. Our approach records an actual run of the program under test on a well-formed input, symbolically evaluates the recorded trace, and gathers constraints on inputs capturing how the program uses these. The collected constraints are then negated one by one and solved with a constraint solver, producing new inputs that exercise different control paths in the program. This process is repeated with the help of a code-coverage maximizing heuristic designed to ﬁnd defects as fast as possible. We have implemented this algorithm in SAGE (Scalable, Automated, Guided Execution), a new tool employing x86 instruction-level tracing and emulation for whitebox fuzzing of arbitrary ﬁle-reading Windows applications. We describe key optimizations needed to make dynamic test generation scale to large input ﬁles and long execution traces with hundreds of millions of instructions. We then present detailed experiments with several Windows applications. Notably, without any format-speciﬁc knowledge, SAGE detects the MS07-017 ANI vulnerability, which was missed by extensive blackbox fuzzing and static analysis tools. Furthermore, while still in an early stage of development, SAGE has already discovered 30+ new bugs in large shipped Windows applications including image processors, media players, and ﬁle decoders. Several of these bugs are potentially exploitable memory access violations.},
	language = {en},
	author = {Godefroid, Patrice and Levin, Michael Y and Molnar, David},
}

@article{godefroid_sage_2012,
	title = {{SAGE}: whitebox fuzzing for security testing},
	volume = {55},
	issn = {0001-0782, 1557-7317},
	shorttitle = {{SAGE}},
	url = {https://dl.acm.org/doi/10.1145/2093548.2093564},
	doi = {10.1145/2093548.2093564},
	abstract = {SAGE has had a remarkable impact at Microsoft.},
	language = {en},
	number = {3},
	urldate = {2025-02-21},
	journal = {Communications of the ACM},
	author = {Godefroid, Patrice and Levin, Michael Y. and Molnar, David},
	month = mar,
	year = {2012},
	pages = {40--44},
}

@article{cadar_symbolic_2013,
	title = {Symbolic execution for software testing: three decades later},
	volume = {56},
	issn = {0001-0782, 1557-7317},
	shorttitle = {Symbolic execution for software testing},
	url = {https://dl.acm.org/doi/10.1145/2408776.2408795},
	doi = {10.1145/2408776.2408795},
	abstract = {The challenges---and great promise---of modern symbolic execution techniques, and the tools to help implement them.},
	language = {en},
	number = {2},
	urldate = {2025-02-20},
	journal = {Communications of the ACM},
	author = {Cadar, Cristian and Sen, Koushik},
	month = feb,
	year = {2013},
	pages = {82--90},
}

@inproceedings{kinder_efficient_2014,
	address = {Lausanne, Switzerland},
	title = {Efficient symbolic execution for software testing},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {978-0-9835678-4-4},
	url = {https://ieeexplore.ieee.org/document/6987585/},
	doi = {10.1109/FMCAD.2014.6987585},
	language = {en},
	urldate = {2025-02-20},
	booktitle = {2014 {Formal} {Methods} in {Computer}-{Aided} {Design} ({FMCAD})},
	publisher = {IEEE},
	author = {Kinder, Johannes},
	month = oct,
	year = {2014},
	pages = {5--5},
}

@inproceedings{godefroid_proving_2010,
	address = {Trento Italy},
	title = {Proving memory safety of floating-point computations by combining static and dynamic program analysis},
	isbn = {978-1-60558-823-0},
	url = {https://dl.acm.org/doi/10.1145/1831708.1831710},
	doi = {10.1145/1831708.1831710},
	abstract = {Whitebox fuzzing is a novel form of security testing based on dynamic symbolic execution and constraint solving. Over the last couple of years, whitebox fuzzers have found many new security vulnerabilities (buﬀer overﬂows) in Windows and Linux applications, including codecs, image viewers and media players. Those types of applications tend to use ﬂoating-point instructions available on modern processors, yet existing whitebox fuzzers and SMT constraint solvers do not handle ﬂoating-point arithmetic. Are there new security vulnerabilities lurking in ﬂoating-point code? A naive solution would be to extend symbolic execution to ﬂoating-point (FP) instructions (months of work), extend SMT solvers to reason about FP constraints (months of work or more), and then face more complex constraints and an even worse path explosion problem. Instead, we propose an alternative approach, based on the rough intuition that FP code should only perform memory safe data-processing of the “payload” of an image or video ﬁle, while the non-FP part of the application should deal with buﬀer allocations and memory address computations, with only the latter being prone to buﬀer overﬂows and other security critical bugs. Our approach combines (1) a lightweight local path-insensitive “may” static analysis of FP instructions with (2) a high-precision wholeprogram path-sensitive “must” dynamic analysis of non-FP instructions. The aim of this combination is to prove memory safety of the FP part of each execution and a form of non-interference between the FP part and the non-FP part with respect to memory address computations.},
	language = {en},
	urldate = {2025-02-20},
	booktitle = {Proceedings of the 19th international symposium on {Software} testing and analysis},
	publisher = {ACM},
	author = {Godefroid, Patrice and Kinder, Johannes},
	month = jul,
	year = {2010},
	pages = {1--12},
}

@article{andreasen_survey_2018,
	title = {A {Survey} of {Dynamic} {Analysis} and {Test} {Generation} for {JavaScript}},
	volume = {50},
	issn = {0360-0300, 1557-7341},
	url = {https://dl.acm.org/doi/10.1145/3106739},
	doi = {10.1145/3106739},
	abstract = {JavaScript has become one of the most prevalent programming languages. Unfortunately, some of the unique properties that contribute to this popularity also make JavaScript programs prone to errors and difficult for program analyses to reason about. These properties include the highly dynamic nature of the language, a set of unusual language features, a lack of encapsulation mechanisms, and the “no crash” philosophy. This article surveys dynamic program analysis and test generation techniques for JavaScript targeted at improving the correctness, reliability, performance, security, and privacy of JavaScript-based software.},
	language = {en},
	number = {5},
	urldate = {2025-02-20},
	journal = {ACM Computing Surveys},
	author = {Andreasen, Esben and Gong, Liang and Møller, Anders and Pradel, Michael and Selakovic, Marija and Sen, Koushik and Staicu, Cristian-Alexandru},
	month = sep,
	year = {2018},
	pages = {1--36},
}

@incollection{barrett_satisfiability_2009,
	series = {Frontiers in {Artificial} {Intelligence} and {Applications}},
	title = {Satisfiability modulo theories},
	isbn = {978-1-58603-929-5},
	url = {http://www.scopus.com/inward/record.url?scp=69949133234&partnerID=8YFLogxK},
	abstract = {Applications in artificial intelligence, formal verification, and other areas have greatly benefited from the recent advances in SAT. It is often the case, however, that applications in these fields require determining the satisfiability of formulas in more expressive logics such as first-order logic. Also, these applications typically require not general first-order satisfiability, but rather satisfiability with respect to some background theory, which fixes the interpretations of certain predicate and function symbols. For many background theories, specialized methods yield decision procedures for the satisfiability of quantifier-free formulas or some subclass thereof. Specialized decision procedures have been discovered for a long and still growing list of theories with practical applications. These include the theory of equality, various theories of arithmetic, and certain theories of arrays, as well as theories of lists, tuples, records, and bit-vectors of a fixed or arbitrary finite size. The research field concerned with determining the satisfiability of formulas with respect to some background theory is called Satisfiability Modulo Theories (SMT). This chapter provides a brief overview of SMT together with references to the relevant literature for a deeper study. It begins with an overview of techniques for solving SMT problems by encodings to SAT. The rest of the chapter is mostly concerned with an alternative approach in which a SAT solver is integrated with a separate decision procedure (called a theory solver) for conjunctions of literals in the background theory. After presenting this approach as a whole, the chapter provides more details on how to construct and combine theory solvers, and discusses several extensions and enhancements.},
	urldate = {2025-02-17},
	booktitle = {Handbook of {Satisfiability}},
	publisher = {IOS Press},
	author = {Barrett, Clark and Sebastiani, Roberto and Seshia, Sanjit A. and Tinelli, Cesare},
	year = {2009},
	doi = {10.3233/978-1-58603-929-5-825},
	pages = {825--885},
}

@inproceedings{csallner_dysy_2008,
	address = {Leipzig, Germany},
	title = {{DySy}: dynamic symbolic execution for invariant inference},
	isbn = {978-1-60558-079-1},
	shorttitle = {{DySy}},
	url = {http://portal.acm.org/citation.cfm?doid=1368088.1368127},
	doi = {10.1145/1368088.1368127},
	abstract = {Dynamically discovering likely program invariants from concrete test executions has emerged as a highly promising software engineering technique. Dynamic invariant inference has the advantage of succinctly summarizing both “expected” program inputs and the subset of program behaviors that is normal under those inputs. In this paper, we introduce a technique that can drastically increase the relevance of inferred invariants, or reduce the size of the test suite required to obtain good invariants. Instead of falsifying invariants produced by pre-set patterns, we determine likely program invariants by combining the concrete execution of actual test cases with a simultaneous symbolic execution of the same tests. The symbolic execution produces abstract conditions over program variables that the concrete tests satisfy during their execution. In this way, we obtain the beneﬁts of dynamic inference tools like Daikon: the inferred invariants correspond to the observed program behaviors. At the same time, however, our inferred invariants are much more suited to the program at hand than Daikon’s hardcoded invariant patterns. The symbolic invariants are literally derived from the program text itself, with appropriate value substitutions as dictated by symbolic execution.},
	language = {en},
	urldate = {2025-02-17},
	booktitle = {Proceedings of the 13th international conference on {Software} engineering  - {ICSE} '08},
	publisher = {ACM Press},
	author = {Csallner, Christoph and Tillmann, Nikolai and Smaragdakis, Yannis},
	year = {2008},
	pages = {281},
}

@inproceedings{elkarablieh_precise_2009,
	address = {Chicago IL USA},
	title = {Precise pointer reasoning for dynamic test generation},
	isbn = {978-1-60558-338-9},
	url = {https://dl.acm.org/doi/10.1145/1572272.1572288},
	doi = {10.1145/1572272.1572288},
	abstract = {Dynamic test generation consists of executing a program while gathering symbolic constraints on inputs from predicates encountered in branch statements, and of using a constraint solver to infer new program inputs from previous constraints in order to steer next executions towards new program paths. Variants of this technique have recently been adopted in several bug detection tools, including our whitebox fuzzer SAGE, which has found dozens of new expensive security-related bugs in many Windows applications and is now routinely used in various Microsoft groups.},
	language = {en},
	urldate = {2025-02-15},
	booktitle = {Proceedings of the eighteenth international symposium on {Software} testing and analysis},
	publisher = {ACM},
	author = {Elkarablieh, Bassem and Godefroid, Patrice and Levin, Michael Y.},
	month = jul,
	year = {2009},
	pages = {129--140},
}

@article{jhala_software_2009,
	title = {Software model checking},
	volume = {41},
	issn = {0360-0300, 1557-7341},
	url = {https://dl.acm.org/doi/10.1145/1592434.1592438},
	doi = {10.1145/1592434.1592438},
	abstract = {We survey recent progress in software model checking.},
	language = {en},
	number = {4},
	urldate = {2025-02-15},
	journal = {ACM Computing Surveys},
	author = {Jhala, Ranjit and Majumdar, Rupak},
	month = oct,
	year = {2009},
	pages = {1--54},
}

@inproceedings{coppa_rethinking_2017,
	address = {Urbana, IL},
	title = {Rethinking pointer reasoning in symbolic execution},
	isbn = {978-1-5386-2684-9},
	url = {http://ieeexplore.ieee.org/document/8115671/},
	doi = {10.1109/ASE.2017.8115671},
	abstract = {Symbolic execution is a popular program analysis technique that allows seeking for bugs by reasoning over multiple alternative execution states at once. As the number of states to explore may grow exponentially, a symbolic executor may quickly run out of space. For instance, a memory access to a symbolic address may potentially reference the entire address space, leading to a combinatorial explosion of the possible resulting execution states. To cope with this issue, state-of-the-art executors concretize symbolic addresses that span memory intervals larger than some threshold. Unfortunately, this could result in missing interesting execution states, e.g., where a bug arises.},
	language = {en},
	urldate = {2025-02-15},
	booktitle = {2017 32nd {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	publisher = {IEEE},
	author = {Coppa, Emilio and D'Elia, Daniele Cono and Demetrescu, Camil},
	month = oct,
	year = {2017},
	pages = {613--618},
}

@article{cadar_symbolic_2013-1,
	title = {Symbolic execution for software testing: three decades later},
	volume = {56},
	issn = {0001-0782, 1557-7317},
	shorttitle = {Symbolic execution for software testing},
	url = {https://dl.acm.org/doi/10.1145/2408776.2408795},
	doi = {10.1145/2408776.2408795},
	abstract = {The challenges---and great promise---of modern symbolic execution techniques, and the tools to help implement them.},
	language = {en},
	number = {2},
	urldate = {2025-02-15},
	journal = {Communications of the ACM},
	author = {Cadar, Cristian and Sen, Koushik},
	month = feb,
	year = {2013},
	pages = {82--90},
}

@article{godefroid_dart_nodate,
	title = {{DART}: {Directed} {Automated} {Random} {Testing}},
	abstract = {We present a new tool, named DART, for automatically testing software that combines three main techniques: (1) automated extraction of the interface of a program with its external environment using static source-code parsing; (2) automatic generation of a test driver for this interface that performs random testing to simulate the most general environment the program can operate in; and (3) dynamic analysis of how the program behaves under random testing and automatic generation of new test inputs to direct systematically the execution along alternative program paths. Together, these three techniques constitute Directed Automated Random Testing, or DART for short. The main strength of DART is thus that testing can be performed completely automatically on any program that compiles – there is no need to write any test driver or harness code. During testing, DART detects standard errors such as program crashes, assertion violations, and non-termination. Preliminary experiments to unit test several examples of C programs are very encouraging.},
	language = {en},
	author = {Godefroid, Patrice and Klarlund, Nils and Sen, Koushik},
}

@inproceedings{godefroid_random_2007,
	address = {Atlanta Georgia},
	title = {Random testing for security: blackbox vs. whitebox fuzzing},
	isbn = {978-1-59593-881-7},
	shorttitle = {Random testing for security},
	url = {https://dl.acm.org/doi/10.1145/1292414.1292416},
	doi = {10.1145/1292414.1292416},
	abstract = {Fuzz testing is an eﬀective technique for ﬁnding security vulnerabilities in software. Fuzz testing is a form of blackbox random testing which randomly mutates well-formed inputs and tests the program on the resulting data. In some cases, grammars are used to randomly generate the well-formed inputs. This also allows the tester to encode applicationspeciﬁc knowledge (such as corner cases of particular interest) as part of the grammar, and to specify test heuristics by assigning probabilistic weights to production rules. Although fuzz testing can be remarkably eﬀective, the limitations of blackbox random testing are well-known. For instance, the then branch of the conditional statement “if (x==10) then” has only one in 232 chances of being exercised if x is a randomly chosen 32-bit input value. This intuitively explains why random testing usually provides low code coverage.},
	language = {en},
	urldate = {2025-02-15},
	booktitle = {Proceedings of the 2nd international workshop on {Random} testing: co-located with the 22nd {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE} 2007)},
	publisher = {ACM},
	author = {Godefroid, Patrice},
	month = nov,
	year = {2007},
	pages = {1--1},
}

@article{demott_evolving_nodate,
	title = {The {Evolving} {Art} of {Fuzzing}},
	language = {en},
	author = {DeMott, Jared},
}

@article{b_w_boehm_verifying_1984,
	title = {Verifying and {Validating} {Software} {Requirements} and {Design} {Specifications}},
	volume = {1},
	doi = {10.1109/MS.1984.233702},
	abstract = {Explore millions of resources from scholarly journals, books, newspapers, videos and more, on the ProQuest Platform.},
	number = {1},
	urldate = {2025-02-13},
	author = {{B. W. Boehm}},
	year = {1984},
	pages = {75--88},
}

@article{boehm_software_nodate,
	title = {Software {Engineering} {Economics}},
	abstract = {This paper summarizes the current state of the art and recent trends in software engineering economics. It provides an overview of economic analysis techniques and their applicability to software engineering and management. It surveys the field of software cost estimation, including the major estimation techniques available, the state of the art in algorithmic cost models, and the outstanding research issues in software cost estimation.},
	language = {en},
	journal = {Prentice Hall},
	author = {Boehm, Barry W},
}

@misc{roy_t_fielding_rest_2008,
	title = {{REST} {APIs} must be hypertext-driven » {Untangled} https://roy.gbiv.com/untangled/2008/rest-apis-must-be-hypertext-driven},
	shorttitle = {{REST} {APIs} must be hypertext-driven » {Untangled} https},
	url = {https://roy.gbiv.com/untangled/2008/rest-apis-must-be-hypertext-driven},
	urldate = {2025-02-12},
	author = {{Roy T. Fielding}},
	year = {2008},
}

@techreport{berners-lee_uniform_2005,
	type = {Request for {Comments}},
	title = {Uniform {Resource} {Identifier} ({URI}): {Generic} {Syntax}},
	shorttitle = {Uniform {Resource} {Identifier} ({URI})},
	url = {https://datatracker.ietf.org/doc/rfc3986},
	abstract = {A Uniform Resource Identifier (URI) is a compact sequence of characters that identifies an abstract or physical resource. This specification defines the generic URI syntax and a process for resolving URI references that might be in relative form, along with guidelines and security considerations for the use of URIs on the Internet. The URI syntax defines a grammar that is a superset of all valid URIs, allowing an implementation to parse the common components of a URI reference without knowing the scheme-specific requirements of every possible identifier. This specification does not define a generative grammar for URIs; that task is performed by the individual specifications of each URI scheme. [STANDARDS-TRACK]},
	number = {RFC 3986},
	urldate = {2025-02-12},
	institution = {Internet Engineering Task Force},
	author = {Berners-Lee, Tim and Fielding, Roy T. and Masinter, Larry M.},
	month = jan,
	year = {2005},
	doi = {10.17487/RFC3986},
	note = {Num Pages: 61},
}

@misc{noauthor_jsonapi_nodate,
	title = {{JSON}:{API} — {Latest} {Specification} (v1.1) https://jsonapi.org/format/},
	url = {https://jsonapi.org/format/},
	urldate = {2025-02-12},
}

@misc{noauthor_serverless_nodate,
	title = {Serverless {Function}, {FaaS} {Serverless} - {AWS} {Lambda} - {AWS}},
	url = {https://aws.amazon.com/lambda/},
	abstract = {AWS Lambda is a serverless compute service for running code without having to provision or manage servers. You pay only for the compute time you consume.},
	language = {en-US},
	urldate = {2025-02-12},
	journal = {Amazon Web Services, Inc.},
}

@techreport{fielding_http_2022,
	type = {Request for {Comments}},
	title = {{HTTP} {Semantics}},
	url = {https://datatracker.ietf.org/doc/rfc9110},
	abstract = {The Hypertext Transfer Protocol (HTTP) is a stateless application-level protocol for distributed, collaborative, hypertext information systems. This document describes the overall architecture of HTTP, establishes common terminology, and defines aspects of the protocol that are shared by all versions. In this definition are core protocol elements, extensibility mechanisms, and the "http" and "https" Uniform Resource Identifier (URI) schemes. This document updates RFC 3864 and obsoletes RFCs 2818, 7231, 7232, 7233, 7235, 7538, 7615, 7694, and portions of 7230.},
	number = {RFC 9110},
	urldate = {2025-02-12},
	institution = {Internet Engineering Task Force},
	author = {Fielding, Roy T. and Nottingham, Mark and Reschke, Julian},
	month = jun,
	year = {2022},
	doi = {10.17487/RFC9110},
	note = {Num Pages: 194},
}

@article{oehlert_violating_2005,
	title = {Violating assumptions with fuzzing},
	volume = {3},
	issn = {1558-4046},
	url = {https://ieeexplore.ieee.org/document/1423963/?arnumber=1423963},
	doi = {10.1109/MSP.2005.55},
	abstract = {Fuzzing is a highly automated testing technique that covers numerous boundary cases using invalid data (from files, network protocols, API calls, and other targets) as application input to better ensure the absence of exploitable vulnerabilities. Fuzzing lets developers or quality assurance (QA) teams test large numbers of boundary cases when doing so with techniques such as functional testing would be cost prohibitive. Comprehensive negative test cases - those that verify that a product does not do something it shouldn't do, rather than that it does something it is supposed to (positive test cases) - are difficult to construct because the number of possible permutations is astronomical. Yet, fuzzing covers a significant portion of negative test cases without forcing the tester to deal with each specific test case for a given boundary condition.},
	number = {2},
	urldate = {2025-02-12},
	journal = {IEEE Security \& Privacy},
	author = {Oehlert, P.},
	month = mar,
	year = {2005},
	note = {Conference Name: IEEE Security \& Privacy},
	keywords = {Application software, Automatic testing, Computer security, Cost function, Data privacy, Data security, Kernel, Network interfaces, Protocols, Quality assurance, boundary conditions, completeness testing, comprehensive negative testing, fuzzing, quality assurance, software testing},
	pages = {58--62},
}

@article{liang_fuzzing_2018,
	title = {Fuzzing: {State} of the {Art}},
	volume = {67},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {0018-9529, 1558-1721},
	shorttitle = {Fuzzing},
	url = {https://ieeexplore.ieee.org/document/8371326/},
	doi = {10.1109/TR.2018.2834476},
	abstract = {As one of the most popular software testing techniques, fuzzing can ﬁnd a variety of weaknesses in a program, such as software bugs and vulnerabilities, by generating numerous test inputs. Due to its effectiveness, fuzzing is regarded as a valuable bug hunting method. In this paper, we present an overview of fuzzing that concentrates on its general process, as well as classiﬁcations, followed by detailed discussion of the key obstacles and some stateof-the-art technologies which aim to overcome or mitigate these obstacles. We further investigate and classify several widely used fuzzing tools. Our primary goal is to equip the stakeholder with a better understanding of fuzzing and the potential solutions for improving fuzzing methods in the spectrum of software testing and security. To inspire future research, we also predict some future directions with regard to fuzzing.},
	language = {en},
	number = {3},
	urldate = {2025-02-12},
	journal = {IEEE Transactions on Reliability},
	author = {Liang, Hongliang and Pei, Xiaoxiao and Jia, Xiaodong and Shen, Wuwei and Zhang, Jian},
	month = sep,
	year = {2018},
	pages = {1199--1218},
}

@misc{noauthor_web_2012,
	title = {The {Web} {Application} {Security} {Consortium}},
	shorttitle = {{WASC}},
	url = {http://projects.webappsec.org/w/page/13246967/The%20Web%20Security%20Glossary#WebApplication},
	urldate = {2025-02-07},
	journal = {The Web Security Glossary},
	year = {2012},
}

@inproceedings{fong_web_2007,
	address = {Waikoloa, HI, USA},
	title = {Web {Application} {Scanners}: {Definitions} and {Functions}},
	shorttitle = {Web {Application} {Scanners}},
	url = {http://ieeexplore.ieee.org/document/4076950/},
	doi = {10.1109/HICSS.2007.611},
	abstract = {There are many commercial software security assurance tools that claim to detect and prevent vulnerabilities in application software. However, a closer look at the tools often leaves one wondering which tools find what vulnerabilities. This paper identifies a taxonomy of software security assurance tools and defines one type of tool: web application scanner, i.e., an automated program that examines web applications for security vulnerabilities. We describe the types of functions that are generally found in a web application scanner and how to test it.},
	language = {en},
	urldate = {2025-02-07},
	booktitle = {2007 40th {Annual} {Hawaii} {International} {Conference} on {System} {Sciences} ({HICSS}'07)},
	publisher = {IEEE},
	author = {Fong, Elizabeth and Okun, Vadim},
	year = {2007},
	pages = {280b--280b},
}

@article{veres_exploration_nodate,
	title = {An {Exploration} of {Current} {Techniques} in {OWASP} {Vulnerability} {Detection} and {Improvement} {Opportunities}},
	abstract = {Web applications are foundational in today’s digital landscape, necessitating advanced security measures. This study delves into Interactive Application Security Testing (IAST) and Web Fuzzing, two pivotal techniques for detecting web vulnerabilities. We systematically evaluate their strengths and weaknesses, emphasizing their potential in addressing vulnerabilities highlighted by the OWASP Top 10. While IAST excels in real-time vulnerability detection, Web Fuzzing offers an expansive approach, adept at uncovering elusive edge cases. Our research suggests that combining these techniques could lead to substantial enhancements in web application security. Additionally, we introduce the idea of an open-source IAST tool and contemplate the benefits that recent advances in artificial intelligence might bring to these techniques. Furthermore, we underscore the significance of understanding these tools’ operation within the realm of cloud computing.},
	language = {en},
	author = {Veres, Andrei-Claudiu},
}

@article{cadar_klee_nodate,
	title = {{KLEE}: {Unassisted} and {Automatic} {Generation} of {High}-{Coverage} {Tests} for {Complex} {Systems} {Programs}},
	abstract = {We present a new symbolic execution tool, KLEE, capable of automatically generating tests that achieve high coverage on a diverse set of complex and environmentally-intensive programs. We used KLEE to thoroughly check all 89 stand-alone programs in the GNU COREUTILS utility suite, which form the core user-level environment installed on millions of Unix systems, and arguably are the single most heavily tested set of open-source programs in existence. KLEE-generated tests achieve high line coverage — on average over 90\% per tool (median: over 94\%) — and signiﬁcantly beat the coverage of the developers’ own hand-written test suite. When we did the same for 75 equivalent tools in the BUSYBOX embedded system suite, results were even better, including 100\% coverage on 31 of them.},
	language = {en},
	author = {Cadar, Cristian and Dunbar, Daniel and Engler, Dawson},
}

@inproceedings{sen_concolic_2007,
	address = {New York, NY, USA},
	series = {{ASE} '07},
	title = {Concolic testing},
	isbn = {978-1-59593-882-4},
	url = {https://doi.org/10.1145/1321631.1321746},
	doi = {10.1145/1321631.1321746},
	abstract = {Concolic testing automates test input generation by combining the concrete and symbolic (concolic) execution of the code under test. Traditional test input generation techniques use either (1) concrete execution or (2) symbolic execution that builds constraints and is followed by a generation of concrete test inputs from these constraints. In contrast, concolic testing tightly couples both concrete and symbolic executions: they run simultaneously, and each gets feedback from the other.We have implemented concolic testing in tools for testing both C and Java programs. We have used the tools to find bugs in several real-world software systems including SGLIB, a popular C data structure library used in a commercial tool, a third-party implementation of the Needham-Schroeder protocol and the TMN protocol, the scheduler of Honeywell's DEOS real-time operating system, and the Sun Microsystems' JDK 1.4 collection framework. In this tutorial, we will describe concolic testing and some of its recent extensions},
	urldate = {2025-01-29},
	booktitle = {Proceedings of the 22nd {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Sen, Koushik},
	month = nov,
	year = {2007},
	pages = {571--572},
}

@article{pak_hybrid_nodate,
	title = {Hybrid {Fuzz} {Testing}: {Discovering} {Software} {Bugs} via {Fuzzing} and {Symbolic} {Execution}},
	language = {en},
	author = {Pak, Brian S},
}

@misc{noauthor_httpswwwusenixorgsystemfilesconferenceusenixsecurity15sec15-paper-ramospdf_nodate,
	title = {https://www.usenix.org/system/files/conference/usenixsecurity15/sec15-paper-ramos.pdf},
	url = {https://www.usenix.org/system/files/conference/usenixsecurity15/sec15-paper-ramos.pdf},
	urldate = {2025-01-24},
}

@article{ramos_under-constrained_nodate,
	title = {Under-{Constrained} {Symbolic} {Execution}: {Correctness} {Checking} for {Real} {Code}},
	abstract = {Software bugs are a well-known source of security vulnerabilities. One technique for ﬁnding bugs, symbolic execution, considers all possible inputs to a program but suffers from scalability limitations. This paper uses a variant, under-constrained symbolic execution, that improves scalability by directly checking individual functions, rather than whole programs. We present UC-KLEE, a novel, scalable framework for checking C/C++ systems code, along with two use cases. First, we use UC-KLEE to check whether patches introduce crashes. We check over 800 patches from BIND and OpenSSL and ﬁnd 12 bugs, including two OpenSSL denial-of-service vulnerabilities. We also verify (with caveats) that 115 patches do not introduce crashes. Second, we use UC-KLEE as a generalized checking framework and implement checkers to ﬁnd memory leaks, uninitialized data, and unsafe user input. We evaluate the checkers on over 20,000 functions from BIND, OpenSSL, and the Linux kernel, ﬁnd 67 bugs, and verify that hundreds of functions are leak free and that thousands of functions do not access uninitialized data.},
	language = {en},
	author = {Ramos, David A and Engler, Dawson},
}

@article{tillmann_unit_2006,
	title = {Unit tests reloaded: parameterized unit testing with symbolic execution},
	volume = {23},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {0740-7459},
	shorttitle = {Unit tests reloaded},
	url = {http://ieeexplore.ieee.org/document/1657937/},
	doi = {10.1109/MS.2006.117},
	language = {en},
	number = {4},
	urldate = {2025-01-19},
	journal = {IEEE Software},
	author = {Tillmann, N. and Schulte, W.},
	month = jul,
	year = {2006},
	pages = {38--47},
}

@inproceedings{cha_unleashing_2012,
	address = {San Francisco, CA, USA},
	title = {Unleashing {Mayhem} on {Binary} {Code}},
	isbn = {978-1-4673-1244-8 978-0-7695-4681-0},
	url = {https://ieeexplore.ieee.org/document/6234425/},
	doi = {10.1109/SP.2012.31},
	abstract = {In this paper we present MAYHEM, a new system for automatically ﬁnding exploitable bugs in binary (i.e., executable) programs. Every bug reported by MAYHEM is accompanied by a working shell-spawning exploit. The working exploits ensure soundness and that each bug report is securitycritical and actionable. MAYHEM works on raw binary code without debugging information. To make exploit generation possible at the binary-level, MAYHEM addresses two major technical challenges: actively managing execution paths without exhausting memory, and reasoning about symbolic memory indices, where a load or a store address depends on user input. To this end, we propose two novel techniques: 1) hybrid symbolic execution for combining online and ofﬂine (concolic) execution to maximize the beneﬁts of both techniques, and 2) index-based memory modeling, a technique that allows MAYHEM to efﬁciently reason about symbolic memory at the binary level. We used MAYHEM to ﬁnd and demonstrate 29 exploitable vulnerabilities in both Linux and Windows programs, 2 of which were previously undocumented.},
	language = {en},
	urldate = {2025-01-09},
	booktitle = {2012 {IEEE} {Symposium} on {Security} and {Privacy}},
	publisher = {IEEE},
	author = {Cha, Sang Kil and Avgerinos, Thanassis and Rebert, Alexandre and Brumley, David},
	month = may,
	year = {2012},
	pages = {380--394},
}

@misc{noauthor_httpsusersececmueduaavgerinpapersmayhem-oakland-12pdf_nodate,
	title = {https://users.ece.cmu.edu/{\textasciitilde}aavgerin/papers/mayhem-oakland-12.pdf},
	url = {https://users.ece.cmu.edu/~aavgerin/papers/mayhem-oakland-12.pdf},
	urldate = {2025-01-09},
}

@article{aydos_security_2022,
	title = {Security testing of web applications: {A} systematic mapping of the literature},
	volume = {34},
	issn = {1319-1578},
	shorttitle = {Security testing of web applications},
	url = {https://www.sciencedirect.com/science/article/pii/S131915782100269X},
	doi = {10.1016/j.jksuci.2021.09.018},
	abstract = {Context
Web application security is a main component of any web-based business. Web applications are subject to attacks from different locations at various levels of scale and complexity. In this context, a large number of testing techniques, tools and frameworks have been proposed by both practitioners and researchers to effectively and efficiently test the security of web applications.
Objective
As the number of papers increases in the security of web applications and this research area matures, reviewing and getting an overview of this area is getting challenging for a practitioner or a new researcher. Our objective is to summarize the state-of-the-art in web application security testing which could benefit practitioners to potentially utilize that information.
Method
We review and structure the body of knowledge related to web application security testing in the form of a systematic literature mapping (SLM). As part of this study, we pose four sets of research questions, define selection and exclusion criteria, and systematically develop and refine a classification schema. The initial pool consisted of 154 articles. Systematic voting was conducted among the authors regarding the inclusion/exclusion of articles. As a result, there were 80 technical articles in our final pool. Accordance with our inclusion and exclusion criteria, the first article was published in 2005 and this review includes all the papers until the end of 2020. During December 2020, January and February 2021, the search phase has been conducted.
Results
This review paper provides an overview of web application security testing with different focused headings. These headings cover contribution types, web security testing tools and their sub features, specific questions/features to the security testing such as vulnerability types, system under testing (SUT) focused headings and more.
Conclusion
The results of this study would benefit researchers working on web application security testing. Also, it could be useful for developers who discuss application security while they develop web applications. Thanks to this paper, these researchers could utilize the all results and use them to catch the trend of web application security testing and secure development.},
	number = {9},
	urldate = {2024-11-11},
	journal = {Journal of King Saud University - Computer and Information Sciences},
	author = {Aydos, Murat and Aldan, Çiğdem and Coşkun, Evren and Soydan, Alperen},
	month = oct,
	year = {2022},
	keywords = {Security testing, Systematic literature mapping, Systematic literature review, Systematic mapping, Web application security},
	pages = {6775--6792},
}

@article{jaiswal_security_2014,
	title = {Security {Testing} of {Web} {Applications}: {Issues} and {Challenges}},
	volume = {88},
	issn = {09758887},
	shorttitle = {Security {Testing} of {Web} {Applications}},
	url = {http://research.ijcaonline.org/volume88/number3/pxc3893667.pdf},
	doi = {10.5120/15334-3667},
	abstract = {Due to the increasing complexity of web systems, security testing has become indispensable and critical activity of web application development life cycle. Security testing aims to maintain the confidentiality of the data, to check against any information leakage and to maintain the functionality as intended. It checks whether the security requirements are fulfilled by the web applications when they are subjected to malicious input data. Due to the rising explosion in the security vulnerabilities, there occurs a need to understand its unique challenges and issues which will eventually serve as a useful input for the security testing tool developers and test managers for their relative projects.},
	language = {en},
	number = {3},
	urldate = {2024-11-11},
	journal = {International Journal of Computer Applications},
	author = {Jaiswal, Arunima and Raj, Gaurav and Singh, Dheerendra},
	month = feb,
	year = {2014},
	pages = {26--32},
}

@inproceedings{loring_expose_2017,
	address = {New York, NY, USA},
	series = {{SPIN} 2017},
	title = {{ExpoSE}: practical symbolic execution of standalone {JavaScript}},
	isbn = {978-1-4503-5077-8},
	shorttitle = {{ExpoSE}},
	url = {https://doi.org/10.1145/3092282.3092295},
	doi = {10.1145/3092282.3092295},
	abstract = {JavaScript has evolved into a versatile ecosystem for not just the web, but also a wide range of server-side and client-side applications. With this increased scope, the potential impact of bugs increases. We introduce ExpoSE, a dynamic symbolic execution engine for Node.js applications. ExpoSE automatically generates test cases to find bugs and cover as many paths in the target program as possible. We discuss the specific challenges for symbolic execution arising from the widespread use of regular expressions in such applications. In particular, we make explicit the issues of capture groups, backreferences, and greediness in JavaScript's flavor of regular expressions, and our models improve over previous work that only partially addressed these. We evaluate ExpoSE on three popular JavaScript libraries that make heavy use of regular expressions, and we report a previously unknown bug in the Minimist library.},
	urldate = {2024-10-28},
	booktitle = {Proceedings of the 24th {ACM} {SIGSOFT} {International} {SPIN} {Symposium} on {Model} {Checking} of {Software}},
	publisher = {Association for Computing Machinery},
	author = {Loring, Blake and Mitchell, Duncan and Kinder, Johannes},
	month = jul,
	year = {2017},
	pages = {196--199},
}

@inproceedings{eriksson_black_2021,
	title = {Black {Widow}: {Blackbox} {Data}-driven {Web} {Scanning}},
	shorttitle = {Black {Widow}},
	url = {https://ieeexplore.ieee.org/abstract/document/9519452},
	doi = {10.1109/SP40001.2021.00022},
	abstract = {Modern web applications are an integral part of our digital lives. As we put more trust in web applications, the need for security increases. At the same time, detecting vulnerabilities in web applications has become increasingly hard, due to the complexity, dynamism, and reliance on third-party components. Blackbox vulnerability scanning is especially challenging because (i) for deep penetration of web applications scanners need to exercise such browsing behavior as user interaction and asynchrony, and (ii) for detection of nontrivial injection attacks, such as stored cross-site scripting (XSS), scanners need to discover inter-page data dependencies.This paper illuminates key challenges for crawling and scanning the modern web. Based on these challenges we identify three core pillars for deep crawling and scanning: navigation modeling, traversing, and tracking inter-state dependencies. While prior efforts are largely limited to the separate pillars, we suggest an approach that leverages all three. We develop Black Widow, a blackbox data-driven approach to web crawling and scanning. We demonstrate the effectiveness of the crawling by code coverage improvements ranging from 63\% to 280\% compared to other crawlers across all applications. Further, we demonstrate the effectiveness of the web vulnerability scanning by featuring no false positives and finding more cross-site scripting vulnerabilities than previous methods. In older applications, used in previous research, we find vulnerabilities that the other methods miss. We also find new vulnerabili-ties in production software, including HotCRP, osCommerce, PrestaShop and WordPress.},
	urldate = {2024-10-28},
	booktitle = {2021 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	author = {Eriksson, Benjamin and Pellegrino, Giancarlo and Sabelfeld, Andrei},
	month = may,
	year = {2021},
	note = {ISSN: 2375-1207},
	keywords = {Crawlers, Cross-site scripting, Distance measurement, Navigation, Privacy, Production, Software, XSS, cross-site scripting, security testing, web application scanning, web crawling},
	pages = {1125--1142},
}

@inproceedings{loring_sound_2019,
	address = {New York, NY, USA},
	series = {{PLDI} 2019},
	title = {Sound regular expression semantics for dynamic symbolic execution of {JavaScript}},
	isbn = {978-1-4503-6712-7},
	url = {https://doi.org/10.1145/3314221.3314645},
	doi = {10.1145/3314221.3314645},
	abstract = {Support for regular expressions in symbolic execution-based tools for test generation and bug finding is insufficient. Common aspects of mainstream regular expression engines, such as backreferences or greedy matching, are ignored or imprecisely approximated, leading to poor test coverage or missed bugs. In this paper, we present a model for the complete regular expression language of ECMAScript 2015 (ES6), which is sound for dynamic symbolic execution of the test and exec functions. We model regular expression operations using string constraints and classical regular expressions and use a refinement scheme to address the problem of matching precedence and greediness. We implemented our model in ExpoSE, a dynamic symbolic execution engine for JavaScript, and evaluated it on over 1,000 Node.js packages containing regular expressions, demonstrating that the strategy is effective and can significantly increase the number of successful regular expression queries and therefore boost coverage.},
	urldate = {2024-10-28},
	booktitle = {Proceedings of the 40th {ACM} {SIGPLAN} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {Association for Computing Machinery},
	author = {Loring, Blake and Mitchell, Duncan and Kinder, Johannes},
	month = jun,
	year = {2019},
	pages = {425--438},
}

@inproceedings{eriksson_black_2023,
	address = {New York, NY, USA},
	series = {{CCS} '23},
	title = {Black {Ostrich}: {Web} {Application} {Scanning} with {String} {Solvers}},
	isbn = {9798400700507},
	shorttitle = {Black {Ostrich}},
	url = {https://dl.acm.org/doi/10.1145/3576915.3616582},
	doi = {10.1145/3576915.3616582},
	abstract = {Securing web applications remains a pressing challenge. Unfortunately, the state of the art in web crawling and security scanning still falls short of deep crawling. A major roadblock is the crawlers' limited ability to pass input validation checks when web applications require data of a certain format, such as email, phone number, or zip code. This paper develops Black Ostrich, a principled approach to deep web crawling and scanning. The key idea is to equip web crawling with string constraint solving capabilities to dynamically infer suitable inputs from regular expression patterns in web applications and thereby pass input validation checks. To enable this use of constraint solvers, we develop new automata-based techniques to process JavaScript regular expressions. We implement our approach extending and combining the Ostrich constraint solver with the Black Widow web crawler. We evaluate Black Ostrich on a set of 8,820 unique validation patterns gathered from over 21,667,978 forms from a combination of the July 2021 Common{\textasciitilde}Crawl and Tranco top 100K. For these forms and reconstructions of input elements corresponding to the patterns, we demonstrate that Black Ostrich achieves a 99\% coverage of the form validations compared to an average of 36\% for the state-of-the-art scanners. Moreover, out of the 66,377 domains using these patterns, we solve all patterns on 66,309 (99\%) while the combined efforts of the other scanners cover 52,632 (79\%). We further show that our approach can boost coverage by evaluating it on three open-source applications. Our empirical studies include a study of email validation patterns, where we find that 213 (26\%) out of the 825 found email validation patterns liberally admit XSS injection payloads.},
	urldate = {2024-10-28},
	booktitle = {Proceedings of the 2023 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {Association for Computing Machinery},
	author = {Eriksson, Benjamin and Stjerna, Amanda and De Masellis, Riccardo and Rüemmer, Philipp and Sabelfeld, Andrei},
	month = nov,
	year = {2023},
	pages = {549--563},
}
