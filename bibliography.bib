
@inproceedings{van_sprundel_fuzzing_2005,
	location = {Berlin, Germany},
	title = {Fuzzing: Breaking software in an automated fashion},
	url = {https://events.ccc.de/congress/2005/fahrplan/attachments/582-paper_fuzzing.pdf},
	booktitle = {Chaos Communication Congress},
	publisher = {Chaos Computer Club},
	author = {van Sprundel, Ilja},
	date = {2005-08-12},
}

@inproceedings{silva_grasp_1996,
	title = {{GRASP} - a new search algorithm for satisfiability},
	url = {https://doi.org/10.1109/ICCAD.1996.569607},
	doi = {10.1109/ICCAD.1996.569607},
	pages = {220--227},
	booktitle = {Proceedings of the 1996 {IEEE}/{ACM} International Conference on Computer-Aided Design, {ICCAD} 1996, San Jose, {CA}, {USA}, November 10-14, 1996},
	author = {Silva, João P. Marques and Sakallah, Karem A.},
	date = {1996},
}

@inproceedings{ryan_sylvia_2023,
	title = {Sylvia: Countering the Path Explosion Problem in the Symbolic Execution of Hardware Designs},
	url = {https://doi.org/10.34727/2023/isbn.978-3-85448-060-0\_19},
	doi = {10.34727/2023/ISBN.978-3-85448-060-0_19},
	pages = {110--121},
	booktitle = {Formal Methods in Computer-Aided Design, {FMCAD} 2023, Ames, {IA}, {USA}, October 24-27, 2023},
	author = {Ryan, Kaki and Sturton, Cynthia},
	date = {2023},
}

@inproceedings{paul_end--end_2001,
	title = {End-to-End Integration Testing},
	url = {https://doi.org/10.1109/APAQS.2001.990022},
	doi = {10.1109/APAQS.2001.990022},
	pages = {211--222},
	booktitle = {2nd Asia-Pacific Conference on Quality Software ({APAQS} 2001), 10-11 December 2001, Hong Kong, China, Proceedings},
	author = {Paul, Raymond A.},
	date = {2001},
}

@report{barrett_smt-lib_2025,
	title = {The {SMT}-{LIB} Standard: Version 2.7},
	institution = {Department of Computer Science, The University of Iowa},
	author = {Barrett, Clark and Fontaine, Pascal and Tinelli, Cesare},
	date = {2025},
}

@article{loring_systematic_2021,
	title = {Systematic Generation of Conformance Tests for {JavaScript}},
	volume = {abs/2108.07075},
	url = {https://arxiv.org/abs/2108.07075},
	journaltitle = {{CoRR}},
	author = {Loring, Blake and Kinder, Johannes},
	date = {2021},
	eprinttype = {arxiv},
	eprint = {2108.07075},
}

@inproceedings{kuznetsov_efficient_2012,
	title = {Efficient state merging in symbolic execution},
	url = {https://doi.org/10.1145/2254064.2254088},
	doi = {10.1145/2254064.2254088},
	pages = {193--204},
	booktitle = {{ACM} {SIGPLAN} Conference on Programming Language Design and Implementation, {PLDI} '12, Beijing, China - June 11 - 16, 2012},
	author = {Kuznetsov, Volodymyr and Kinder, Johannes and Bucur, Stefan and Candea, George},
	date = {2012},
}

@article{nieuwenhuis_solving_2006,
	title = {Solving {SAT} and {SAT} Modulo Theories: From an abstract Davis–Putnam–Logemann–Loveland procedure to {DPLL}(T)},
	volume = {53},
	issn = {0004-5411},
	url = {https://doi.org/10.1145/1217856.1217859},
	doi = {10.1145/1217856.1217859},
	abstract = {We first introduce Abstract {DPLL}, a rule-based formulation of the Davis–Putnam–Logemann–Loveland ({DPLL}) procedure for propositional satisfiability. This abstract framework allows one to cleanly express practical {DPLL} algorithms and to formally reason about them in a simple way. Its properties, such as soundness, completeness or termination, immediately carry over to the modern {DPLL} implementations with features such as backjumping or clause learning.We then extend the framework to Satisfiability Modulo background Theories ({SMT}) and use it to model several variants of the so-called lazy approach for {SMT}. In particular, we use it to introduce a few variants of a new, efficient and modular approach for {SMT} based on a general {DPLL}(X) engine, whose parameter X can be instantiated with a specialized solver {SolverT} for a given theory T, thus producing a {DPLL}(T) system. We describe the high-level design of {DPLL}(X) and its cooperation with {SolverT}, discuss the role of theory propagation, and describe different {DPLL}(T) strategies for some theories arising in industrial applications.Our extensive experimental evidence, summarized in this article, shows that {DPLL}(T) systems can significantly outperform the other state-of-the-art tools, frequently even in orders of magnitude, and have better scaling properties.},
	pages = {937--977},
	number = {6},
	journaltitle = {J. {ACM}},
	author = {Nieuwenhuis, Robert and Oliveras, Albert and Tinelli, Cesare},
	date = {2006-11},
	note = {Place: New York, {NY}, {USA}
Publisher: Association for Computing Machinery},
	keywords = {{SAT} solvers, Satisfiability Modulo Theories},
}

@inproceedings{moskewicz_chaff_2001,
	title = {Chaff: Engineering an Efficient {SAT} Solver},
	url = {https://doi.org/10.1145/378239.379017},
	doi = {10.1145/378239.379017},
	pages = {530--535},
	booktitle = {Proceedings of the 38th Design Automation Conference, {DAC} 2001, Las Vegas, {NV}, {USA}, June 18-22, 2001},
	author = {Moskewicz, Matthew W. and Madigan, Conor F. and Zhao, Ying and Zhang, Lintao and Malik, Sharad},
	date = {2001},
}

@incollection{boyer_integrating_1988,
	location = {{USA}},
	title = {Integrating decision procedures into heuristic theorem provers: a case study of linear arithmetic},
	isbn = {0-19-853718-2},
	pages = {83--124},
	booktitle = {Machine Intelligence 11},
	publisher = {Oxford University Press, Inc.},
	author = {Boyer, R. S. and Moore, J. S.},
	date = {1988},
}

@article{cadar_exe_2008,
	title = {{EXE}: Automatically Generating Inputs of Death},
	volume = {12},
	url = {https://doi.org/10.1145/1455518.1455522},
	doi = {10.1145/1455518.1455522},
	pages = {10:1--10:38},
	number = {2},
	journaltitle = {{ACM} Trans. Inf. Syst. Secur.},
	author = {Cadar, Cristian and Ganesh, Vijay and Pawlowski, Peter M. and Dill, David L. and Engler, Dawson R.},
	date = {2008},
}

@inproceedings{godefroid_dart_2005,
	title = {{DART}: directed automated random testing},
	url = {https://doi.org/10.1145/1065010.1065036},
	doi = {10.1145/1065010.1065036},
	pages = {213--223},
	booktitle = {Proceedings of the {ACM} {SIGPLAN} 2005 Conference on Programming Language Design and Implementation, Chicago, {IL}, {USA}, June 12-15, 2005},
	author = {Godefroid, Patrice and Klarlund, Nils and Sen, Koushik},
	date = {2005},
}

@inproceedings{godefroid_automated_2008,
	title = {Automated Whitebox Fuzz Testing},
	url = {https://www.ndss-symposium.org/ndss2008/automated-whitebox-fuzz-testing/},
	booktitle = {Proceedings of the Network and Distributed System Security Symposium, {NDSS} 2008, San Diego, California, {USA}, 10th February - 13th February 2008},
	author = {Godefroid, Patrice and Levin, Michael Y. and Molnar, David A.},
	date = {2008},
}

@article{godefroid_automated_nodate,
	title = {Automated Whitebox Fuzz Testing},
	abstract = {Fuzz testing is an effective technique for ﬁnding security vulnerabilities in software. Traditionally, fuzz testing tools apply random mutations to well-formed inputs of a program and test the resulting values. We present an alternative whitebox fuzz testing approach inspired by recent advances in symbolic execution and dynamic test generation. Our approach records an actual run of the program under test on a well-formed input, symbolically evaluates the recorded trace, and gathers constraints on inputs capturing how the program uses these. The collected constraints are then negated one by one and solved with a constraint solver, producing new inputs that exercise different control paths in the program. This process is repeated with the help of a code-coverage maximizing heuristic designed to ﬁnd defects as fast as possible. We have implemented this algorithm in {SAGE} (Scalable, Automated, Guided Execution), a new tool employing x86 instruction-level tracing and emulation for whitebox fuzzing of arbitrary ﬁle-reading Windows applications. We describe key optimizations needed to make dynamic test generation scale to large input ﬁles and long execution traces with hundreds of millions of instructions. We then present detailed experiments with several Windows applications. Notably, without any format-speciﬁc knowledge, {SAGE} detects the {MS}07-017 {ANI} vulnerability, which was missed by extensive blackbox fuzzing and static analysis tools. Furthermore, while still in an early stage of development, {SAGE} has already discovered 30+ new bugs in large shipped Windows applications including image processors, media players, and ﬁle decoders. Several of these bugs are potentially exploitable memory access violations.},
	author = {Godefroid, Patrice and Levin, Michael Y and Molnar, David},
	langid = {english},
}

@article{umar_comparative_2021,
	title = {A Comparative Study Of Dynamic Software Testing Techniques},
	volume = {12},
	doi = {10.35444/IJANA.2020.12301},
	pages = {4575--4584},
	author = {Umar, Mubarak Albarka and Chen, Zhanfang},
	date = {2021-01},
}

@article{artho_combined_2005,
	title = {Combined Static and Dynamic Analysis},
	volume = {131},
	issn = {1571-0661},
	url = {https://www.sciencedirect.com/science/article/pii/S1571066105002537},
	doi = {https://doi.org/10.1016/j.entcs.2005.01.018},
	abstract = {Static analysis is usually faster than dynamic analysis but less precise. Therefore it is often desirable to retain information from static analysis for run-time verification, or to compare the results of both techniques. However, this requires writing two programs, which may not act identically under the same conditions. It would be desirable to share the same generic algorithm by static and dynamic analysis. In {JNuke}, a framework for static and dynamic analysis of Java programs, this has been achieved. By keeping the architecture of static analysis similar to a virtual machine, the only key difference between abstract interpretation and execution remains the nature of program states. In dynamic analysis, concrete states are available, while in static analysis, sets of (abstract) states are considered. Our new analysis is generic because it can re-use the same algorithm in static analysis and dynamic analysis. This paper describes the architecture of such a generic analysis. To our knowledge, {JNuke} is the first tool that has achieved this integration, which enables static and dynamic analysis to interact in novel ways.},
	pages = {3--14},
	journaltitle = {Electronic Notes in Theoretical Computer Science},
	author = {Artho, Cyrille and Biere, Armin},
	date = {2005},
	keywords = {Java, Static analysis, dynamic analysis},
}

@article{umar_comparative_2020,
	title = {A Comparative Study Of Dynamic Software Testing Techniques},
	volume = {12},
	issn = {09750290, 09750282},
	url = {https://www.ijana.in/download12-3-1.php?file=V12I3-1.pdf},
	doi = {10.35444/IJANA.2020.12301},
	pages = {4575--4584},
	number = {3},
	journaltitle = {International Journal of Advanced Networking and Applications},
	shortjournal = {{IJANA}},
	author = {Umar, Mubarak Albarka and Zhanfang, Chen},
	urldate = {2025-04-05},
	date = {2020},
	langid = {english},
}

@article{hennell_comparison_1990,
	title = {A Comparison of Static and Dynamic Conformance Analyses},
	volume = {23},
	issn = {1474-6670},
	url = {https://www.sciencedirect.com/science/article/pii/S1474667017521886},
	doi = {https://doi.org/10.1016/S1474-6670(17)52188-6},
	abstract = {The paper examines the strengths and weaknesses of Static and Dynamic Analysis when used to show conformance with a set of assertions.},
	pages = {119--124},
	number = {6},
	journaltitle = {{IFAC} Proceedings Volumes},
	author = {Hennell, M. A. and Fergus, E.},
	date = {1990},
	keywords = {Dynamic Analysis \& Conformance, Static Analysis},
}

@software{openai_chatgpt_2025,
	title = {{ChatGPT}},
	url = {https://chat.openai.com/chat.},
	version = {4.0},
	author = {{OpenAI}},
	date = {2025-03-31},
}

@inproceedings{xu_symbolic_2024,
	title = {Symbolic Execution with Test Cases Generated by Large Language Models},
	doi = {10.1109/QRS62785.2024.00031},
	pages = {228--237},
	booktitle = {2024 {IEEE} 24th International Conference on Software Quality, Reliability and Security ({QRS})},
	author = {Xu, Jiahe and Xu, Jingwei and Chen, Taolue and Ma, Xiaoxing},
	date = {2024},
	keywords = {Codes, Large language models, Manuals, Pipelines, Reliability engineering, Software quality, Software reliability, large language model, software testing, symbolic execution},
}

@inproceedings{trabish_chopped_2018,
	location = {Gothenburg Sweden},
	title = {Chopped symbolic execution},
	isbn = {978-1-4503-5638-1},
	url = {https://dl.acm.org/doi/10.1145/3180155.3180251},
	doi = {10.1145/3180155.3180251},
	abstract = {Symbolic execution is a powerful program analysis technique that systematically explores multiple program paths. However, despite important technical advances, symbolic execution often struggles to reach deep parts of the code due to the well-known path explosion problem and constraint solving limitations.},
	eventtitle = {{ICSE} '18: 40th International Conference on Software Engineering},
	pages = {350--360},
	booktitle = {Proceedings of the 40th International Conference on Software Engineering},
	publisher = {{ACM}},
	author = {Trabish, David and Mattavelli, Andrea and Rinetzky, Noam and Cadar, Cristian},
	urldate = {2025-03-31},
	date = {2018-05-27},
	langid = {english},
}

@article{cadar_exe_nodate,
	title = {{EXE}: Automatically Generating Inputs of Death},
	abstract = {This paper presents {EXE}, an eﬀective bug-ﬁnding tool that automatically generates inputs that crash real code. Instead of running code on manually or randomly constructed input, {EXE} runs it on symbolic input initially allowed to be “anything.” As checked code runs, {EXE} tracks the constraints on each symbolic (i.e., input-derived) memory location. If a statement uses a symbolic value, {EXE} does not run it, but instead adds it as an input-constraint; all other statements run as usual. If code conditionally checks a symbolic expression, {EXE} forks execution, constraining the expression to be true on the true branch and false on the other. Because {EXE} reasons about all possible values on a path, it has much more power than a traditional runtime tool: (1) it can force execution down any feasible program path and (2) at dangerous operations (e.g., a pointer dereference), it detects if the current path constraints allow any value that causes a bug. When a path terminates or hits a bug, {EXE} automatically generates a test case by solving the current path constraints to ﬁnd concrete values using its own codesigned constraint solver, {STP}. Because {EXE}’s constraints have no approximations, feeding this concrete input to an uninstrumented version of the checked code will cause it to follow the same path and hit the same bug (assuming deterministic code).},
	author = {Cadar, Cristian and Ganesh, Vijay and Pawlowski, Peter M and Dill, David L and Engler, Dawson R},
	langid = {english},
}

@article{chipounov_selective_nodate,
	title = {Selective Symbolic Execution},
	abstract = {Symbolic execution is a powerful technique for analyzing program behavior, ﬁnding bugs, and generating tests, but suffers from severely limited scalability: the largest programs that can be symbolically executed today are on the order of thousands of lines of code. To ensure feasibility of symbolic execution, even small programs must curtail their interactions with libraries, the operating system, and hardware devices. This paper introduces selective symbolic execution, a technique for creating the illusion of fullsystem symbolic execution, while symbolically running only the code that is of interest to the developer. We describe a prototype that can symbolically execute arbitrary portions of a full system, including applications, libraries, operating system, and device drivers. It seamlessly transitions back and forth between symbolic and concrete execution, while transparently converting system state from symbolic to concrete and back. Our technique makes symbolic execution practical for large software that runs in real environments, without requiring explicit modeling of these environments.},
	author = {Chipounov, Vitaly and Georgescu, Vlad and Zamﬁr, Cristian and Candea, George},
	langid = {english},
}

@article{barrett_smt-lib_nodate,
	title = {The {SMT}-{LIB} Standard},
	author = {Barrett, Clark and Stump, Aaron},
	langid = {english},
}

@incollection{hutchison_smtinterpol_2012,
	location = {Berlin, Heidelberg},
	title = {{SMTInterpol}: An Interpolating {SMT} Solver},
	volume = {7385},
	isbn = {978-3-642-31758-3 978-3-642-31759-0},
	url = {http://link.springer.com/10.1007/978-3-642-31759-0_19},
	shorttitle = {{SMTInterpol}},
	abstract = {Craig interpolation is an active research topic and has become a powerful technique in veriﬁcation. We present {SMTInterpol}, an interpolating {SMT} solver for the quantiﬁer-free fragment of the combination of the theory of uninterpreted functions and the theory of linear arithmetic over integers and reals. {SMTInterpol} is {SMTLIB} 2 compliant and available under an open source software license ({LGPL} v3).},
	pages = {248--254},
	booktitle = {Model Checking Software},
	publisher = {Springer Berlin Heidelberg},
	author = {Christ, Jürgen and Hoenicke, Jochen and Nutz, Alexander},
	editor = {Donaldson, Alastair and Parker, David},
	editorb = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard},
	editorbtype = {redactor},
	urldate = {2025-03-30},
	date = {2012},
	langid = {english},
	doi = {10.1007/978-3-642-31759-0_19},
	note = {Series Title: Lecture Notes in Computer Science},
}

@inproceedings{barbosa_cvc5_2022,
	title = {cvc5: A Versatile and Industrial-Strength {SMT} Solver},
	volume = {13243},
	url = {https://doi.org/10.1007/978-3-030-99524-9\_24},
	doi = {10.1007/978-3-030-99524-9_24},
	series = {Lecture Notes in Computer Science},
	pages = {415--442},
	booktitle = {Tools and Algorithms for the Construction and Analysis of Systems - 28th International Conference, {TACAS} 2022, Held as Part of the European Joint Conferences on Theory and Practice of Software, {ETAPS} 2022, Munich, Germany, April 2-7, 2022, Proceedings, Part I},
	publisher = {Springer},
	author = {Barbosa, Haniel and Barrett, Clark W. and Brain, Martin and Kremer, Gereon and Lachnitt, Hanna and Mann, Makai and Mohamed, Abdalrhman and Mohamed, Mudathir and Niemetz, Aina and Nötzli, Andres and Ozdemir, Alex and Preiner, Mathias and Reynolds, Andrew and Sheng, Ying and Tinelli, Cesare and Zohar, Yoni},
	editor = {Fisman, Dana and Rosu, Grigore},
	date = {2022},
}

@incollection{biere_chapter_2021,
	title = {Chapter 33. Satisfiability Modulo Theories},
	isbn = {978-1-64368-160-3 978-1-64368-161-0},
	url = {http://ebooks.iospress.nl/doi/10.3233/FAIA201017},
	abstract = {Applications in artificial intelligence, formal verification, and other areas have greatly benefited from the recent advances in {SAT}. It is often the case, however, that applications in these fields require determining the satisfiability of formulas in more expressive logics such as first-order logic. Also, these applications typically require not general first-order satisfiability, but rather satisfiability with respect to some background theory, which fixes the interpretations of certain predicate and function symbols. For many background theories, specialized methods yield decision procedures for the satisfiability of quantifier-free formulas or some subclass thereof. Specialized decision procedures have been discovered for a long and still growing list of theories with practical applications. These include the theory of equality, various theories of arithmetic, and certain theories of arrays, as well as theories of lists, tuples, records, and bit-vectors of a fixed or arbitrary finite size. The research field concerned with determining the satisfiability of formulas with respect to some background theory is called Satisfiability Modulo Theories ({SMT}). This chapter provides a brief overview of {SMT} together with references to the relevant literature for a deeper study. It begins with an overview of techniques for solving {SMT} problems by encodings to {SAT}. The rest of the chapter is mostly concerned with an alternative approach in which a {SAT} solver is integrated with a separate decision procedure (called a theory solver) for conjunctions of literals in the background theory. After presenting this approach as a whole, the chapter provides more details on how to construct and combine theory solvers, and discusses several extensions and enhancements.},
	booktitle = {Frontiers in Artificial Intelligence and Applications},
	publisher = {{IOS} Press},
	author = {Barrett, Clark and Sebastiani, Roberto and Seshia, Sanjit A. and Tinelli, Cesare},
	editor = {Biere, Armin and Heule, Marijn and Van Maaren, Hans and Walsh, Toby},
	urldate = {2025-03-30},
	date = {2021-02-02},
	langid = {english},
	doi = {10.3233/FAIA201017},
}

@article{nieuwenhuis_solving_nodate,
	title = {Solving {SAT} and {SAT} Modulo Theories: from an Abstract Davis-Putnam-Logemann-Loveland Procedure to {DPLL}(T)},
	abstract = {{DPLL} formalism, we discuss in a clean and uniform way properties such as soundness, completeness, and termination. These properties immediately carry over to modern {DPLL} implementations with features such as backjumping and learning.},
	journaltitle = {Journal of the {ACM}},
	author = {Nieuwenhuis, Robert and Oliveras, Albert},
	langid = {english},
}

@incollection{bonacina_strategy_2013,
	location = {Berlin, Heidelberg},
	title = {The Strategy Challenge in {SMT} Solving},
	volume = {7788},
	isbn = {978-3-642-36674-1 978-3-642-36675-8},
	url = {http://link.springer.com/10.1007/978-3-642-36675-8_2},
	abstract = {High-performance {SMT} solvers contain many tightly integrated, hand-crafted heuristic combinations of algorithmic proof methods. While these heuristic combinations tend to be highly tuned for known classes of problems, they may easily perform badly on classes of problems not anticipated by solver developers. This issue is becoming increasingly pressing as {SMT} solvers begin to gain the attention of practitioners in diverse areas of science and engineering. We present a challenge to the {SMT} community: to develop methods through which users can exert strategic control over core heuristic aspects of {SMT} solvers. We present evidence that the adaptation of ideas of strategy prevalent both within the Argonne and {LCF} theorem proving paradigms can go a long way towards realizing this goal.},
	pages = {15--44},
	booktitle = {Automated Reasoning and Mathematics},
	publisher = {Springer Berlin Heidelberg},
	author = {De Moura, Leonardo and Passmore, Grant Olney},
	editor = {Bonacina, Maria Paola and Stickel, Mark E.},
	urldate = {2025-03-30},
	date = {2013},
	langid = {english},
	doi = {10.1007/978-3-642-36675-8_2},
	note = {Series Title: Lecture Notes in Computer Science},
}

@inproceedings{cadar_klee_2008,
	title = {{KLEE}: Unassisted and Automatic Generation of High-Coverage Tests for Complex Systems Programs},
	url = {http://www.usenix.org/events/osdi08/tech/full\_papers/cadar/cadar.pdf},
	pages = {209--224},
	booktitle = {8th {USENIX} Symposium on Operating Systems Design and Implementation, {OSDI} 2008, December 8-10, 2008, San Diego, California, {USA}, Proceedings},
	author = {Cadar, Cristian and Dunbar, Daniel and Engler, Dawson R.},
	date = {2008},
}

@article{chen_ostrich_nodate,
	title = {The {OSTRICH} String Solver},
	abstract = {This paper gives a high-level overview of the string solver {OSTRICH} version 1.2, a solver entering {SMTCOMP} 2022. For more details and theoretical results we refer to the full version of the paper [4] and to the website https://github.com/uuverifiers/ostrich.},
	author = {Chen, Taolue and Masellis, Riccardo De and Flores-Lamas, Alejandro and Hague, Matthew and Han, Zhilei and Hu, Denghang and Kan, Shuanglong and Lin, Anthony W and Markgraf, Oliver and Rümmer, Philipp and Stjerna, Amanda and Wu, Zhilin},
	langid = {english},
}

@inproceedings{wassermann_static_2008,
	location = {Leipzig, Germany},
	title = {Static detection of cross-site scripting vulnerabilities},
	isbn = {978-1-60558-079-1},
	url = {http://portal.acm.org/citation.cfm?doid=1368088.1368112},
	doi = {10.1145/1368088.1368112},
	abstract = {Web applications function in many of our daily activities, but they often have security problems, and their accessibility makes them easy to exploit. In cross-site scripting ({XSS}), an attacker exploits the trust a web client (browser) has for a trusted server and executes injected script on the browser with the server’s privileges. In 2006, {XSS} constituted the largest class of newly reported vulnerabilities making it the most prevalent class of attacks today. Web applications have {XSS} vulnerabilities because the validation they perform on untrusted input does not suﬃce to prevent that input from invoking a browser’s {JavaScript} (or other script) interpreter, and this validation is particularly diﬃcult to get right if it must admit some {HTML} mark-up. Most existing approaches to ﬁnding {XSS} vulnerabilities are taint-based, and taint-based approaches assume input validation functions to be adequate, so they either miss real vulnerabilities or report many false positives.},
	eventtitle = {the 13th international conference},
	pages = {171},
	booktitle = {Proceedings of the 13th international conference on Software engineering  - {ICSE} '08},
	publisher = {{ACM} Press},
	author = {Wassermann, Gary and Su, Zhendong},
	urldate = {2025-03-25},
	date = {2008},
	langid = {english},
}

@article{patil_cross_2011,
	title = {Cross Site Scripting: An Overview},
	abstract = {This paper describes the security attacks and specially focuses on Cross Site Scripting attacks. It further also discusses types and several counter measures. The major problem faced by the web application is the parameter manipulation, through which the attackers are aiming to access the database. Generally web applications maintain same structure and value. In that, required information is being accessed by the identical variables and keywords through web parameters. Parameter manipulation is the major issue in the web application used by the attacker to manipulate the parameter being sent by the browser and executed by the server.},
	journaltitle = {Intelligent Systems},
	author = {Patil, Vishwajit S and Bamnote, Dr G R and Nair, Sanil S},
	date = {2011},
	langid = {english},
}

@online{noauthor_owasp_2025,
	title = {{OWASP} Top Ten},
	url = {https://owasp.org/www-project-top-ten/},
	shorttitle = {{OWASP} Top Ten {\textbar} {OWASP} Foundation https},
	abstract = {The {OWASP} Top 10 is the reference standard for the most critical web application security risks. Adopting the {OWASP} Top 10 is perhaps the most effective first step towards changing your software development culture focused on producing secure code.},
	titleaddon = {owasp},
	urldate = {2025-03-25},
	date = {2025-03-23},
	langid = {english},
}

@article{tang_identifying_2012,
	title = {Identifying Cross-Site Scripting Attacks Based on {URL} Analysis},
	volume = {2},
	doi = {10.5815/ijem.2012.05.08},
	pages = {52--61},
	journaltitle = {International Journal of Engineering and Manufacturing},
	author = {Tang, Zhihua and Zheng, Ning and Xu, Ming},
	date = {2012-10},
}

@online{noauthor_cross_nodate,
	title = {Cross Site Scripting Prevention - {OWASP} Cheat Sheet Series https://cheatsheetseries.owasp.org/cheatsheets/Cross\_Site\_Scripting\_Prevention\_Cheat\_Sheet.html},
	url = {https://cheatsheetseries.owasp.org/cheatsheets/Cross_Site_Scripting_Prevention_Cheat_Sheet.html},
	urldate = {2025-03-25},
}

@inproceedings{kiezun_automatic_2009,
	title = {Automatic creation of {SQL} Injection and cross-site scripting attacks},
	url = {https://doi.org/10.1109/ICSE.2009.5070521},
	doi = {10.1109/ICSE.2009.5070521},
	pages = {199--209},
	booktitle = {31st International Conference on Software Engineering, {ICSE} 2009, May 16-24, 2009, Vancouver, Canada, Proceedings},
	author = {Kiezun, Adam and Guo, Philip J. and Jayaraman, Karthick and Ernst, Michael D.},
	date = {2009},
}

@inproceedings{kieyzun_automatic_2009,
	location = {Vancouver, {BC}, Canada},
	title = {Automatic creation of {SQL} Injection and cross-site scripting attacks},
	isbn = {978-1-4244-3453-4},
	url = {http://ieeexplore.ieee.org/document/5070521/},
	doi = {10.1109/ICSE.2009.5070521},
	abstract = {We present a technique for ﬁnding security vulnerabilities in Web applications. {SQL} Injection ({SQLI}) and crosssite scripting ({XSS}) attacks are widespread forms of attack in which the attacker crafts the input to the application to access or modify user data and execute malicious code. In the most serious attacks (called second-order, or persistent, {XSS}), an attacker can corrupt a database so as to cause subsequent users to execute malicious code.},
	eventtitle = {2009 {IEEE} 31st International Conference on Software Engineering},
	pages = {199--209},
	booktitle = {2009 {IEEE} 31st International Conference on Software Engineering},
	publisher = {{IEEE}},
	author = {Kieyzun, Adam and Guo, Philip J. and Jayaraman, Karthick and Ernst, Michael D.},
	urldate = {2025-03-25},
	date = {2009},
	langid = {english},
}

@article{moskewicz_chaff_nodate,
	title = {Chaff: Engineering an Efficient {SAT} Solver},
	abstract = {Boolean Satisfiability is probably the most studied of combinatorial optimization/search problems. Significant effort has been devoted to trying to provide practical solutions to this problem for problem instances encountered in a range of applications in Electronic Design Automation ({EDA}), as well as in Artificial Intelligence ({AI}). This study has culminated in the development of several {SAT} packages, both proprietary and in the public domain (e.g. {GRASP}, {SATO}) which find significant use in both research and industry. Most existing complete solvers are variants of the Davis-Putnam ({DP}) search algorithm. In this paper we describe the development of a new complete solver, Chaff, which achieves significant performance gains through careful engineering of all aspects of the search – especially a particularly efficient implementation of Boolean constraint propagation ({BCP}) and a novel low overhead decision strategy. Chaff has been able to obtain one to two orders of magnitude performance improvement on difficult {SAT} benchmarks in comparison with other solvers ({DP} or otherwise), including {GRASP} and {SATO}.},
	author = {Moskewicz, Matthew W and Madigan, Conor F and Zhao, Ying and Zhang, Lintao and Malik, Sharad},
	langid = {english},
}

@article{biere_symbolic_nodate,
	title = {Symbolic Model Checking without {BDDs}?},
	abstract = {Symbolic Model Checking [3, 14] has proven to be a powerful technique for the veriﬁcation of reactive systems. {BDDs} [2] have traditionally been used as a symbolic representation of the system. In this paper we show how boolean decision procedures, like St˚almarck’s Method [16] or the Davis \& Putnam Procedure [7], can replace {BDDs}. This new technique avoids the space blow up of {BDDs}, generates counterexamples much faster, and sometimes speeds up the veriﬁcation. In addition, it produces counterexamples of minimal length. We introduce a bounded model checking procedure for {LTL} which reduces model checking to propositional satisﬁability. We show that bounded {LTL} model checking can be done without a tableau construction. We have implemented a model checker {BMC}, based on bounded model checking, and preliminary results are presented.},
	author = {Biere, Armin and Cimatti, Alessandro and Clarke, Edmund and Zhu, Yunshan},
	langid = {english},
}

@article{silva_graspnew_nodate,
	title = {{GRASP}—A New Search Algorithm for Satisﬁability},
	author = {Silva, João P Marques and Sakallah, Karem A},
	langid = {english},
}

@article{boyer_integrating_1988-1,
	title = {Integrating decision procedures into heuristic theorem provers: a case study of linear arithmetic},
	pages = {83--124},
	journaltitle = {Machine intelligence},
	author = {Boyer, Robert S. and Moore, J. Strother},
	date = {1988},
}

@article{shostak_algorithm_1978,
	title = {An algorithm for reasoning about equality},
	volume = {21},
	issn = {0001-0782},
	url = {https://doi.org/10.1145/359545.359570},
	doi = {10.1145/359545.359570},
	abstract = {A simple technique for reasoning about equalities that is fast and complete for ground formulas with function symbols and equality is presented. A proof of correctness is given as well.},
	pages = {583--585},
	number = {7},
	journaltitle = {Commun. {ACM}},
	author = {Shostak, Robert E.},
	date = {1978-07},
	note = {Place: New York, {NY}, {USA}
Publisher: Association for Computing Machinery},
	keywords = {deduction, equality, program verification, theorem proving},
}

@article{nelson_fast_1980,
	title = {Fast Decision Procedures Based on Congruence Closure},
	volume = {27},
	issn = {0004-5411},
	url = {https://doi.org/10.1145/322186.322198},
	doi = {10.1145/322186.322198},
	abstract = {The notion of the congruence closure of a relation on a graph is defined and several algorithms for computing it are surveyed. A simple proof is given that the congruence closure algorithm provides a decision procedure for the quantifier-free theory of equality. A decision procedure is then given for the quantifier-free theory of {LISP} list structure based on the congruence closure algorithm. Both decision procedures determine the satisfiability of a conjunction of literals of length n in average time O(n log n) using the fastest known congruence closure algorithm. It is also shown that if the axiomatization of the theory of list structure is changed slightly, the problem of determining the satisfiability of a conjunction of literals becomes {NP}-complete. The decision procedures have been implemented in the authors' simplifier for the Stanford Pascal Verifier.},
	pages = {356--364},
	number = {2},
	journaltitle = {J. {ACM}},
	author = {Nelson, Greg and Oppen, Derek C.},
	date = {1980-04},
	note = {Place: New York, {NY}, {USA}
Publisher: Association for Computing Machinery},
}

@inproceedings{paul_end--end_2001-1,
	title = {End-to-end integration testing},
	doi = {10.1109/APAQS.2001.990022},
	pages = {211--220},
	booktitle = {Proceedings Second Asia-Pacific Conference on Quality Software},
	author = {Paul, R.},
	date = {2001},
	keywords = {Collaboration, Investments, Java, Object oriented modeling, Project management, Risk analysis, Software quality, Software testing, Statistical analysis, System testing},
}

@book{beck_test-driven_2003,
	title = {Test-driven Development - by example},
	isbn = {978-0-321-14653-3},
	series = {The Addison-Wesley signature series},
	publisher = {Addison-Wesley},
	author = {Beck, Kent L.},
	date = {2003},
}

@incollection{hutchison_cute_2006,
	location = {Berlin, Heidelberg},
	title = {{CUTE} and {jCUTE}: Concolic Unit Testing and Explicit Path Model-Checking Tools},
	volume = {4144},
	isbn = {978-3-540-37406-0 978-3-540-37411-4},
	url = {http://link.springer.com/10.1007/11817963_38},
	shorttitle = {{CUTE} and {jCUTE}},
	abstract = {{CUTE}, a Concolic Unit Testing Engine for C and Java, is a tool to systematically and automatically test sequential C programs (including pointers) and concurrent Java programs. {CUTE} combines concrete and symbolic execution in a way that avoids redundant test cases as well as false warnings. The tool also introduces a race-ﬂipping technique to eﬃciently test and model check concurrent programs with data inputs.},
	pages = {419--423},
	booktitle = {Computer Aided Verification},
	publisher = {Springer Berlin Heidelberg},
	author = {Sen, Koushik and Agha, Gul},
	editor = {Ball, Thomas and Jones, Robert B.},
	editorb = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard},
	editorbtype = {redactor},
	urldate = {2025-03-22},
	date = {2006},
	langid = {english},
	doi = {10.1007/11817963_38},
	note = {Series Title: Lecture Notes in Computer Science},
}

@inproceedings{balasubramanian_dynamic_2019,
	location = {Limassol Cyprus},
	title = {Dynamic symbolic execution for the analysis of web server applications in Java},
	isbn = {978-1-4503-5933-7},
	url = {https://dl.acm.org/doi/10.1145/3297280.3297494},
	doi = {10.1145/3297280.3297494},
	abstract = {Symbolic execution is a well-known program analysis technique that explores multiple program paths simultaneously. Among other things, it is used to uncover subtle bugs and corner cases in programs, as well as to produce high-coverage test suites. Even though symbolic execution has seen successful use in practice, there remain challenges in applying it to programs like web servers that use features such as multithreading and callbacks. This paper describes our dynamic symbolic execution framework for Java that was designed with these types of features in mind. Our framework uses bytecode instrumentation combined with a run-time agent to perform the symbolic execution. We give a detailed description of the challenges we faced along with our design choices. We also present benchmark results on various examples including programs that use web server frameworks.},
	eventtitle = {{SAC} '19: The 34th {ACM}/{SIGAPP} Symposium on Applied Computing},
	pages = {2178--2185},
	booktitle = {Proceedings of the 34th {ACM}/{SIGAPP} Symposium on Applied Computing},
	publisher = {{ACM}},
	author = {Balasubramanian, Daniel and Zhang, Zhenkai and {McDermet}, Dan and Karsai, Gabor},
	urldate = {2025-03-22},
	date = {2019-04-08},
	langid = {english},
}

@inproceedings{saxena_symbolic_2010,
	location = {{USA}},
	title = {A Symbolic Execution Framework for {JavaScript}},
	isbn = {978-0-7695-4035-1},
	url = {https://doi.org/10.1109/SP.2010.38},
	doi = {10.1109/SP.2010.38},
	series = {{SP} '10},
	abstract = {As {AJAX} applications gain popularity, client-side {JavaScript} code is becoming increasingly complex. However, few automated vulnerability analysis tools for {JavaScript} exist. In this paper, we describe the first system for exploring the execution space of {JavaScript} code using symbolic execution. To handle {JavaScript} code’s complex use of string operations, we design a new language of string constraints and implement a solver for it. We build an automatic end-to-end tool, Kudzu, and apply it to the problem of finding client-side code injection vulnerabilities. In experiments on 18 live web applications, Kudzu automatically discovers 2 previously unknown vulnerabilities and 9 more that were previously found only with a manually-constructed test suite.},
	pages = {513--528},
	booktitle = {Proceedings of the 2010 {IEEE} Symposium on Security and Privacy},
	publisher = {{IEEE} Computer Society},
	author = {Saxena, Prateek and Akhawe, Devdatta and Hanna, Steve and Mao, Feng and {McCamant}, Stephen and Song, Dawn},
	date = {2010},
}

@inproceedings{chipounov_s2e_2011,
	location = {New York, {NY}, {USA}},
	title = {S2E: a platform for in-vivo multi-path analysis of software systems},
	isbn = {978-1-4503-0266-1},
	url = {https://doi.org/10.1145/1950365.1950396},
	doi = {10.1145/1950365.1950396},
	series = {{ASPLOS} {XVI}},
	abstract = {This paper presents S2E, a platform for analyzing the properties and behavior of software systems. We demonstrate S2E's use in developing practical tools for comprehensive performance profiling, reverse engineering of proprietary software, and bug finding for both kernel-mode and user-mode binaries. Building these tools on top of S2E took less than 770 {LOC} and 40 person-hours each.S2E's novelty consists of its ability to scale to large real systems, such as a full Windows stack. S2E is based on two new ideas: selective symbolic execution, a way to automatically minimize the amount of code that has to be executed symbolically given a target analysis, and relaxed execution consistency models, a way to make principled performance/accuracy trade-offs in complex analyses. These techniques give S2E three key abilities: to simultaneously analyze entire families of execution paths, instead of just one execution at a time; to perform the analyses in-vivo within a real software stack–user programs, libraries, kernel, drivers, etc.–instead of using abstract models of these layers; and to operate directly on binaries, thus being able to analyze even proprietary software.Conceptually, S2E is an automated path explorer with modular path analyzers: the explorer drives the target system down all execution paths of interest, while analyzers check properties of each such path (e.g., to look for bugs) or simply collect information (e.g., count page faults). Desired paths can be specified in multiple ways, and S2E users can either combine existing analyzers to build a custom analysis tool, or write new analyzers using the S2E {API}.},
	pages = {265--278},
	booktitle = {Proceedings of the Sixteenth International Conference on Architectural Support for Programming Languages and Operating Systems},
	publisher = {Association for Computing Machinery},
	author = {Chipounov, Vitaly and Kuznetsov, Volodymyr and Candea, George},
	date = {2011},
	note = {event-place: Newport Beach, California, {USA}},
	keywords = {analysis, binary, consistency models, dbt, framework, in-vivo, performance, symbolic execution, testing, virtualization},
}

@article{chipounov_s2e_2011-1,
	title = {S2E: a platform for in-vivo multi-path analysis of software systems},
	volume = {39},
	issn = {0163-5964},
	url = {https://doi.org/10.1145/1961295.1950396},
	doi = {10.1145/1961295.1950396},
	abstract = {This paper presents S2E, a platform for analyzing the properties and behavior of software systems. We demonstrate S2E's use in developing practical tools for comprehensive performance profiling, reverse engineering of proprietary software, and bug finding for both kernel-mode and user-mode binaries. Building these tools on top of S2E took less than 770 {LOC} and 40 person-hours each.S2E's novelty consists of its ability to scale to large real systems, such as a full Windows stack. S2E is based on two new ideas: selective symbolic execution, a way to automatically minimize the amount of code that has to be executed symbolically given a target analysis, and relaxed execution consistency models, a way to make principled performance/accuracy trade-offs in complex analyses. These techniques give S2E three key abilities: to simultaneously analyze entire families of execution paths, instead of just one execution at a time; to perform the analyses in-vivo within a real software stack–user programs, libraries, kernel, drivers, etc.–instead of using abstract models of these layers; and to operate directly on binaries, thus being able to analyze even proprietary software.Conceptually, S2E is an automated path explorer with modular path analyzers: the explorer drives the target system down all execution paths of interest, while analyzers check properties of each such path (e.g., to look for bugs) or simply collect information (e.g., count page faults). Desired paths can be specified in multiple ways, and S2E users can either combine existing analyzers to build a custom analysis tool, or write new analyzers using the S2E {API}.},
	pages = {265--278},
	number = {1},
	journaltitle = {{SIGARCH} Comput. Archit. News},
	author = {Chipounov, Vitaly and Kuznetsov, Volodymyr and Candea, George},
	date = {2011-03},
	note = {Place: New York, {NY}, {USA}
Publisher: Association for Computing Machinery},
	keywords = {analysis, binary, consistency models, dbt, framework, in-vivo, performance, symbolic execution, testing, virtualization},
}

@inproceedings{bucur_prototyping_2014,
	location = {New York, {NY}, {USA}},
	title = {Prototyping symbolic execution engines for interpreted languages},
	isbn = {978-1-4503-2305-5},
	url = {https://doi.org/10.1145/2541940.2541977},
	doi = {10.1145/2541940.2541977},
	series = {{ASPLOS} '14},
	abstract = {Symbolic execution is being successfully used to automatically test statically compiled code. However, increasingly more systems and applications are written in dynamic interpreted languages like Python. Building a new symbolic execution engine is a monumental effort, and so is keeping it up-to-date as the target language evolves. Furthermore, ambiguous language specifications lead to their implementation in a symbolic execution engine potentially differing from the production interpreter in subtle ways.We address these challenges by flipping the problem and using the interpreter itself as a specification of the language semantics. We present a recipe and tool (called Chef) for turning a vanilla interpreter into a sound and complete symbolic execution engine. Chef symbolically executes the target program by symbolically executing the interpreter's binary while exploiting inferred knowledge about the program's high-level structure.Using Chef, we developed a symbolic execution engine for Python in 5 person-days and one for Lua in 3 person-days. They offer complete and faithful coverage of language features in a way that keeps up with future language versions at near-zero cost. Chef-produced engines are up to 1000 times more performant than if directly executing the interpreter symbolically without Chef.},
	pages = {239--254},
	booktitle = {Proceedings of the 19th International Conference on Architectural Support for Programming Languages and Operating Systems},
	publisher = {Association for Computing Machinery},
	author = {Bucur, Stefan and Kinder, Johannes and Candea, George},
	date = {2014},
	note = {event-place: Salt Lake City, Utah, {USA}},
	keywords = {interpreter instrumentation, software analysis optimizations, state selection strategies},
}

@article{bucur_prototyping_2014-1,
	title = {Prototyping symbolic execution engines for interpreted languages},
	volume = {49},
	issn = {0362-1340},
	url = {https://doi.org/10.1145/2644865.2541977},
	doi = {10.1145/2644865.2541977},
	abstract = {Symbolic execution is being successfully used to automatically test statically compiled code. However, increasingly more systems and applications are written in dynamic interpreted languages like Python. Building a new symbolic execution engine is a monumental effort, and so is keeping it up-to-date as the target language evolves. Furthermore, ambiguous language specifications lead to their implementation in a symbolic execution engine potentially differing from the production interpreter in subtle ways.We address these challenges by flipping the problem and using the interpreter itself as a specification of the language semantics. We present a recipe and tool (called Chef) for turning a vanilla interpreter into a sound and complete symbolic execution engine. Chef symbolically executes the target program by symbolically executing the interpreter's binary while exploiting inferred knowledge about the program's high-level structure.Using Chef, we developed a symbolic execution engine for Python in 5 person-days and one for Lua in 3 person-days. They offer complete and faithful coverage of language features in a way that keeps up with future language versions at near-zero cost. Chef-produced engines are up to 1000 times more performant than if directly executing the interpreter symbolically without Chef.},
	pages = {239--254},
	number = {4},
	journaltitle = {{SIGPLAN} Not.},
	author = {Bucur, Stefan and Kinder, Johannes and Candea, George},
	date = {2014-02},
	note = {Place: New York, {NY}, {USA}
Publisher: Association for Computing Machinery},
	keywords = {interpreter instrumentation, software analysis optimizations, state selection strategies},
}

@article{godefroid_fuzzing_2020,
	title = {Fuzzing: hack, art, and science},
	volume = {63},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/3363824},
	doi = {10.1145/3363824},
	shorttitle = {Fuzzing},
	abstract = {Reviewing software testing techniques for finding security vulnerabilities.},
	pages = {70--76},
	number = {2},
	journaltitle = {Communications of the {ACM}},
	shortjournal = {Commun. {ACM}},
	author = {Godefroid, Patrice},
	urldate = {2025-03-20},
	date = {2020-01-22},
	langid = {english},
}

@online{oxford_dictionary_definition_nodate,
	title = {Definition Reliable},
	url = {https://www.oxfordlearnersdictionaries.com/definition/american_english/reliable},
	titleaddon = {https://www.oxfordlearnersdictionaries.com/definition},
	author = {{Oxford Dictionary}},
	urldate = {2025-03-19},
}

@article{davis_computing_1960,
	title = {A Computing Procedure for Quantification Theory},
	volume = {7},
	url = {https://doi.org/10.1145/321033.321034},
	doi = {10.1145/321033.321034},
	pages = {201--215},
	number = {3},
	journaltitle = {J. {ACM}},
	author = {Davis, Martin and Putnam, Hilary},
	date = {1960},
}

@article{davis_machine_1962,
	title = {A machine program for theorem-proving},
	volume = {5},
	issn = {0001-0782},
	url = {https://doi.org/10.1145/368273.368557},
	doi = {10.1145/368273.368557},
	abstract = {The programming of a proof procedure is discussed in connection with trial runs and possible improvements.},
	pages = {394--397},
	number = {7},
	journaltitle = {Commun. {ACM}},
	author = {Davis, Martin and Logemann, George and Loveland, Donald},
	date = {1962-07},
	note = {Place: New York, {NY}, {USA}
Publisher: Association for Computing Machinery},
}

@article{davis_machine_nodate,
	title = {A machine program for theorem-proving},
	author = {Davis, Martin and Logemann, George and Loveland, Donald},
	langid = {english},
}

@incollection{hutchison_computing_2014,
	location = {Cham},
	title = {Computing with an {SMT} Solver},
	volume = {8570},
	isbn = {978-3-319-09098-6 978-3-319-09099-3},
	url = {http://link.springer.com/10.1007/978-3-319-09099-3_2},
	abstract = {Satisﬁability modulo theories ({SMT}) solvers that support quantiﬁer instantiations via matching triggers can be programmed to give practical support for user-deﬁned theories. Care must be taken to avoid so-called matching loops, which may prevent termination of the solver. By design, such avoidance limits the extent to which the {SMT} solver is able to apply the deﬁnitions of user-deﬁned functions. For some inputs to these functions, however, it is instead desireable to allow unadulterated use of the functions; in particular, if it is known that evaluation will terminate.},
	pages = {20--35},
	booktitle = {Tests and Proofs},
	publisher = {Springer International Publishing},
	author = {Amin, Nada and Leino, K. Rustan M. and Rompf, Tiark},
	editor = {Seidl, Martina and Tillmann, Nikolai},
	editorb = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Kobsa, Alfred and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Terzopoulos, Demetri and Tygar, Doug and Weikum, Gerhard},
	editorbtype = {redactor},
	urldate = {2025-03-19},
	date = {2014},
	langid = {english},
	doi = {10.1007/978-3-319-09099-3_2},
	note = {Series Title: Lecture Notes in Computer Science},
}

@inproceedings{abraham_smt_2017,
	location = {Timisoara},
	title = {{SMT} Solving for Arithmetic Theories: Theory and Tool Support},
	isbn = {978-1-5386-2626-9},
	url = {https://ieeexplore.ieee.org/document/8531255/},
	doi = {10.1109/SYNASC.2017.00009},
	shorttitle = {{SMT} Solving for Arithmetic Theories},
	abstract = {Satisﬁability checking aims to develop algorithms and tools for checking the satisﬁability of logical formulas. Driven by the impressive success of {SAT} solvers for propositional logic, Satisﬁability-Modulo-Theories ({SMT}) solvers were developed to extend the scope also to different theories. Today, {SMT} solving is widely used in many applications, for example veriﬁcation, synthesis, planning or product design automation. In this tutorial paper we give a short introduction to the foundations of {SMT} solving, describe some popular {SMT} solvers and illustrate their usage. We also present our own solver {SMT}-{RAT}, which was developed to support the strategic combination of different decision procedures, putting a focus on arithmetic theories.},
	eventtitle = {2017 19th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing ({SYNASC})},
	pages = {1--8},
	booktitle = {2017 19th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing ({SYNASC})},
	publisher = {{IEEE}},
	author = {Abraham, Erika and Kremer, Gereon},
	urldate = {2025-03-19},
	date = {2017-09},
	langid = {english},
}

@misc{loring_systematic_2021-1,
	title = {Systematic Generation of Conformance Tests for {JavaScript}},
	url = {http://arxiv.org/abs/2108.07075},
	doi = {10.48550/arXiv.2108.07075},
	abstract = {{JavaScript} implementations are tested for conformance to the {ECMAScript} standard using a large hand-written test suite. Not only in this a tedious approach, it also relies solely on the natural language specification for differentiating behaviors, while hidden implementation details can also affect behavior and introduce divergences. We propose to generate conformance tests through dynamic symbolic execution of polyfills, drop-in replacements for newer {JavaScript} language features that are not yet widely supported. We then run these generated tests against multiple implementations of {JavaScript}, using a majority vote to identify the correct behavior. To facilitate test generation for polyfill code, we introduce a model for structured symbolic inputs that is suited to the dynamic nature of {JavaScript}. In our evaluation, we found 17 divergences in the widely used core-js polyfill and were able to increase branch coverage in interpreter code by up to 15\%. Because polyfills are typically written even before standardization, our approach will allow to maintain and extend standardization test suites with reduced effort.},
	number = {{arXiv}:2108.07075},
	publisher = {{arXiv}},
	author = {Loring, Blake and Kinder, Johannes},
	urldate = {2025-03-18},
	date = {2021-08-16},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2108.07075 [cs]},
	keywords = {Computer Science - Programming Languages, Computer Science - Software Engineering},
}

@inproceedings{he_learning_2021,
	location = {New York, {NY}, {USA}},
	title = {Learning to Explore Paths for Symbolic Execution},
	isbn = {978-1-4503-8454-4},
	url = {https://doi.org/10.1145/3460120.3484813},
	doi = {10.1145/3460120.3484813},
	series = {{CCS} '21},
	abstract = {Symbolic execution is a powerful technique that can generate tests steering program execution into desired paths. However, the scalability of symbolic execution is often limited by path explosion, i.e., the number of symbolic states representing the paths under exploration quickly explodes as execution goes on. Therefore, the effectiveness of symbolic execution engines hinges on the ability to select and explore the right symbolic states.In this work, we propose a novel learning-based strategy, called Learch, able to effectively select promising states for symbolic execution to tackle the path explosion problem. Learch directly estimates the contribution of each state towards the goal of maximizing coverage within a time budget, as opposed to relying on manually crafted heuristics based on simple statistics as a crude proxy for the objective. Moreover, Learch leverages existing heuristics in training data generation and feature extraction, and can thus benefit from any new expert-designed heuristics. We instantiated Learch in {KLEE}, a widely adopted symbolic execution engine. We evaluated Learch on a diverse set of programs, showing that Learch is practically effective: it covers more code and detects more security violations than existing manual heuristics, as well as combinations of those heuristics. We also show that using tests generated by Learch as initial fuzzing seeds enables the popular fuzzer {AFL} to find more paths and security violations.},
	pages = {2526--2540},
	booktitle = {Proceedings of the 2021 {ACM} {SIGSAC} Conference on Computer and Communications Security},
	publisher = {Association for Computing Machinery},
	author = {He, Jingxuan and Sivanrupan, Gishor and Tsankov, Petar and Vechev, Martin},
	urldate = {2025-03-18},
	date = {2021-11-13},
}

@article{miao_extensive_nodate,
	title = {An Extensive Empirical Study of Nondeterministic Behavior in Static Analysis Tools},
	abstract = {Recent research has studied the importance and identified causes of nondeterminism in software. Static analysis tools exhibit many risk factors for nondeterministic behavior, but no work has analyzed the occurrence of such behavior in these tools. To bridge this gap, we perform an extensive empirical study aiming to understand past and ongoing nondeterminism in 12 popular, open-source static analysis tools that target 5 types of projects. We first conduct a qualitative study to understand the extent to which nondeterministic behavior has been found and addressed within the tools under study, and find results in 7 tool repositories. After classifying the issues and commits by root cause, we find that the majority of nondeterminisms are caused by concurrency issues, incorrect analysis logic, or assumed orderings of unordered data structures, which have shared patterns. We also perform a quantitative analysis, where we use two strategies and diverse input programs and configurations to detect yet-unknown nondeterministic behaviors. We discover such behavior in 8 out of the 12 tools, including 3 which had no results from the qualitative analysis. We find that nondeterminism often appears in multiple configurations on a variety of input programs. We communicated all identified nondeterminism to the developers, and received confirmation of five tools. Finally, we detail a case study of fixing {FlowDroid}’s nondeterministic behavior.},
	author = {Miao, Miao and Mordahl, Austin},
	langid = {english},
}

@article{ryan_countering_nodate,
	title = {Countering the Path Explosion Problem in the Symbolic Execution of Hardware Designs},
	abstract = {Symbolic execution is a powerful veriﬁcation tool for hardware designs, but suffers from the path explosion problem. We introduce a new approach, piecewise composition, which leverages the modular structure of hardware to transfer the work of path exploration to {SMT} solvers. We present a symbolic execution engine implementing the technique. The engine operates directly over register transfer level ({RTL}) Verilog designs without requiring translation to a netlist or software simulation. In our evaluation, piecewise composition reduces the number of paths explored by an order of magnitude and reduces the runtime by 97\%. Using 84 properties from the literature we ﬁnd assertion violations in 5 open-source designs including an {SoC} and {CPU}.},
	author = {Ryan, Kaki and Sturton, Cynthia},
	langid = {english},
}

@article{kuznetsov_efficient_nodate,
	title = {Efficient State Merging in Symbolic Execution},
	abstract = {Symbolic execution has proven to be a practical technique for building automated test case generation and bug ﬁnding tools. Nevertheless, due to state explosion, these tools still struggle to achieve scalability. Given a program, one way to reduce the number of states that the tools need to explore is to merge states obtained on different paths. Alas, doing so increases the size of symbolic path conditions (thereby stressing the underlying constraint solver) and interferes with optimizations of the exploration process (also referred to as search strategies). The net effect is that state merging may actually lower performance rather than increase it.},
	author = {Kuznetsov, Volodymyr and Kinder, Johannes and Bucur, Stefan and Candea, George},
	langid = {english},
}

@book{martin_managing_1983,
	location = {{USA}},
	edition = {1st},
	title = {Managing the Data Base Environment},
	isbn = {0-13-550582-8},
	publisher = {Prentice Hall {PTR}},
	author = {Martin, James},
	date = {1983},
}

@inproceedings{bisht_xss-guard_2008,
	location = {Berlin, Heidelberg},
	title = {{XSS}-{GUARD}: Precise Dynamic Prevention of Cross-Site Scripting Attacks},
	isbn = {978-3-540-70541-3},
	url = {https://doi.org/10.1007/978-3-540-70542-0_2},
	doi = {10.1007/978-3-540-70542-0_2},
	series = {{DIMVA} '08},
	abstract = {This paper focuses on defense mechanisms for cross-site scripting attacks, the top threat on web applications today. It is believed that input validation (or filtering) can effectively prevent {XSS} attacks on the server side. In this paper, we discuss several recent real-world {XSS} attacks and analyze the reasons for the failure of filtering mechanisms in defending these attacks. We conclude that while filtering is useful as a first level of defense against {XSS} attacks, it is ineffective in preventing several instances of attack, especially when user input includes content-rich {HTML}. We then propose {XSS}-Guard , a new framework that is designed to be a prevention mechanism against {XSS} attacks on the server side. {XSS}-Guard works by dynamically learning the set of scripts that a web application intends to create for any {HTML} request. Our approach also includes a robust mechanism for identifying scripts at the server side and removes any script in the output that is not intended by the web application. We discuss extensive experimental results that demonstrate the resilience of {XSS}-Guard in preventing a number of real-world {XSS} exploits.},
	pages = {23--43},
	booktitle = {Proceedings of the 5th International Conference on Detection of Intrusions and Malware, and Vulnerability Assessment},
	publisher = {Springer-Verlag},
	author = {Bisht, Prithvi and Venkatakrishnan, V. N.},
	date = {2008},
	note = {event-place: Paris, France},
	keywords = {Attack Prevention, Cross-site scripting ({XSS}), Filtering, Security},
}

@collection{hutchison_detection_2008,
	location = {Berlin, Heidelberg},
	title = {Detection of Intrusions and Malware, and Vulnerability Assessment: 5th International Conference, {DIMVA} 2008, Paris, France, July 10-11, 2008. Proceedings},
	isbn = {978-3-540-70541-3 978-3-540-70542-0},
	series = {Lecture Notes in Computer Science},
	shorttitle = {Detection of Intrusions and Malware, and Vulnerability Assessment},
	number = {5137},
	publisher = {Springer Berlin Heidelberg},
	editor = {Hutchison, David and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Zamboni, Diego and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C.},
	date = {2008},
	langid = {english},
	doi = {10.1007/978-3-540-70542-0},
}

@article{miller_empirical_1990,
	title = {An empirical study of the reliability of {UNIX} utilities},
	volume = {33},
	issn = {0001-0782},
	url = {https://doi.org/10.1145/96267.96279},
	doi = {10.1145/96267.96279},
	abstract = {The following section describes the tools we built to test the utilities. These tools include the fuzz (random character) generator, ptyjig (to test interactive utilities), and scripts to automate the testing process. Next, we will describe the tests we performed, giving the types of input we presented to the utilities. Results from the tests will follow along with an analysis of the results, including identification and classification of the program bugs that caused the crashes. The final section presents concluding remarks, including suggestions for avoiding the types of problems detected by our study and some commentary on the bugs we found. We include an Appendix with the user manual pages for fuzz and ptyjig.},
	pages = {32--44},
	number = {12},
	journaltitle = {Commun. {ACM}},
	author = {Miller, Barton P. and Fredriksen, Lars and So, Bryan},
	date = {1990-12},
	note = {Place: New York, {NY}, {USA}
Publisher: Association for Computing Machinery},
}

@article{miller_empirical_1990-1,
	title = {An empirical study of the reliability of {UNIX} utilities},
	volume = {33},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/96267.96279},
	doi = {10.1145/96267.96279},
	abstract = {Operating system facilities, such as the kernel and utility programs, are typically assumed to be reliable. In our recent experiments, we have been able to crash 25-33\% of the utility programs on any version of {UNIX} that was tested. This report describes these tests and an analysis of the program bugs that caused the crashes.},
	pages = {32--44},
	number = {12},
	journaltitle = {Communications of the {ACM}},
	shortjournal = {Commun. {ACM}},
	author = {Miller, Barton P. and Fredriksen, Lars and So, Bryan},
	urldate = {2025-03-12},
	date = {1990-12},
	langid = {english},
}

@inproceedings{boyer_selectformal_1975,
	location = {New York, {NY}, {USA}},
	title = {{SELECT}—a formal system for testing and debugging programs by symbolic execution},
	isbn = {978-1-4503-7385-2},
	url = {https://doi.org/10.1145/800027.808445},
	doi = {10.1145/800027.808445},
	abstract = {{SELECT} is an experimental system for assisting in the formal systematic debugging of programs. It is intended to be a compromise between an automated program proving system and the current ad hoc debugging practice, and is similar to a system being developed by King et al. of {IBM}. {SELECT} systematically handles the paths of programs written in a {LISP} subset that includes arrays. For each execution path {SELECT} returns simplified conditions on input variables that cause the path to be executed, and simplified symbolic values for program variables at the path output. For conditions which form a system of linear equalities and inequalities {SELECT} will return input variable values that can serve as sample test data. The user can insert constraint conditions, at any point in the program including the output, in the form of symbolically executable assertions. These conditions can induce the system to select test data in user-specified regions. {SELECT} can also determine if the path is correct with respect to an output assertion. We present four examples demonstrating the various modes of system operation and their effectiveness in finding bugs. In some examples, {SELECT} was successful in automatically finding useful test data. In others, user interaction was required in the form of output assertions. {SELECT} appears to be a useful tool for rapidly revealing program errors, but for the future there is a need to expand its expressive and deductive power.},
	pages = {234--245},
	booktitle = {Proceedings of the International Conference on Reliable Software},
	publisher = {Association for Computing Machinery},
	author = {Boyer, Robert S. and Elspas, Bernard and Levitt, Karl N.},
	date = {1975},
	note = {event-place: Los Angeles, California},
	keywords = {Program debugging, Program testing, Program verification, Solution of systems of inequalities, Symbolic execution, Test data generation},
}

@inproceedings{king_new_1975,
	location = {New York, {NY}, {USA}},
	title = {A new approach to program testing},
	isbn = {978-1-4503-7385-2},
	url = {https://doi.org/10.1145/800027.808444},
	doi = {10.1145/800027.808444},
	abstract = {The current approach for testing a program is, in principle, quite primitive. Some small sample of the data that a program is expected to handle is presented to the program. If the program produces correct results for the sample, it is assumed to be correct. Much current work focuses on the question of how to choose this sample. We propose that a program can be more effectively tested by executing it "symbolically." Instead of supplying specific constants as input values to a program being tested, one supplies symbols. The normal computational definitions for the basic operations performed by a program can be expanded to accept symbolic inputs and produce symbolic formulae as output. If the flow of control in the program is completely independent of its input parameters, then all output values can be symbolically computed as formulae over the symbolic inputs and examined for correctness. When the control flow of the program is input dependent, a case analysis can be performed producing output formulae for each class of inputs determined by the control flow dependencies. Using these ideas, we have designed and implemented an interactive debugging/testing system called {EFFIGY}.},
	pages = {228--233},
	booktitle = {Proceedings of the International Conference on Reliable Software},
	publisher = {Association for Computing Machinery},
	author = {King, James C.},
	date = {1975},
	note = {event-place: Los Angeles, California},
	keywords = {Program correctness, Program testing, Program verification, Symbolic execution, Symbolic interpretation},
}

@article{king_symbolic_1976,
	title = {Symbolic execution and program testing},
	volume = {19},
	issn = {0001-0782},
	url = {https://doi.org/10.1145/360248.360252},
	doi = {10.1145/360248.360252},
	abstract = {This paper describes the symbolic execution of programs. Instead of supplying the normal inputs to a program (e.g. numbers) one supplies symbols representing arbitrary values. The execution proceeds as in a normal execution except that values may be symbolic formulas over the input symbols. The difficult, yet interesting issues arise during the symbolic execution of conditional branch type statements. A particular system called {EFFIGY} which provides symbolic execution for program testing and debugging is also described. It interpretively executes programs written in a simple {PL}/I style programming language. It includes many standard debugging features, the ability to manage and to prove things about symbolic expressions, a simple program testing manager, and a program verifier. A brief discussion of the relationship between symbolic execution and program proving is also included.},
	pages = {385--394},
	number = {7},
	journaltitle = {Commun. {ACM}},
	author = {King, James C.},
	date = {1976-07},
	note = {Place: New York, {NY}, {USA}
Publisher: Association for Computing Machinery},
	keywords = {program debugging, program proving, program testing, program verification, symbolic execution, symbolic interpretation},
}

@article{king_new_nodate,
	title = {A {NEW} {APPROACH} {TO} {PROGRAM} {TESTING}},
	abstract = {The current approach for testing a program is, in principle, quite primitive. Some small sample of the data that a program is expected to handle is presented to the program. If the program produces correct results for the sample, it is assumed to be correct. Much current work focuses on the question of how to choose this sample. We propose that a program can be more effectively tested by executing it "symbolically." Instead of supplying specific constants as input values to a program being tested, one supplies symbols. The normal computational definitions for the basic operations performed by a program can be expanded to accept symbolic inputs and produce symbolic formulae as output.},
	author = {King, James C},
	langid = {english},
}

@misc{baldoni_survey_2018,
	title = {A Survey of Symbolic Execution Techniques},
	url = {http://arxiv.org/abs/1610.00502},
	doi = {10.48550/arXiv.1610.00502},
	abstract = {Many security and software testing applications require checking whether certain properties of a program hold for any possible usage scenario. For instance, a tool for identifying software vulnerabilities may need to rule out the existence of any backdoor to bypass a program's authentication. One approach would be to test the program using different, possibly random inputs. As the backdoor may only be hit for very specific program workloads, automated exploration of the space of possible inputs is of the essence. Symbolic execution provides an elegant solution to the problem, by systematically exploring many possible execution paths at the same time without necessarily requiring concrete inputs. Rather than taking on fully specified input values, the technique abstractly represents them as symbols, resorting to constraint solvers to construct actual instances that would cause property violations. Symbolic execution has been incubated in dozens of tools developed over the last four decades, leading to major practical breakthroughs in a number of prominent software reliability applications. The goal of this survey is to provide an overview of the main ideas, challenges, and solutions developed in the area, distilling them for a broad audience. The present survey has been accepted for publication at {ACM} Computing Surveys. If you are considering citing this survey, we would appreciate if you could use the following {BibTeX} entry: http://goo.gl/Hf5Fvc},
	number = {{arXiv}:1610.00502},
	publisher = {{arXiv}},
	author = {Baldoni, Roberto and Coppa, Emilio and D'Elia, Daniele Cono and Demetrescu, Camil and Finocchi, Irene},
	urldate = {2025-03-12},
	date = {2018-05-02},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1610.00502 [cs]},
	keywords = {Computer Science - Programming Languages, Computer Science - Software Engineering},
}

@article{king_symbolic_1976-1,
	title = {Symbolic execution and program testing},
	volume = {19},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/360248.360252},
	doi = {10.1145/360248.360252},
	abstract = {This paper describes the symbolic execution of programs. Instead of supplying the normal inputs to a program (e.g. numbers) one supplies symbols representing arbitrary values. The execution proceeds as in a normal execution except that values may he symbolic formulas over the input symbols. The difficult, yet interesting issues arise during the symbolic execution of conditional branch type statements. A particular system called {EFFIGY} which provides symbolic execution for program testing and debugging is also described, it interpretively executes programs written in a simple {PL}/I style programming language. It includes many standard debugging features, the ability to manage and to prove things about symbolic expressions, a simple program testing manager, and a program verifier. A brief discussion of the relationship between symbolic execution and program proving is also included.},
	pages = {385--394},
	number = {7},
	journaltitle = {Communications of the {ACM}},
	shortjournal = {Commun. {ACM}},
	author = {King, James C.},
	urldate = {2025-03-12},
	date = {1976-07},
	langid = {english},
}

@article{cadar_klee_nodate,
	title = {{KLEE}: Unassisted and Automatic Generation of High-Coverage Tests for Complex Systems Programs},
	abstract = {We present a new symbolic execution tool, {KLEE}, capable of automatically generating tests that achieve high coverage on a diverse set of complex and environmentally-intensive programs. We applied {KLEE} to all 90 programs in the {GNU} {COREUTILS} utility suite, which form the core user-level environment installed on almost all Unix systems and, as such, represent some of the most heavily used and tested open-source programs in existence. For 84\% of these utilities, {KLEE}’s automatically generated tests covered 80–100\% of executable statements and, in aggregate, signiﬁcantly beat the coverage of the developers’ own hand-written test suites. {KLEE} also found nine serious bugs (including three that had been missed for over 15 years!) and produced concrete inputs that triggered the errors when run on the uninstrumented code. When applied to {MINIX}’s versions of a small selection of the same applications, {KLEE} achieved similar coverage (along with two bugs). In addition, we also used {KLEE} to automatically ﬁnd numerous incorrect differences between several {MINIX} and {COREUTILS} tools. Finally, we checked the kernel of the {HISTAR} operating system, generating tests that achieved 76.4\% (without paging enabled) and 67.1\% coverage (with paging) and found one important security bug.},
	author = {Cadar, Cristian and Dunbar, Daniel and Engler, Dawson},
	langid = {english},
}

@inproceedings{burnim_heuristics_2008,
	location = {L'Aquila, Italy},
	title = {Heuristics for Scalable Dynamic Test Generation},
	isbn = {978-1-4244-2187-9},
	url = {http://ieeexplore.ieee.org/document/4639362/},
	doi = {10.1109/ASE.2008.69},
	abstract = {Recently there has been great success in using symbolic execution to automatically generate test inputs for small software systems. A primary challenge in scaling such approaches to larger programs is the combinatorial explosion of the path space. It is likely that sophisticated strategies for searching this path space are needed to generate inputs that effectively test large programs (by, e.g., achieving signiﬁcant branch coverage). We present several such heuristic search strategies, including a novel strategy guided by the control ﬂow graph of the program under test. We have implemented these strategies in {CREST}, our open source concolic testing tool for C, and evaluated them on two widely-used software tools, grep 2.2 (15K lines of code) and Vim 5.7 (150K lines). On these benchmarks, the presented heuristics achieve signiﬁcantly greater branch coverage on the same testing budget than concolic testing with a traditional depth-ﬁrst search strategy.},
	eventtitle = {2008 23rd {IEEE}/{ACM} International Conference on Automated Software Engineering},
	pages = {443--446},
	booktitle = {2008 23rd {IEEE}/{ACM} International Conference on Automated Software Engineering},
	publisher = {{IEEE}},
	author = {Burnim, Jacob and Sen, Koushik},
	urldate = {2025-03-12},
	date = {2008-09},
	langid = {english},
}

@incollection{hutchison_generating_2013,
	location = {Berlin, Heidelberg},
	title = {Generating Test Suites with Augmented Dynamic Symbolic Execution},
	volume = {7942},
	isbn = {978-3-642-38915-3 978-3-642-38916-0},
	url = {http://link.springer.com/10.1007/978-3-642-38916-0_9},
	abstract = {Unit test generation tools typically aim at one of two objectives: to explore the program behavior in order to exercise automated oracles, or to produce a representative test set that can be used to manually add oracles or to use as a regression test set. Dynamic symbolic execution ({DSE}) can eﬃciently explore all simple paths through a program, exercising automated oracles such as assertions or code contracts. However, its original intention was not to produce representative test sets. Although {DSE} tools like Pex can retain subsets of the tests seen during the exploration, customer feedback revealed that users expect different values than those produced by Pex, and sometimes also more than one value for a given condition or program path. This threatens the applicability of {DSE} in a scenario without automated oracles. Indeed, even though all paths might be covered by {DSE}, the resulting tests are usually not sensitive enough to make a good regression test suite. In this paper, we present augmented dynamic symbolic execution, which aims to produce representative test sets with {DSE} by augmenting path conditions with additional conditions that enforce target criteria such as boundary or mutation adequacy, or logical coverage criteria. Experiments with our Apex prototype demonstrate that the resulting test cases can detect up to 30\% more seeded defects than those produced with Pex.},
	pages = {152--167},
	booktitle = {Tests and Proofs},
	publisher = {Springer Berlin Heidelberg},
	author = {Jamrozik, Konrad and Fraser, Gordon and Tillman, Nikolai and De Halleux, Jonathan},
	editor = {Veanes, Margus and Viganò, Luca},
	editorb = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard},
	editorbtype = {redactor},
	urldate = {2025-03-12},
	date = {2013},
	langid = {english},
	doi = {10.1007/978-3-642-38916-0_9},
	note = {Series Title: Lecture Notes in Computer Science},
}

@inproceedings{berthier_efficient_2023,
	title = {An Efficient Black-Box Support of Advanced Coverage Criteria for Klee},
	url = {http://arxiv.org/abs/2211.14592},
	doi = {10.1145/3555776.3577713},
	abstract = {Dynamic symbolic execution ({DSE}) is a powerful test generation approach based on an exploration of the path space of the program under test. Well-adapted for path coverage, this approach is however less efficient for conditions, decisions, advanced coverage criteria (such as multiple conditions, weak mutations, boundary testing) or user-provided test objectives. While theoretical solutions to adapt {DSE} to a large set of criteria have been proposed, they have never been integrated into publicly available testing tools. This paper presents a first integration of an optimized test generation strategy for advanced coverage criteria into a popular open-source testing tool based on {DSE}, namely, Klee. The integration is performed in a fully black-box manner, and can therefore inspire an easy integration into other similar tools. The resulting version of the tool, named Klee4labels, is publicly available. We present the design of the proposed technique and evaluate it on several benchmarks. Our results confirm the benefits of the proposed tool for advanced coverage criteria.},
	pages = {1706--1715},
	booktitle = {Proceedings of the 38th {ACM}/{SIGAPP} Symposium on Applied Computing},
	author = {Berthier, Nicolas and Oliveira, Steven De and Kosmatov, Nikolai and Longuet, Delphine and Soulat, Romain},
	urldate = {2025-03-12},
	date = {2023-03-27},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2211.14592 [cs]},
	keywords = {Computer Science - Software Engineering},
}

@inproceedings{eriksson_black_2021,
	title = {Black Widow: Blackbox Data-driven Web Scanning},
	url = {https://doi.org/10.1109/SP40001.2021.00022},
	doi = {10.1109/SP40001.2021.00022},
	pages = {1125--1142},
	booktitle = {42nd {IEEE} Symposium on Security and Privacy, {SP} 2021, San Francisco, {CA}, {USA}, 24-27 May 2021},
	publisher = {{IEEE}},
	author = {Eriksson, Benjamin and Pellegrino, Giancarlo and Sabelfeld, Andrei},
	date = {2021},
}

@inproceedings{eriksson_black_2023,
	title = {Black Ostrich: Web Application Scanning with String Solvers},
	url = {https://doi.org/10.1145/3576915.3616582},
	doi = {10.1145/3576915.3616582},
	pages = {549--563},
	booktitle = {Proceedings of the 2023 {ACM} {SIGSAC} Conference on Computer and Communications Security, {CCS} 2023, Copenhagen, Denmark, November 26-30, 2023},
	publisher = {{ACM}},
	author = {Eriksson, Benjamin and Stjerna, Amanda and Masellis, Riccardo De and Rümmer, Philipp and Sabelfeld, Andrei},
	editor = {Meng, Weizhi and Jensen, Christian Damsgaard and Cremers, Cas and Kirda, Engin},
	date = {2023},
}

@inproceedings{cha_unleashing_2012,
	title = {Unleashing Mayhem on Binary Code},
	url = {https://doi.org/10.1109/SP.2012.31},
	doi = {10.1109/SP.2012.31},
	pages = {380--394},
	booktitle = {{IEEE} Symposium on Security and Privacy, {SP} 2012, 21-23 May 2012, San Francisco, California, {USA}},
	publisher = {{IEEE} Computer Society},
	author = {Cha, Sang Kil and Avgerinos, Thanassis and Rebert, Alexandre and Brumley, David},
	date = {2012},
}

@incollection{barrett_satisfiability_2009,
	title = {Satisfiability Modulo Theories},
	volume = {185},
	url = {https://doi.org/10.3233/978-1-58603-929-5-825},
	series = {Frontiers in Artificial Intelligence and Applications},
	pages = {825--885},
	booktitle = {Handbook of Satisfiability},
	publisher = {{IOS} Press},
	author = {Barrett, Clark W. and Sebastiani, Roberto and Seshia, Sanjit A. and Tinelli, Cesare},
	editor = {Biere, Armin and Heule, Marijn and Maaren, Hans van and Walsh, Toby},
	date = {2009},
	doi = {10.3233/978-1-58603-929-5-825},
}

@article{yurichev_satsmt_nodate,
	title = {{SAT}/{SMT} by Example},
	author = {Yurichev, Dennis},
	langid = {english},
}

@thesis{loring_practical_2021,
	title = {Practical Dynamic Symbolic Execution for {JavaScript}},
	institution = {Royal Holloway, University of London},
	type = {phdthesis},
	author = {Loring, Blake William},
	date = {2021},
	langid = {english},
}

@online{bill_q_2002,
	title = {Q\&A: Bill Gates On Trustworthy Computing {\textbar} {InformationWeek}},
	url = {https://www.informationweek.com/it-leadership/q-a-bill-gates-on-trustworthy-computing},
	shorttitle = {Q\&A},
	abstract = {Bill Gates, Microsoft's chairman and chief software architect, spoke with \&lt;I\&gt;{InformationWeek}\&lt;/I\&gt; about the companywide effort to improve the quality and security of Microsoft's products.},
	titleaddon = {Information Week},
	author = {Bill, Gates},
	urldate = {2025-03-06},
	date = {2002-05},
	langid = {english},
}

@article{yun_qsym_nodate,
	title = {{QSYM}: A Practical Concolic Execution Engine Tailored for Hybrid Fuzzing},
	abstract = {Recently, hybrid fuzzing has been proposed to address the limitations of fuzzing and concolic execution by combining both approaches. The hybrid approach has shown its effectiveness in various synthetic benchmarks such as {DARPA} Cyber Grand Challenge ({CGC}) binaries, but it still suffers from scaling to find bugs in complex, realworld software. We observed that the performance bottleneck of the existing concolic executor is the main limiting factor for its adoption beyond a small-scale study. To overcome this problem, we design a fast concolic execution engine, called {QSYM}, to support hybrid fuzzing. The key idea is to tightly integrate the symbolic emulation with the native execution using dynamic binary translation, making it possible to implement more fine-grained, so faster, instruction-level symbolic emulation. Additionally, {QSYM} loosens the strict soundness requirements of conventional concolic executors for better performance, yet takes advantage of a faster fuzzer for validation, providing unprecedented opportunities for performance optimizations, e.g., optimistically solving constraints and pruning uninteresting basic blocks. Our evaluation shows that {QSYM} does not just outperform state-of-the-art fuzzers (i.e., found 14× more bugs than {VUzzer} in the {LAVA}-M dataset, and outperformed Driller in 104 binaries out of 126), but also found 13 previously unknown security bugs in eight real-world programs like Dropbox Lepton, ffmpeg, and {OpenJPEG}, which have already been intensively tested by the stateof-the-art fuzzers, {AFL} and {OSS}-Fuzz.},
	author = {Yun, Insu and Lee, Sangho and Xu, Meng and Jang, Yeongjin and Kim, Taesoo},
	langid = {english},
}

@inproceedings{farina_relational_2019,
	location = {Porto Portugal},
	title = {Relational Symbolic Execution},
	isbn = {978-1-4503-7249-7},
	url = {https://dl.acm.org/doi/10.1145/3354166.3354175},
	doi = {10.1145/3354166.3354175},
	abstract = {Symbolic execution is a classical program analysis technique used to show that programs satisfy or violate given specifications. In this work we generalize symbolic execution to support program analysis for relational specifications in the form of relational properties - these are properties about two runs of two programs on related inputs, or about two executions of a single program on related inputs. Relational properties are useful to formalize notions in security and privacy, and to reason about program optimizations. We design a relational symbolic execution engine, named {RelSym} which supports interactive refutation, as well as proving of relational properties for programs written in a language with arrays and for-like loops.},
	eventtitle = {{PPDP} '19: Principles and Practice of Programming Languages 2019},
	pages = {1--14},
	booktitle = {Proceedings of the 21st International Symposium on Principles and Practice of Declarative Programming},
	publisher = {{ACM}},
	author = {Farina, Gian Pietro and Chong, Stephen and Gaboardi, Marco},
	urldate = {2025-03-05},
	date = {2019-10-07},
	langid = {english},
}

@inproceedings{de_moura_z3_2008,
	title = {Z3: an efficient {SMT} solver},
	volume = {4963},
	isbn = {978-3-540-78799-0},
	doi = {10.1007/978-3-540-78800-3_24},
	pages = {337--340},
	booktitle = {Tools and Algorithms for the Construction and Analysis of Systems},
	author = {de Moura, Leonardo and Bjørner, Nikolaj},
	date = {2008-04},
}

@inproceedings{sen_jalangi_2013,
	location = {Saint Petersburg Russia},
	title = {Jalangi: a selective record-replay and dynamic analysis framework for {JavaScript}},
	isbn = {978-1-4503-2237-9},
	url = {https://dl.acm.org/doi/10.1145/2491411.2491447},
	doi = {10.1145/2491411.2491447},
	shorttitle = {Jalangi},
	abstract = {{JavaScript} is widely used for writing client-side web applications and is getting increasingly popular for writing mobile applications. However, unlike C, C++, and Java, there are not that many tools available for analysis and testing of {JavaScript} applications. In this paper, we present a simple yet powerful framework, called Jalangi, for writing heavyweight dynamic analyses. Our framework incorporates two key techniques: 1) selective record-replay, a technique which enables to record and to faithfully replay a user-selected part of the program, and 2) shadow values and shadow execution, which enables easy implementation of heavy-weight dynamic analyses. Our implementation makes no special assumption about {JavaScript}, which makes it applicable to realworld {JavaScript} programs running on multiple platforms. We have implemented concolic testing, an analysis to track origins of nulls and undeﬁned, a simple form of taint analysis, an analysis to detect likely type inconsistencies, and an object allocation proﬁler in Jalangi. Our evaluation of Jalangi on the {SunSpider} benchmark suite and on ﬁve web applications shows that Jalangi has an average slowdown of 26X during recording and 30X slowdown during replay and analysis. The slowdowns are comparable with slowdowns reported for similar tools, such as {PIN} and Valgrind for x86 binaries. We believe that the techniques proposed in this paper are applicable to other dynamic languages.},
	eventtitle = {{ESEC}/{FSE}'13: Joint Meeting of the European Software Engineering Conference and the {ACM} {SIGSOFT} Symposium on the Foundations of Software Engineering},
	pages = {488--498},
	booktitle = {Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering},
	publisher = {{ACM}},
	author = {Sen, Koushik and Kalasapur, Swaroop and Brutch, Tasneem and Gibbs, Simon},
	urldate = {2025-02-26},
	date = {2013-08-18},
	langid = {english},
}

@online{max_ogden_callback_2019,
	title = {Callback Hell http://callbackhell.com/},
	url = {http://callbackhell.com/},
	titleaddon = {Callback Hell},
	author = {{Max Ogden}},
	urldate = {2025-02-26},
	date = {2019},
}

@software{noauthor_z3proverz3_2025,
	title = {Z3Prover/z3},
	url = {https://github.com/Z3Prover/z3},
	abstract = {The Z3 Theorem Prover},
	publisher = {Z3 Theorem Prover},
	urldate = {2025-02-26},
	date = {2025-02-26},
	note = {original-date: 2015-03-26T18:16:07Z},
}

@online{noauthor_samsungjalangi2_nodate,
	title = {Samsung/jalangi2: Dynamic analysis framework for {JavaScript}},
	url = {https://github.com/Samsung/jalangi2/tree/master},
	urldate = {2025-02-25},
}

@article{ball_deconstructing_nodate,
	title = {Deconstructing Dynamic Symbolic Execution},
	abstract = {Dynamic symbolic execution ({DSE}) is a well-known technique for automatically generating tests to achieve higher levels of coverage in a program. Two keys ideas of {DSE} are to: (1) seed symbolic execution by executing a program on an initial input; (2) using concrete values from the program execution in place of symbolic expressions whenever symbolic reasoning is hard or not desired. We describe {DSE} for a simple core language and then present a minimalist implementation of {DSE} for Python (in Python) that follows this basic recipe. The code is available at https://www.github.com/thomasjball/{PyExZ}3/ (tagged “v1.0”) and has been designed to make it easy to experiment with and extend.},
	author = {Ball, Thomas and Daniel, Jakub},
	langid = {english},
}

@article{gong_dynamic_nodate,
	title = {Dynamic Analysis for {JavaScript} Code},
	author = {Gong, Liang},
	langid = {english},
}

@inproceedings{santos_symbolic_2018,
	location = {Frankfurt am Main Germany},
	title = {Symbolic Execution for {JavaScript}},
	isbn = {978-1-4503-6441-6},
	url = {https://dl.acm.org/doi/10.1145/3236950.3236956},
	doi = {10.1145/3236950.3236956},
	abstract = {We present a framework for trustworthy symbolic execution of {JavaScripts} programs, whose aim is to assist developers in the testing of their code: the developer writes symbolic tests for which the framework provides concrete counter-models. We create the framework following a new, general methodology for designing compositional program analyses for dynamic languages. We prove that the underlying symbolic execution is sound and does not generate false positives. We establish additional trust by using the theory to precisely guide the implementation and by thorough testing. We apply our framework to whole-program symbolic testing of real-world {JavaScript} libraries and compositional debugging of separation logic specifications of {JavaScript} programs.},
	eventtitle = {{PPDP} '18: The 20th International Symposium on Principles and Practice of Declarative Programming},
	pages = {1--14},
	booktitle = {Proceedings of the 20th International Symposium on Principles and Practice of Declarative Programming},
	publisher = {{ACM}},
	author = {Santos, José Fragoso and Maksimović, Petar and Grohens, Théotime and Dolby, Julian and Gardner, Philippa},
	urldate = {2025-02-24},
	date = {2018-09-03},
	langid = {english},
}

@article{godefroid_automated_nodate,
	title = {Automated Whitebox Fuzz Testing},
	abstract = {Fuzz testing is an effective technique for ﬁnding security vulnerabilities in software. Traditionally, fuzz testing tools apply random mutations to well-formed inputs of a program and test the resulting values. We present an alternative whitebox fuzz testing approach inspired by recent advances in symbolic execution and dynamic test generation. Our approach records an actual run of the program under test on a well-formed input, symbolically evaluates the recorded trace, and gathers constraints on inputs capturing how the program uses these. The collected constraints are then negated one by one and solved with a constraint solver, producing new inputs that exercise different control paths in the program. This process is repeated with the help of a code-coverage maximizing heuristic designed to ﬁnd defects as fast as possible. We have implemented this algorithm in {SAGE} (Scalable, Automated, Guided Execution), a new tool employing x86 instruction-level tracing and emulation for whitebox fuzzing of arbitrary ﬁle-reading Windows applications. We describe key optimizations needed to make dynamic test generation scale to large input ﬁles and long execution traces with hundreds of millions of instructions. We then present detailed experiments with several Windows applications. Notably, without any format-speciﬁc knowledge, {SAGE} detects the {MS}07-017 {ANI} vulnerability, which was missed by extensive blackbox fuzzing and static analysis tools. Furthermore, while still in an early stage of development, {SAGE} has already discovered 30+ new bugs in large shipped Windows applications including image processors, media players, and ﬁle decoders. Several of these bugs are potentially exploitable memory access violations.},
	author = {Godefroid, Patrice and Levin, Michael Y and Molnar, David},
	langid = {english},
}

@article{godefroid_sage_2012,
	title = {{SAGE}: whitebox fuzzing for security testing},
	volume = {55},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/2093548.2093564},
	doi = {10.1145/2093548.2093564},
	shorttitle = {{SAGE}},
	abstract = {{SAGE} has had a remarkable impact at Microsoft.},
	pages = {40--44},
	number = {3},
	journaltitle = {Communications of the {ACM}},
	shortjournal = {Commun. {ACM}},
	author = {Godefroid, Patrice and Levin, Michael Y. and Molnar, David},
	urldate = {2025-02-21},
	date = {2012-03},
	langid = {english},
}

@article{cadar_symbolic_2013,
	title = {Symbolic execution for software testing: three decades later},
	volume = {56},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/2408776.2408795},
	doi = {10.1145/2408776.2408795},
	shorttitle = {Symbolic execution for software testing},
	abstract = {The challenges---and great promise---of modern symbolic execution techniques, and the tools to help implement them.},
	pages = {82--90},
	number = {2},
	journaltitle = {Communications of the {ACM}},
	shortjournal = {Commun. {ACM}},
	author = {Cadar, Cristian and Sen, Koushik},
	urldate = {2025-02-20},
	date = {2013-02},
	langid = {english},
}

@inproceedings{kinder_efficient_2014,
	location = {Lausanne, Switzerland},
	title = {Efficient symbolic execution for software testing},
	rights = {https://doi.org/10.15223/policy-029},
	isbn = {978-0-9835678-4-4},
	url = {https://ieeexplore.ieee.org/document/6987585/},
	doi = {10.1109/FMCAD.2014.6987585},
	eventtitle = {2014 Formal Methods in Computer-Aided Design ({FMCAD})},
	pages = {5--5},
	booktitle = {2014 Formal Methods in Computer-Aided Design ({FMCAD})},
	publisher = {{IEEE}},
	author = {Kinder, Johannes},
	urldate = {2025-02-20},
	date = {2014-10},
	langid = {english},
}

@inproceedings{godefroid_proving_2010,
	location = {Trento Italy},
	title = {Proving memory safety of floating-point computations by combining static and dynamic program analysis},
	isbn = {978-1-60558-823-0},
	url = {https://dl.acm.org/doi/10.1145/1831708.1831710},
	doi = {10.1145/1831708.1831710},
	abstract = {Whitebox fuzzing is a novel form of security testing based on dynamic symbolic execution and constraint solving. Over the last couple of years, whitebox fuzzers have found many new security vulnerabilities (buﬀer overﬂows) in Windows and Linux applications, including codecs, image viewers and media players. Those types of applications tend to use ﬂoating-point instructions available on modern processors, yet existing whitebox fuzzers and {SMT} constraint solvers do not handle ﬂoating-point arithmetic. Are there new security vulnerabilities lurking in ﬂoating-point code? A naive solution would be to extend symbolic execution to ﬂoating-point ({FP}) instructions (months of work), extend {SMT} solvers to reason about {FP} constraints (months of work or more), and then face more complex constraints and an even worse path explosion problem. Instead, we propose an alternative approach, based on the rough intuition that {FP} code should only perform memory safe data-processing of the “payload” of an image or video ﬁle, while the non-{FP} part of the application should deal with buﬀer allocations and memory address computations, with only the latter being prone to buﬀer overﬂows and other security critical bugs. Our approach combines (1) a lightweight local path-insensitive “may” static analysis of {FP} instructions with (2) a high-precision wholeprogram path-sensitive “must” dynamic analysis of non-{FP} instructions. The aim of this combination is to prove memory safety of the {FP} part of each execution and a form of non-interference between the {FP} part and the non-{FP} part with respect to memory address computations.},
	eventtitle = {{ISSTA} '10: International Symposium on Software Testing and Analysis},
	pages = {1--12},
	booktitle = {Proceedings of the 19th international symposium on Software testing and analysis},
	publisher = {{ACM}},
	author = {Godefroid, Patrice and Kinder, Johannes},
	urldate = {2025-02-20},
	date = {2010-07-12},
	langid = {english},
}

@article{andreasen_survey_2018,
	title = {A Survey of Dynamic Analysis and Test Generation for {JavaScript}},
	volume = {50},
	issn = {0360-0300, 1557-7341},
	url = {https://dl.acm.org/doi/10.1145/3106739},
	doi = {10.1145/3106739},
	abstract = {{JavaScript} has become one of the most prevalent programming languages. Unfortunately, some of the unique properties that contribute to this popularity also make {JavaScript} programs prone to errors and difficult for program analyses to reason about. These properties include the highly dynamic nature of the language, a set of unusual language features, a lack of encapsulation mechanisms, and the “no crash” philosophy. This article surveys dynamic program analysis and test generation techniques for {JavaScript} targeted at improving the correctness, reliability, performance, security, and privacy of {JavaScript}-based software.},
	pages = {1--36},
	number = {5},
	journaltitle = {{ACM} Computing Surveys},
	shortjournal = {{ACM} Comput. Surv.},
	author = {Andreasen, Esben and Gong, Liang and Møller, Anders and Pradel, Michael and Selakovic, Marija and Sen, Koushik and Staicu, Cristian-Alexandru},
	urldate = {2025-02-20},
	date = {2018-09-30},
	langid = {english},
}

@incollection{barrett_satisfiability_2009,
	title = {Satisfiability modulo theories},
	isbn = {978-1-58603-929-5},
	url = {http://www.scopus.com/inward/record.url?scp=69949133234&partnerID=8YFLogxK},
	series = {Frontiers in Artificial Intelligence and Applications},
	abstract = {Applications in artificial intelligence, formal verification, and other areas have greatly benefited from the recent advances in {SAT}. It is often the case, however, that applications in these fields require determining the satisfiability of formulas in more expressive logics such as first-order logic. Also, these applications typically require not general first-order satisfiability, but rather satisfiability with respect to some background theory, which fixes the interpretations of certain predicate and function symbols. For many background theories, specialized methods yield decision procedures for the satisfiability of quantifier-free formulas or some subclass thereof. Specialized decision procedures have been discovered for a long and still growing list of theories with practical applications. These include the theory of equality, various theories of arithmetic, and certain theories of arrays, as well as theories of lists, tuples, records, and bit-vectors of a fixed or arbitrary finite size. The research field concerned with determining the satisfiability of formulas with respect to some background theory is called Satisfiability Modulo Theories ({SMT}). This chapter provides a brief overview of {SMT} together with references to the relevant literature for a deeper study. It begins with an overview of techniques for solving {SMT} problems by encodings to {SAT}. The rest of the chapter is mostly concerned with an alternative approach in which a {SAT} solver is integrated with a separate decision procedure (called a theory solver) for conjunctions of literals in the background theory. After presenting this approach as a whole, the chapter provides more details on how to construct and combine theory solvers, and discusses several extensions and enhancements.},
	pages = {825--885},
	booktitle = {Handbook of Satisfiability},
	publisher = {{IOS} Press},
	author = {Barrett, Clark and Sebastiani, Roberto and Seshia, Sanjit A. and Tinelli, Cesare},
	urldate = {2025-02-17},
	date = {2009},
	doi = {10.3233/978-1-58603-929-5-825},
}

@inproceedings{csallner_dysy_2008,
	location = {Leipzig, Germany},
	title = {{DySy}: dynamic symbolic execution for invariant inference},
	isbn = {978-1-60558-079-1},
	url = {http://portal.acm.org/citation.cfm?doid=1368088.1368127},
	doi = {10.1145/1368088.1368127},
	shorttitle = {{DySy}},
	abstract = {Dynamically discovering likely program invariants from concrete test executions has emerged as a highly promising software engineering technique. Dynamic invariant inference has the advantage of succinctly summarizing both “expected” program inputs and the subset of program behaviors that is normal under those inputs. In this paper, we introduce a technique that can drastically increase the relevance of inferred invariants, or reduce the size of the test suite required to obtain good invariants. Instead of falsifying invariants produced by pre-set patterns, we determine likely program invariants by combining the concrete execution of actual test cases with a simultaneous symbolic execution of the same tests. The symbolic execution produces abstract conditions over program variables that the concrete tests satisfy during their execution. In this way, we obtain the beneﬁts of dynamic inference tools like Daikon: the inferred invariants correspond to the observed program behaviors. At the same time, however, our inferred invariants are much more suited to the program at hand than Daikon’s hardcoded invariant patterns. The symbolic invariants are literally derived from the program text itself, with appropriate value substitutions as dictated by symbolic execution.},
	eventtitle = {the 13th international conference},
	pages = {281},
	booktitle = {Proceedings of the 13th international conference on Software engineering  - {ICSE} '08},
	publisher = {{ACM} Press},
	author = {Csallner, Christoph and Tillmann, Nikolai and Smaragdakis, Yannis},
	urldate = {2025-02-17},
	date = {2008},
	langid = {english},
}

@inproceedings{elkarablieh_precise_2009,
	location = {Chicago {IL} {USA}},
	title = {Precise pointer reasoning for dynamic test generation},
	isbn = {978-1-60558-338-9},
	url = {https://dl.acm.org/doi/10.1145/1572272.1572288},
	doi = {10.1145/1572272.1572288},
	abstract = {Dynamic test generation consists of executing a program while gathering symbolic constraints on inputs from predicates encountered in branch statements, and of using a constraint solver to infer new program inputs from previous constraints in order to steer next executions towards new program paths. Variants of this technique have recently been adopted in several bug detection tools, including our whitebox fuzzer {SAGE}, which has found dozens of new expensive security-related bugs in many Windows applications and is now routinely used in various Microsoft groups.},
	eventtitle = {{ISSTA} '09: International Symposium on Software Testing and Analysis},
	pages = {129--140},
	booktitle = {Proceedings of the eighteenth international symposium on Software testing and analysis},
	publisher = {{ACM}},
	author = {Elkarablieh, Bassem and Godefroid, Patrice and Levin, Michael Y.},
	urldate = {2025-02-15},
	date = {2009-07-19},
	langid = {english},
}

@article{jhala_software_2009,
	title = {Software model checking},
	volume = {41},
	issn = {0360-0300, 1557-7341},
	url = {https://dl.acm.org/doi/10.1145/1592434.1592438},
	doi = {10.1145/1592434.1592438},
	abstract = {We survey recent progress in software model checking.},
	pages = {1--54},
	number = {4},
	journaltitle = {{ACM} Computing Surveys},
	shortjournal = {{ACM} Comput. Surv.},
	author = {Jhala, Ranjit and Majumdar, Rupak},
	urldate = {2025-02-15},
	date = {2009-10},
	langid = {english},
}

@inproceedings{coppa_rethinking_2017,
	location = {Urbana, {IL}},
	title = {Rethinking pointer reasoning in symbolic execution},
	isbn = {978-1-5386-2684-9},
	url = {http://ieeexplore.ieee.org/document/8115671/},
	doi = {10.1109/ASE.2017.8115671},
	abstract = {Symbolic execution is a popular program analysis technique that allows seeking for bugs by reasoning over multiple alternative execution states at once. As the number of states to explore may grow exponentially, a symbolic executor may quickly run out of space. For instance, a memory access to a symbolic address may potentially reference the entire address space, leading to a combinatorial explosion of the possible resulting execution states. To cope with this issue, state-of-the-art executors concretize symbolic addresses that span memory intervals larger than some threshold. Unfortunately, this could result in missing interesting execution states, e.g., where a bug arises.},
	eventtitle = {2017 32nd {IEEE}/{ACM} International Conference on Automated Software Engineering ({ASE})},
	pages = {613--618},
	booktitle = {2017 32nd {IEEE}/{ACM} International Conference on Automated Software Engineering ({ASE})},
	publisher = {{IEEE}},
	author = {Coppa, Emilio and D'Elia, Daniele Cono and Demetrescu, Camil},
	urldate = {2025-02-15},
	date = {2017-10},
	langid = {english},
}

@article{cadar_symbolic_2013-1,
	title = {Symbolic execution for software testing: three decades later},
	volume = {56},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/2408776.2408795},
	doi = {10.1145/2408776.2408795},
	shorttitle = {Symbolic execution for software testing},
	abstract = {The challenges---and great promise---of modern symbolic execution techniques, and the tools to help implement them.},
	pages = {82--90},
	number = {2},
	journaltitle = {Communications of the {ACM}},
	shortjournal = {Commun. {ACM}},
	author = {Cadar, Cristian and Sen, Koushik},
	urldate = {2025-02-15},
	date = {2013-02},
	langid = {english},
}

@article{godefroid_dart_nodate,
	title = {{DART}: Directed Automated Random Testing},
	abstract = {We present a new tool, named {DART}, for automatically testing software that combines three main techniques: (1) automated extraction of the interface of a program with its external environment using static source-code parsing; (2) automatic generation of a test driver for this interface that performs random testing to simulate the most general environment the program can operate in; and (3) dynamic analysis of how the program behaves under random testing and automatic generation of new test inputs to direct systematically the execution along alternative program paths. Together, these three techniques constitute Directed Automated Random Testing, or {DART} for short. The main strength of {DART} is thus that testing can be performed completely automatically on any program that compiles – there is no need to write any test driver or harness code. During testing, {DART} detects standard errors such as program crashes, assertion violations, and non-termination. Preliminary experiments to unit test several examples of C programs are very encouraging.},
	author = {Godefroid, Patrice and Klarlund, Nils and Sen, Koushik},
	langid = {english},
}

@inproceedings{godefroid_random_2007,
	location = {Atlanta Georgia},
	title = {Random testing for security: blackbox vs. whitebox fuzzing},
	isbn = {978-1-59593-881-7},
	url = {https://dl.acm.org/doi/10.1145/1292414.1292416},
	doi = {10.1145/1292414.1292416},
	shorttitle = {Random testing for security},
	abstract = {Fuzz testing is an eﬀective technique for ﬁnding security vulnerabilities in software. Fuzz testing is a form of blackbox random testing which randomly mutates well-formed inputs and tests the program on the resulting data. In some cases, grammars are used to randomly generate the well-formed inputs. This also allows the tester to encode applicationspeciﬁc knowledge (such as corner cases of particular interest) as part of the grammar, and to specify test heuristics by assigning probabilistic weights to production rules. Although fuzz testing can be remarkably eﬀective, the limitations of blackbox random testing are well-known. For instance, the then branch of the conditional statement “if (x==10) then” has only one in 232 chances of being exercised if x is a randomly chosen 32-bit input value. This intuitively explains why random testing usually provides low code coverage.},
	eventtitle = {{ASE}07: International Conference on Automated Software Engineering 2007},
	pages = {1--1},
	booktitle = {Proceedings of the 2nd international workshop on Random testing: co-located with the 22nd {IEEE}/{ACM} International Conference on Automated Software Engineering ({ASE} 2007)},
	publisher = {{ACM}},
	author = {Godefroid, Patrice},
	urldate = {2025-02-15},
	date = {2007-11-06},
	langid = {english},
}

@article{demott_evolving_nodate,
	title = {The Evolving Art of Fuzzing},
	author = {{DeMott}, Jared},
	langid = {english},
}

@article{b_w_boehm_verifying_1984,
	title = {Verifying and Validating Software Requirements and Design Specifications},
	volume = {1},
	doi = {10.1109/MS.1984.233702},
	abstract = {Explore millions of resources from scholarly journals, books, newspapers, videos and more, on the {ProQuest} Platform.},
	pages = {75--88},
	number = {1},
	author = {{B. W. Boehm}},
	urldate = {2025-02-13},
	date = {1984},
}

@article{boehm_software_nodate,
	title = {Software Engineering Economics},
	abstract = {This paper summarizes the current state of the art and recent trends in software engineering economics. It provides an overview of economic analysis techniques and their applicability to software engineering and management. It surveys the field of software cost estimation, including the major estimation techniques available, the state of the art in algorithmic cost models, and the outstanding research issues in software cost estimation.},
	journaltitle = {Prentice Hall},
	author = {Boehm, Barry W},
	langid = {english},
}

@online{roy_t_fielding_rest_2008,
	title = {{REST} {APIs} must be hypertext-driven » Untangled https://roy.gbiv.com/untangled/2008/rest-apis-must-be-hypertext-driven},
	url = {https://roy.gbiv.com/untangled/2008/rest-apis-must-be-hypertext-driven},
	shorttitle = {{REST} {APIs} must be hypertext-driven » Untangled https},
	author = {{Roy T. Fielding}},
	urldate = {2025-02-12},
	date = {2008},
}

@report{berners-lee_uniform_2005,
	title = {Uniform Resource Identifier ({URI}): Generic Syntax},
	url = {https://datatracker.ietf.org/doc/rfc3986},
	shorttitle = {Uniform Resource Identifier ({URI})},
	abstract = {A Uniform Resource Identifier ({URI}) is a compact sequence of characters that identifies an abstract or physical resource. This specification defines the generic {URI} syntax and a process for resolving {URI} references that might be in relative form, along with guidelines and security considerations for the use of {URIs} on the Internet. The {URI} syntax defines a grammar that is a superset of all valid {URIs}, allowing an implementation to parse the common components of a {URI} reference without knowing the scheme-specific requirements of every possible identifier. This specification does not define a generative grammar for {URIs}; that task is performed by the individual specifications of each {URI} scheme. [{STANDARDS}-{TRACK}]},
	number = {{RFC} 3986},
	institution = {Internet Engineering Task Force},
	type = {Request for Comments},
	author = {Berners-Lee, Tim and Fielding, Roy T. and Masinter, Larry M.},
	urldate = {2025-02-12},
	date = {2005-01},
	doi = {10.17487/RFC3986},
	note = {Num Pages: 61},
}

@online{noauthor_jsonapi_nodate,
	title = {{JSON}:{API} — Latest Specification (v1.1) https://jsonapi.org/format/},
	url = {https://jsonapi.org/format/},
	urldate = {2025-02-12},
}

@online{noauthor_serverless_nodate,
	title = {Serverless Function, {FaaS} Serverless - {AWS} Lambda - {AWS}},
	url = {https://aws.amazon.com/lambda/},
	abstract = {{AWS} Lambda is a serverless compute service for running code without having to provision or manage servers. You pay only for the compute time you consume.},
	titleaddon = {Amazon Web Services, Inc.},
	urldate = {2025-02-12},
	langid = {american},
}

@report{fielding_http_2022,
	title = {{HTTP} Semantics},
	url = {https://datatracker.ietf.org/doc/rfc9110},
	abstract = {The Hypertext Transfer Protocol ({HTTP}) is a stateless application-level protocol for distributed, collaborative, hypertext information systems. This document describes the overall architecture of {HTTP}, establishes common terminology, and defines aspects of the protocol that are shared by all versions. In this definition are core protocol elements, extensibility mechanisms, and the "http" and "https" Uniform Resource Identifier ({URI}) schemes. This document updates {RFC} 3864 and obsoletes {RFCs} 2818, 7231, 7232, 7233, 7235, 7538, 7615, 7694, and portions of 7230.},
	number = {{RFC} 9110},
	institution = {Internet Engineering Task Force},
	type = {Request for Comments},
	author = {Fielding, Roy T. and Nottingham, Mark and Reschke, Julian},
	urldate = {2025-02-12},
	date = {2022-06},
	doi = {10.17487/RFC9110},
	note = {Num Pages: 194},
}

@article{oehlert_violating_2005,
	title = {Violating assumptions with fuzzing},
	volume = {3},
	issn = {1558-4046},
	url = {https://ieeexplore.ieee.org/document/1423963/?arnumber=1423963},
	doi = {10.1109/MSP.2005.55},
	abstract = {Fuzzing is a highly automated testing technique that covers numerous boundary cases using invalid data (from files, network protocols, {API} calls, and other targets) as application input to better ensure the absence of exploitable vulnerabilities. Fuzzing lets developers or quality assurance ({QA}) teams test large numbers of boundary cases when doing so with techniques such as functional testing would be cost prohibitive. Comprehensive negative test cases - those that verify that a product does not do something it shouldn't do, rather than that it does something it is supposed to (positive test cases) - are difficult to construct because the number of possible permutations is astronomical. Yet, fuzzing covers a significant portion of negative test cases without forcing the tester to deal with each specific test case for a given boundary condition.},
	pages = {58--62},
	number = {2},
	journaltitle = {{IEEE} Security \& Privacy},
	author = {Oehlert, P.},
	urldate = {2025-02-12},
	date = {2005-03},
	note = {Conference Name: {IEEE} Security \& Privacy},
	keywords = {Application software, Automatic testing, Computer security, Cost function, Data privacy, Data security, Kernel, Network interfaces, Protocols, Quality assurance, boundary conditions, completeness testing, comprehensive negative testing, fuzzing, quality assurance, software testing},
}

@article{liang_fuzzing_2018,
	title = {Fuzzing: State of the Art},
	volume = {67},
	rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
	issn = {0018-9529, 1558-1721},
	url = {https://ieeexplore.ieee.org/document/8371326/},
	doi = {10.1109/TR.2018.2834476},
	shorttitle = {Fuzzing},
	abstract = {As one of the most popular software testing techniques, fuzzing can ﬁnd a variety of weaknesses in a program, such as software bugs and vulnerabilities, by generating numerous test inputs. Due to its effectiveness, fuzzing is regarded as a valuable bug hunting method. In this paper, we present an overview of fuzzing that concentrates on its general process, as well as classiﬁcations, followed by detailed discussion of the key obstacles and some stateof-the-art technologies which aim to overcome or mitigate these obstacles. We further investigate and classify several widely used fuzzing tools. Our primary goal is to equip the stakeholder with a better understanding of fuzzing and the potential solutions for improving fuzzing methods in the spectrum of software testing and security. To inspire future research, we also predict some future directions with regard to fuzzing.},
	pages = {1199--1218},
	number = {3},
	journaltitle = {{IEEE} Transactions on Reliability},
	shortjournal = {{IEEE} Trans. Rel.},
	author = {Liang, Hongliang and Pei, Xiaoxiao and Jia, Xiaodong and Shen, Wuwei and Zhang, Jian},
	urldate = {2025-02-12},
	date = {2018-09},
	langid = {english},
}

@online{noauthor_web_2012,
	title = {The Web Application Security Consortium},
	url = {http://projects.webappsec.org/w/page/13246967/The%20Web%20Security%20Glossary#WebApplication},
	shorttitle = {{WASC}},
	titleaddon = {The Web Security Glossary},
	urldate = {2025-02-07},
	date = {2012},
}

@inproceedings{fong_web_2007,
	location = {Waikoloa, {HI}, {USA}},
	title = {Web Application Scanners: Definitions and Functions},
	url = {http://ieeexplore.ieee.org/document/4076950/},
	doi = {10.1109/HICSS.2007.611},
	shorttitle = {Web Application Scanners},
	abstract = {There are many commercial software security assurance tools that claim to detect and prevent vulnerabilities in application software. However, a closer look at the tools often leaves one wondering which tools find what vulnerabilities. This paper identifies a taxonomy of software security assurance tools and defines one type of tool: web application scanner, i.e., an automated program that examines web applications for security vulnerabilities. We describe the types of functions that are generally found in a web application scanner and how to test it.},
	eventtitle = {2007 40th Annual Hawaii International Conference on System Sciences ({HICSS}'07)},
	pages = {280b--280b},
	booktitle = {2007 40th Annual Hawaii International Conference on System Sciences ({HICSS}'07)},
	publisher = {{IEEE}},
	author = {Fong, Elizabeth and Okun, Vadim},
	urldate = {2025-02-07},
	date = {2007},
	langid = {english},
}

@article{veres_exploration_nodate,
	title = {An Exploration of Current Techniques in {OWASP} Vulnerability Detection and Improvement Opportunities},
	abstract = {Web applications are foundational in today’s digital landscape, necessitating advanced security measures. This study delves into Interactive Application Security Testing ({IAST}) and Web Fuzzing, two pivotal techniques for detecting web vulnerabilities. We systematically evaluate their strengths and weaknesses, emphasizing their potential in addressing vulnerabilities highlighted by the {OWASP} Top 10. While {IAST} excels in real-time vulnerability detection, Web Fuzzing offers an expansive approach, adept at uncovering elusive edge cases. Our research suggests that combining these techniques could lead to substantial enhancements in web application security. Additionally, we introduce the idea of an open-source {IAST} tool and contemplate the benefits that recent advances in artificial intelligence might bring to these techniques. Furthermore, we underscore the significance of understanding these tools’ operation within the realm of cloud computing.},
	author = {Veres, Andrei-Claudiu},
	langid = {english},
}

@article{cadar_klee_nodate,
	title = {{KLEE}: Unassisted and Automatic Generation of High-Coverage Tests for Complex Systems Programs},
	abstract = {We present a new symbolic execution tool, {KLEE}, capable of automatically generating tests that achieve high coverage on a diverse set of complex and environmentally-intensive programs. We used {KLEE} to thoroughly check all 89 stand-alone programs in the {GNU} {COREUTILS} utility suite, which form the core user-level environment installed on millions of Unix systems, and arguably are the single most heavily tested set of open-source programs in existence. {KLEE}-generated tests achieve high line coverage — on average over 90\% per tool (median: over 94\%) — and signiﬁcantly beat the coverage of the developers’ own hand-written test suite. When we did the same for 75 equivalent tools in the {BUSYBOX} embedded system suite, results were even better, including 100\% coverage on 31 of them.},
	author = {Cadar, Cristian and Dunbar, Daniel and Engler, Dawson},
	langid = {english},
}

@inproceedings{sen_concolic_2007,
	location = {New York, {NY}, {USA}},
	title = {Concolic testing},
	isbn = {978-1-59593-882-4},
	url = {https://doi.org/10.1145/1321631.1321746},
	doi = {10.1145/1321631.1321746},
	series = {{ASE} '07},
	abstract = {Concolic testing automates test input generation by combining the concrete and symbolic (concolic) execution of the code under test. Traditional test input generation techniques use either (1) concrete execution or (2) symbolic execution that builds constraints and is followed by a generation of concrete test inputs from these constraints. In contrast, concolic testing tightly couples both concrete and symbolic executions: they run simultaneously, and each gets feedback from the other.We have implemented concolic testing in tools for testing both C and Java programs. We have used the tools to find bugs in several real-world software systems including {SGLIB}, a popular C data structure library used in a commercial tool, a third-party implementation of the Needham-Schroeder protocol and the {TMN} protocol, the scheduler of Honeywell's {DEOS} real-time operating system, and the Sun Microsystems' {JDK} 1.4 collection framework. In this tutorial, we will describe concolic testing and some of its recent extensions},
	pages = {571--572},
	booktitle = {Proceedings of the 22nd {IEEE}/{ACM} International Conference on Automated Software Engineering},
	publisher = {Association for Computing Machinery},
	author = {Sen, Koushik},
	urldate = {2025-01-29},
	date = {2007-11-05},
}

@article{pak_hybrid_nodate,
	title = {Hybrid Fuzz Testing: Discovering Software Bugs via Fuzzing and Symbolic Execution},
	author = {Pak, Brian S},
	langid = {english},
}

@online{noauthor_httpswwwusenixorgsystemfilesconferenceusenixsecurity15sec15-paper-ramospdf_nodate,
	title = {https://www.usenix.org/system/files/conference/usenixsecurity15/sec15-paper-ramos.pdf},
	url = {https://www.usenix.org/system/files/conference/usenixsecurity15/sec15-paper-ramos.pdf},
	urldate = {2025-01-24},
}

@article{ramos_under-constrained_nodate,
	title = {Under-Constrained Symbolic Execution: Correctness Checking for Real Code},
	abstract = {Software bugs are a well-known source of security vulnerabilities. One technique for ﬁnding bugs, symbolic execution, considers all possible inputs to a program but suffers from scalability limitations. This paper uses a variant, under-constrained symbolic execution, that improves scalability by directly checking individual functions, rather than whole programs. We present {UC}-{KLEE}, a novel, scalable framework for checking C/C++ systems code, along with two use cases. First, we use {UC}-{KLEE} to check whether patches introduce crashes. We check over 800 patches from {BIND} and {OpenSSL} and ﬁnd 12 bugs, including two {OpenSSL} denial-of-service vulnerabilities. We also verify (with caveats) that 115 patches do not introduce crashes. Second, we use {UC}-{KLEE} as a generalized checking framework and implement checkers to ﬁnd memory leaks, uninitialized data, and unsafe user input. We evaluate the checkers on over 20,000 functions from {BIND}, {OpenSSL}, and the Linux kernel, ﬁnd 67 bugs, and verify that hundreds of functions are leak free and that thousands of functions do not access uninitialized data.},
	author = {Ramos, David A and Engler, Dawson},
	langid = {english},
}

@article{tillmann_unit_2006,
	title = {Unit tests reloaded: parameterized unit testing with symbolic execution},
	volume = {23},
	rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
	issn = {0740-7459},
	url = {http://ieeexplore.ieee.org/document/1657937/},
	doi = {10.1109/MS.2006.117},
	shorttitle = {Unit tests reloaded},
	pages = {38--47},
	number = {4},
	journaltitle = {{IEEE} Software},
	shortjournal = {{IEEE} Softw.},
	author = {Tillmann, N. and Schulte, W.},
	urldate = {2025-01-19},
	date = {2006-07},
	langid = {english},
}

@inproceedings{cha_unleashing_2012,
	location = {San Francisco, {CA}, {USA}},
	title = {Unleashing Mayhem on Binary Code},
	isbn = {978-1-4673-1244-8 978-0-7695-4681-0},
	url = {https://ieeexplore.ieee.org/document/6234425/},
	doi = {10.1109/SP.2012.31},
	abstract = {In this paper we present {MAYHEM}, a new system for automatically ﬁnding exploitable bugs in binary (i.e., executable) programs. Every bug reported by {MAYHEM} is accompanied by a working shell-spawning exploit. The working exploits ensure soundness and that each bug report is securitycritical and actionable. {MAYHEM} works on raw binary code without debugging information. To make exploit generation possible at the binary-level, {MAYHEM} addresses two major technical challenges: actively managing execution paths without exhausting memory, and reasoning about symbolic memory indices, where a load or a store address depends on user input. To this end, we propose two novel techniques: 1) hybrid symbolic execution for combining online and ofﬂine (concolic) execution to maximize the beneﬁts of both techniques, and 2) index-based memory modeling, a technique that allows {MAYHEM} to efﬁciently reason about symbolic memory at the binary level. We used {MAYHEM} to ﬁnd and demonstrate 29 exploitable vulnerabilities in both Linux and Windows programs, 2 of which were previously undocumented.},
	eventtitle = {2012 {IEEE} Symposium on Security and Privacy ({SP}) Conference dates subject to change},
	pages = {380--394},
	booktitle = {2012 {IEEE} Symposium on Security and Privacy},
	publisher = {{IEEE}},
	author = {Cha, Sang Kil and Avgerinos, Thanassis and Rebert, Alexandre and Brumley, David},
	urldate = {2025-01-09},
	date = {2012-05},
	langid = {english},
}

@online{noauthor_httpsusersececmueduaavgerinpapersmayhem-oakland-12pdf_nodate,
	title = {https://users.ece.cmu.edu/{\textasciitilde}aavgerin/papers/mayhem-oakland-12.pdf},
	url = {https://users.ece.cmu.edu/~aavgerin/papers/mayhem-oakland-12.pdf},
	urldate = {2025-01-09},
}

@article{aydos_security_2022,
	title = {Security testing of web applications: A systematic mapping of the literature},
	volume = {34},
	issn = {1319-1578},
	url = {https://www.sciencedirect.com/science/article/pii/S131915782100269X},
	doi = {10.1016/j.jksuci.2021.09.018},
	shorttitle = {Security testing of web applications},
	abstract = {Context
Web application security is a main component of any web-based business. Web applications are subject to attacks from different locations at various levels of scale and complexity. In this context, a large number of testing techniques, tools and frameworks have been proposed by both practitioners and researchers to effectively and efficiently test the security of web applications.
Objective
As the number of papers increases in the security of web applications and this research area matures, reviewing and getting an overview of this area is getting challenging for a practitioner or a new researcher. Our objective is to summarize the state-of-the-art in web application security testing which could benefit practitioners to potentially utilize that information.
Method
We review and structure the body of knowledge related to web application security testing in the form of a systematic literature mapping ({SLM}). As part of this study, we pose four sets of research questions, define selection and exclusion criteria, and systematically develop and refine a classification schema. The initial pool consisted of 154 articles. Systematic voting was conducted among the authors regarding the inclusion/exclusion of articles. As a result, there were 80 technical articles in our final pool. Accordance with our inclusion and exclusion criteria, the first article was published in 2005 and this review includes all the papers until the end of 2020. During December 2020, January and February 2021, the search phase has been conducted.
Results
This review paper provides an overview of web application security testing with different focused headings. These headings cover contribution types, web security testing tools and their sub features, specific questions/features to the security testing such as vulnerability types, system under testing ({SUT}) focused headings and more.
Conclusion
The results of this study would benefit researchers working on web application security testing. Also, it could be useful for developers who discuss application security while they develop web applications. Thanks to this paper, these researchers could utilize the all results and use them to catch the trend of web application security testing and secure development.},
	pages = {6775--6792},
	number = {9},
	journaltitle = {Journal of King Saud University - Computer and Information Sciences},
	shortjournal = {Journal of King Saud University - Computer and Information Sciences},
	author = {Aydos, Murat and Aldan, Çiğdem and Coşkun, Evren and Soydan, Alperen},
	urldate = {2024-11-11},
	date = {2022-10-01},
	keywords = {Security testing, Systematic literature mapping, Systematic literature review, Systematic mapping, Web application security},
}

@article{jaiswal_security_2014,
	title = {Security Testing of Web Applications: Issues and Challenges},
	volume = {88},
	issn = {09758887},
	url = {http://research.ijcaonline.org/volume88/number3/pxc3893667.pdf},
	doi = {10.5120/15334-3667},
	shorttitle = {Security Testing of Web Applications},
	abstract = {Due to the increasing complexity of web systems, security testing has become indispensable and critical activity of web application development life cycle. Security testing aims to maintain the confidentiality of the data, to check against any information leakage and to maintain the functionality as intended. It checks whether the security requirements are fulfilled by the web applications when they are subjected to malicious input data. Due to the rising explosion in the security vulnerabilities, there occurs a need to understand its unique challenges and issues which will eventually serve as a useful input for the security testing tool developers and test managers for their relative projects.},
	pages = {26--32},
	number = {3},
	journaltitle = {International Journal of Computer Applications},
	shortjournal = {{IJCA}},
	author = {Jaiswal, Arunima and Raj, Gaurav and Singh, Dheerendra},
	urldate = {2024-11-11},
	date = {2014-02-14},
	langid = {english},
}

@inproceedings{loring_expose_2017,
	location = {New York, {NY}, {USA}},
	title = {{ExpoSE}: practical symbolic execution of standalone {JavaScript}},
	isbn = {978-1-4503-5077-8},
	url = {https://doi.org/10.1145/3092282.3092295},
	doi = {10.1145/3092282.3092295},
	series = {{SPIN} 2017},
	shorttitle = {{ExpoSE}},
	abstract = {{JavaScript} has evolved into a versatile ecosystem for not just the web, but also a wide range of server-side and client-side applications. With this increased scope, the potential impact of bugs increases. We introduce {ExpoSE}, a dynamic symbolic execution engine for Node.js applications. {ExpoSE} automatically generates test cases to find bugs and cover as many paths in the target program as possible. We discuss the specific challenges for symbolic execution arising from the widespread use of regular expressions in such applications. In particular, we make explicit the issues of capture groups, backreferences, and greediness in {JavaScript}'s flavor of regular expressions, and our models improve over previous work that only partially addressed these. We evaluate {ExpoSE} on three popular {JavaScript} libraries that make heavy use of regular expressions, and we report a previously unknown bug in the Minimist library.},
	pages = {196--199},
	booktitle = {Proceedings of the 24th {ACM} {SIGSOFT} International {SPIN} Symposium on Model Checking of Software},
	publisher = {Association for Computing Machinery},
	author = {Loring, Blake and Mitchell, Duncan and Kinder, Johannes},
	urldate = {2024-10-28},
	date = {2017-07-13},
}

@inproceedings{eriksson_black_2021,
	title = {Black Widow: Blackbox Data-driven Web Scanning},
	url = {https://ieeexplore.ieee.org/abstract/document/9519452},
	doi = {10.1109/SP40001.2021.00022},
	shorttitle = {Black Widow},
	abstract = {Modern web applications are an integral part of our digital lives. As we put more trust in web applications, the need for security increases. At the same time, detecting vulnerabilities in web applications has become increasingly hard, due to the complexity, dynamism, and reliance on third-party components. Blackbox vulnerability scanning is especially challenging because (i) for deep penetration of web applications scanners need to exercise such browsing behavior as user interaction and asynchrony, and (ii) for detection of nontrivial injection attacks, such as stored cross-site scripting ({XSS}), scanners need to discover inter-page data dependencies.This paper illuminates key challenges for crawling and scanning the modern web. Based on these challenges we identify three core pillars for deep crawling and scanning: navigation modeling, traversing, and tracking inter-state dependencies. While prior efforts are largely limited to the separate pillars, we suggest an approach that leverages all three. We develop Black Widow, a blackbox data-driven approach to web crawling and scanning. We demonstrate the effectiveness of the crawling by code coverage improvements ranging from 63\% to 280\% compared to other crawlers across all applications. Further, we demonstrate the effectiveness of the web vulnerability scanning by featuring no false positives and finding more cross-site scripting vulnerabilities than previous methods. In older applications, used in previous research, we find vulnerabilities that the other methods miss. We also find new vulnerabili-ties in production software, including {HotCRP}, {osCommerce}, {PrestaShop} and {WordPress}.},
	eventtitle = {2021 {IEEE} Symposium on Security and Privacy ({SP})},
	pages = {1125--1142},
	booktitle = {2021 {IEEE} Symposium on Security and Privacy ({SP})},
	author = {Eriksson, Benjamin and Pellegrino, Giancarlo and Sabelfeld, Andrei},
	urldate = {2024-10-28},
	date = {2021-05},
	note = {{ISSN}: 2375-1207},
	keywords = {Crawlers, Cross-site scripting, Distance measurement, Navigation, Privacy, Production, Software, {XSS}, cross-site scripting, security testing, web application scanning, web crawling},
}

@inproceedings{loring_sound_2019,
	location = {New York, {NY}, {USA}},
	title = {Sound regular expression semantics for dynamic symbolic execution of {JavaScript}},
	isbn = {978-1-4503-6712-7},
	url = {https://doi.org/10.1145/3314221.3314645},
	doi = {10.1145/3314221.3314645},
	series = {{PLDI} 2019},
	abstract = {Support for regular expressions in symbolic execution-based tools for test generation and bug finding is insufficient. Common aspects of mainstream regular expression engines, such as backreferences or greedy matching, are ignored or imprecisely approximated, leading to poor test coverage or missed bugs. In this paper, we present a model for the complete regular expression language of {ECMAScript} 2015 ({ES}6), which is sound for dynamic symbolic execution of the test and exec functions. We model regular expression operations using string constraints and classical regular expressions and use a refinement scheme to address the problem of matching precedence and greediness. We implemented our model in {ExpoSE}, a dynamic symbolic execution engine for {JavaScript}, and evaluated it on over 1,000 Node.js packages containing regular expressions, demonstrating that the strategy is effective and can significantly increase the number of successful regular expression queries and therefore boost coverage.},
	pages = {425--438},
	booktitle = {Proceedings of the 40th {ACM} {SIGPLAN} Conference on Programming Language Design and Implementation},
	publisher = {Association for Computing Machinery},
	author = {Loring, Blake and Mitchell, Duncan and Kinder, Johannes},
	urldate = {2024-10-28},
	date = {2019-06-08},
}

@inproceedings{eriksson_black_2023,
	location = {New York, {NY}, {USA}},
	title = {Black Ostrich: Web Application Scanning with String Solvers},
	isbn = {9798400700507},
	url = {https://dl.acm.org/doi/10.1145/3576915.3616582},
	doi = {10.1145/3576915.3616582},
	series = {{CCS} '23},
	shorttitle = {Black Ostrich},
	abstract = {Securing web applications remains a pressing challenge. Unfortunately, the state of the art in web crawling and security scanning still falls short of deep crawling. A major roadblock is the crawlers' limited ability to pass input validation checks when web applications require data of a certain format, such as email, phone number, or zip code. This paper develops Black Ostrich, a principled approach to deep web crawling and scanning. The key idea is to equip web crawling with string constraint solving capabilities to dynamically infer suitable inputs from regular expression patterns in web applications and thereby pass input validation checks. To enable this use of constraint solvers, we develop new automata-based techniques to process {JavaScript} regular expressions. We implement our approach extending and combining the Ostrich constraint solver with the Black Widow web crawler. We evaluate Black Ostrich on a set of 8,820 unique validation patterns gathered from over 21,667,978 forms from a combination of the July 2021 Common{\textasciitilde}Crawl and Tranco top 100K. For these forms and reconstructions of input elements corresponding to the patterns, we demonstrate that Black Ostrich achieves a 99\% coverage of the form validations compared to an average of 36\% for the state-of-the-art scanners. Moreover, out of the 66,377 domains using these patterns, we solve all patterns on 66,309 (99\%) while the combined efforts of the other scanners cover 52,632 (79\%). We further show that our approach can boost coverage by evaluating it on three open-source applications. Our empirical studies include a study of email validation patterns, where we find that 213 (26\%) out of the 825 found email validation patterns liberally admit {XSS} injection payloads.},
	pages = {549--563},
	booktitle = {Proceedings of the 2023 {ACM} {SIGSAC} Conference on Computer and Communications Security},
	publisher = {Association for Computing Machinery},
	author = {Eriksson, Benjamin and Stjerna, Amanda and De Masellis, Riccardo and Rüemmer, Philipp and Sabelfeld, Andrei},
	urldate = {2024-10-28},
	date = {2023-11-21},
}
